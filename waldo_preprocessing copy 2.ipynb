{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Wheres Waldo?\n",
    "### Name: Eileanor LaRocco\n",
    "In this assignment, you will develop an object detection algorithm to locate Waldo in a set of images. You will develop a model to detect the bounding box around Waldo. Your final task is to submit your predictions on Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process/Issues\n",
    "- Double-checked that the images we were given were correctly bounded (did this by visualizing the boxes on the images - they look good!)\n",
    "- Complication: Originally when I creating augmented images, the bounding box labels did not also augment. I also had to try out a few types of augmentation to see what made sense for waldo. The augmented images may still not be as different from one another as they could be which could allow the model to favor the training images that occur more frequently.\n",
    "- Complication: Similarly, when resizing the images, ensuring the bounding boxes not only are also adjusted if necessary, but ensuring they do not get cut off and the image is not stretched/shrunk too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/albumentations/check_version.py:51: UserWarning: Error fetching version info <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import opendatasets as od\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading 2024-fall-ml-3-hw-4-wheres-waldo.zip to ./2024-fall-ml-3-hw-4-wheres-waldo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38.2M/38.2M [00:01<00:00, 20.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./2024-fall-ml-3-hw-4-wheres-waldo/2024-fall-ml-3-hw-4-wheres-waldo.zip to ./2024-fall-ml-3-hw-4-wheres-waldo\n"
     ]
    }
   ],
   "source": [
    "od.download('https://www.kaggle.com/competitions/2024-fall-ml-3-hw-4-wheres-waldo/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train\" # Original Train Images\n",
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\" # Original Test Images\n",
    "annotations_file = \"2024-fall-ml-3-hw-4-wheres-waldo/annotations.csv\" # Original Annotations File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Images (Crop/Augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train and validation sets\n",
    "annotations = pd.read_csv(annotations_file)\n",
    "image_files = annotations[\"filename\"].unique()\n",
    "train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv_by_column(input_csv, output_csv, column_name, values_list):\n",
    "    \"\"\"\n",
    "    Filters rows in a CSV file and keeps only those where the specified column's value is in a given list.\n",
    "\n",
    "    Parameters:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to save the filtered CSV file.\n",
    "        column_name (str): Column to filter on.\n",
    "        values_list (list): List of values to keep.\n",
    "    \"\"\"\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df[column_name].isin(values_list)]\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    filtered_df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split annotations into train and val\n",
    "values_list = list(train_images)\n",
    "\n",
    "# Example usage\n",
    "output_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\"  # Replace with your output file path\n",
    "column_name = \"filename\"  # Replace with the column you want to filter\n",
    "\n",
    "filter_csv_by_column(annotations_file, output_csv, column_name, values_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "values_list = list(val_images)\n",
    "\n",
    "# Example usage\n",
    "output_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\"  # Replace with your output file path\n",
    "column_name = \"filename\"  # Replace with the column you want to filter\n",
    "\n",
    "filter_csv_by_column(annotations_file, output_csv, column_name, values_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved: 18.jpg\n",
      "Moved: 5.jpg\n",
      "Moved: 19.jpg\n",
      "Moved: 22.jpg\n",
      "Moved: 1.jpg\n",
      "Moved: 3.jpg\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def split_directory(source_dir, target_dir, file_list):\n",
    "    \"\"\"Splits files from source_dir to target_dir based on file_list.\"\"\"\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    for file_name in file_list:\n",
    "        source_path = os.path.join(source_dir, file_name)\n",
    "        target_path = os.path.join(target_dir, file_name)\n",
    "\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.move(source_path, target_path)\n",
    "            print(f\"Moved: {file_name}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_dir = train_folder\n",
    "    target_dir = \"2024-fall-ml-3-hw-4-wheres-waldo/train/val\"\n",
    "    file_list = list(val_images)\n",
    "\n",
    "    split_directory(source_dir, target_dir, file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "class WaldoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transforms=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = F.to_tensor(image)\n",
    "        \n",
    "        # Read bounding box data, ensuring all are converted to float\n",
    "        box_data = self.img_labels.iloc[idx, 4:8].values\n",
    "        boxes = []\n",
    "        for item in box_data:\n",
    "            try:\n",
    "                boxes.append(float(item))\n",
    "            except ValueError as e:\n",
    "                raise ValueError(f\"Error converting bounding box data to float: {e}\")\n",
    "\n",
    "        # Create tensors\n",
    "        boxes = torch.as_tensor([boxes], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "            image = np.array(image)\n",
    "            target = np.array(target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Create the dataset\n",
    "train_dataset = WaldoDataset(annotations_file= \"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\"\n",
    "                             , img_dir=train_folder\n",
    "                             , transforms = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.Resize((224, 224)),\n",
    "                                            #torchvision.transforms.ToTensor(),\n",
    "                                            #torchvision.transforms.ToPILImage()\n",
    "                                        ]))\n",
    "val_dataset = WaldoDataset(annotations_file= \"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\"\n",
    "                           , img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/val\"\n",
    "                        , transforms = torchvision.transforms.Compose([\n",
    "                                        torchvision.transforms.Resize((224, 224)),\n",
    "                                        #torchvision.transforms.ToTensor(),\n",
    "                                        #torchvision.transforms.ToPILImage()\n",
    "                                    ]))\n",
    "\n",
    "# Now, you can use this dataset with a DataLoader to train your model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: zip(*x)\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: zip(*x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 18, 750, 500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleYOLOv3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleYOLOv3, self).__init__()\n",
    "\n",
    "        # Backbone: Feature extractor (simplified)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Add more convolutional layers as needed\n",
    "        )\n",
    "\n",
    "        # Detection head (simplified)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, (5 + num_classes) * 3, 1),  # 3 bounding boxes per cell\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.backbone(x)\n",
    "        #print(f\"After backbone: {x.shape}\")\n",
    "        \n",
    "        x = self.head(x)\n",
    "        #print(f\"After head: {x.shape}\")\n",
    "        \n",
    "        return x \n",
    "\n",
    "# Instantiate and check the model\n",
    "model = SimpleYOLOv3(num_classes=1)\n",
    "input_image = torch.randn(1, 3, 1500, 1000)  # Example batch\n",
    "output = model(input_image)\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 750, 500])\n",
      "torch.Size([1, 18, 750, 500])\n",
      "torch.Size([1, 18, 750, 500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleYOLOv3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleYOLOv3, self).__init__()\n",
    "\n",
    "        # Backbone: Feature extractor (simplified)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Add more convolutional layers as needed\n",
    "        )\n",
    "\n",
    "        # Detection head (simplified)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, (5 + num_classes) * 3, 1),  # 3 bounding boxes per cell\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        print(x.size())\n",
    "        x = self.head(x)\n",
    "        print(x.size())\n",
    "        return x \n",
    "\n",
    "# Instantiate and check the model\n",
    "model = SimpleYOLOv3(num_classes=1)\n",
    "input_image = torch.randn(1, 3, 1500, 1000)  # Example batch\n",
    "output = model(input_image)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray)!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray)!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     evaluate(model, val_data_loader, device)\n",
      "Cell \u001b[0;32mIn[68], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;66;03m#.to(device)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Extract predicted boxes and target boxes (for simplicity, assuming one grid cell)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m predictions[:, :\u001b[38;5;241m4\u001b[39m]  \u001b[38;5;66;03m# first 4 are bounding box coordinates\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[67], line 26\u001b[0m, in \u001b[0;36mSimpleYOLOv3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#print(f\"Input shape: {x.shape}\")\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m#print(f\"After backbone: {x.shape}\")\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray)!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray)!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "model = SimpleYOLOv3(num_classes=1)\n",
    "\n",
    "# IoU calculation\n",
    "def compute_iou(pred_boxes, true_boxes):\n",
    "    # pred_boxes and true_boxes should be in (x_min, y_min, x_max, y_max)\n",
    "    inter_xmin = torch.max(pred_boxes[:, 0], true_boxes[:, 0])\n",
    "    inter_ymin = torch.max(pred_boxes[:, 1], true_boxes[:, 1])\n",
    "    inter_xmax = torch.min(pred_boxes[:, 2], true_boxes[:, 2])\n",
    "    inter_ymax = torch.min(pred_boxes[:, 3], true_boxes[:, 3])\n",
    "\n",
    "    inter_area = torch.clamp(inter_xmax - inter_xmin, min=0) * torch.clamp(inter_ymax - inter_ymin, min=0)\n",
    "    pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "    true_area = (true_boxes[:, 2] - true_boxes[:, 0]) * (true_boxes[:, 3] - true_boxes[:, 1])\n",
    "\n",
    "    union_area = pred_area + true_area - inter_area\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "# Simple IoU loss function\n",
    "def iou_loss(pred_boxes, true_boxes):\n",
    "    iou = compute_iou(pred_boxes, true_boxes)\n",
    "    return 1 - iou.mean()  # We want to maximize IoU, so minimize 1 - IoU\n",
    "\n",
    "# Custom YOLOv3 training loop\n",
    "def train(model, train_data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, targets in train_data_loader:\n",
    "        #print(images.shape())\n",
    "        #print(images)\n",
    "        #print(targets.shape())\n",
    "        images = images#.to(device)\n",
    "        targets = targets#.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Extract predicted boxes and target boxes (for simplicity, assuming one grid cell)\n",
    "        pred_boxes = predictions[:, :4]  # first 4 are bounding box coordinates\n",
    "        pred_conf = predictions[:, 4]    # 5th is objectness confidence\n",
    "        pred_class = predictions[:, 5:]  # remaining are class predictions\n",
    "\n",
    "        true_boxes = targets[:, :4]  # Ground truth boxes\n",
    "        true_conf = targets[:, 4]    # Objectness confidence\n",
    "        true_class = targets[:, 5:]  # Ground truth class\n",
    "\n",
    "        # Losses\n",
    "        loss_loc = iou_loss(pred_boxes, true_boxes)  # IoU loss\n",
    "        loss_conf = torch.nn.BCEWithLogitsLoss()(pred_conf, true_conf)  # Confidence loss\n",
    "        loss_class = torch.nn.BCEWithLogitsLoss()(pred_class, true_class)  # Classification loss\n",
    "\n",
    "        # Total loss (sum or weighted sum)\n",
    "        loss = loss_loc + loss_conf + loss_class\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_data_loader)\n",
    "    print(f\"Training loss: {avg_loss}\")\n",
    "\n",
    "# Evaluation (testing) function\n",
    "def evaluate(model, val_data_loader, device):\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_data_loader:\n",
    "            images = images#.to(device)\n",
    "            targets = targets#.to(device)\n",
    "\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Extract predicted boxes and target boxes\n",
    "            pred_boxes = predictions[:, :4]\n",
    "            true_boxes = targets[:, :4]\n",
    "\n",
    "            # Calculate IoU for the batch\n",
    "            iou = compute_iou(pred_boxes, true_boxes)\n",
    "            total_iou += iou.mean().item()\n",
    "\n",
    "    avg_iou = total_iou / len(val_data_loader)\n",
    "    print(f\"Average IoU on test set: {avg_iou}\")\n",
    "\n",
    "# Initialize model, optimizer, and device\n",
    "model = SimpleYOLOv3(num_classes=1)#.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train(model, train_data_loader, optimizer, device)\n",
    "    evaluate(model, val_data_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO model\n",
    "#model = YOLO(\"yolov5su.pt\")  # Load pretrained weights\n",
    "#model.train(data=\"yolo.yaml\", epochs=15, imgsz=640, pretrained=True, augment=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 210.5ms\n",
      "1: 640x640 (no detections), 210.5ms\n",
      "2: 640x640 (no detections), 210.5ms\n",
      "3: 640x640 (no detections), 210.5ms\n",
      "4: 640x640 (no detections), 210.5ms\n",
      "5: 640x640 (no detections), 210.5ms\n",
      "6: 640x640 (no detections), 210.5ms\n",
      "7: 640x640 (no detections), 210.5ms\n",
      "8: 640x640 (no detections), 210.5ms\n",
      "Speed: 1.5ms preprocess, 210.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolo_test_predictions/train2\u001b[0m\n",
      "0 label saved to yolo_test_predictions/train2/labels\n",
      "Predictions saved to yolo_test_predictions/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\"\n",
    "\n",
    "# Predict on test images\n",
    "test_images = [os.path.join(test_folder, img) for img in os.listdir(test_folder) if img.endswith(\".jpg\")]\n",
    "results = model.predict(source=test_images, save=True, save_txt=True, project=\"yolo_test_predictions\")\n",
    "\n",
    "# Prepare to save the predictions\n",
    "output_csv_path = os.path.join(\"yolo_test_predictions\", \"predictions.csv\")\n",
    "predictions = []\n",
    "\n",
    "# Process results\n",
    "for result in results:\n",
    "    image_name = os.path.basename(result.path)  # Get the image name\n",
    "    if result.boxes is not None and len(result.boxes) > 0:  # Check if there are predictions\n",
    "        # Convert result.boxes to tensor for easier access\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()  # Convert bounding boxes to array\n",
    "        confidences = result.boxes.conf.cpu().numpy()  # Convert confidence scores to array\n",
    "\n",
    "        # Find the index of the box with the highest confidence\n",
    "        best_idx = confidences.argmax()\n",
    "        best_box = boxes[best_idx]\n",
    "        conf = confidences[best_idx]\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        x_min, y_min, x_max, y_max = best_box\n",
    "        predictions.append([image_name, x_min, y_min, x_max, y_max, conf])\n",
    "    else:\n",
    "        # No predictions for this image\n",
    "        predictions.append([image_name, None, None, None, None, None])\n",
    "\n",
    "# Save predictions to CSV\n",
    "df = pd.DataFrame(predictions, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"confidence\"])\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
