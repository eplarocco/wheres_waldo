{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Wheres Waldo?\n",
    "### Name: Eileanor LaRocco\n",
    "In this assignment, you will develop an object detection algorithm to locate Waldo in a set of images. You will develop a model to detect the bounding box around Waldo. Your final task is to submit your predictions on Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process/Issues\n",
    "- Double-checked that the images we were given were correctly bounded (did this by visualizing the boxes on the images - they look good!)\n",
    "- Complication: Originally when I creating augmented images, the bounding box labels did not also augment. I also had to try out a few types of augmentation to see what made sense for waldo. The augmented images may still not be as different from one another as they could be which could allow the model to favor the training images that occur more frequently.\n",
    "- Complication: Similarly, when resizing the images, ensuring the bounding boxes not only are also adjusted if necessary, but ensuring they do not get cut off and the image is not stretched/shrunk too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/albumentations/check_version.py:51: UserWarning: Error fetching version info <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import opendatasets as od\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./2024-fall-ml-3-hw-4-wheres-waldo\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download('https://www.kaggle.com/competitions/2024-fall-ml-3-hw-4-wheres-waldo/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train\" # Original Train Images\n",
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\" # Original Test Images\n",
    "annotations_file = \"2024-fall-ml-3-hw-4-wheres-waldo/annotations.csv\" # Original Annotations File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Images (Crop/Augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Image Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 8.jpg, Width: 2800, Height: 1760\n",
      "Image: 9.jpg, Width: 1298, Height: 951\n",
      "Image: 14.jpg, Width: 1700, Height: 2340\n",
      "Image: 15.jpg, Width: 1600, Height: 1006\n",
      "Image: 17.jpg, Width: 1599, Height: 1230\n",
      "Image: 16.jpg, Width: 1525, Height: 3415\n",
      "Image: 12.jpg, Width: 1276, Height: 1754\n",
      "Image: 13.jpg, Width: 1280, Height: 864\n",
      "Image: 11.jpg, Width: 2828, Height: 1828\n",
      "Image: 10.jpg, Width: 1600, Height: 980\n",
      "Image: 21.jpg, Width: 2048, Height: 1515\n",
      "Image: 20.jpg, Width: 2953, Height: 2088\n",
      "Image: 22.jpg, Width: 500, Height: 256\n",
      "Image: 23.jpg, Width: 325, Height: 300\n",
      "Image: 27.jpg, Width: 591, Height: 629\n",
      "Image: 26.jpg, Width: 600, Height: 374\n",
      "Image: 18.jpg, Width: 1590, Height: 981\n",
      "Image: 24.jpg, Width: 456, Height: 256\n",
      "Image: 25.jpg, Width: 413, Height: 500\n",
      "Image: 19.jpg, Width: 1280, Height: 864\n",
      "Image: 4.jpg, Width: 2048, Height: 1272\n",
      "Image: 5.jpg, Width: 2100, Height: 1760\n",
      "Image: 7.jpg, Width: 1949, Height: 1419\n",
      "Image: 6.jpg, Width: 2048, Height: 1454\n",
      "Image: 2.jpg, Width: 1286, Height: 946\n",
      "Image: 3.jpg, Width: 2048, Height: 1346\n",
      "Image: 1.jpg, Width: 2048, Height: 1251\n",
      "Image: 8.jpg, Width: 1286, Height: 944\n",
      "Image: 9.jpg, Width: 900, Height: 648\n",
      "Image: 4.jpg, Width: 736, Height: 536\n",
      "Image: 5.jpg, Width: 954, Height: 702\n",
      "Image: 7.jpg, Width: 661, Height: 720\n",
      "Image: 6.jpg, Width: 700, Height: 700\n",
      "Image: 2.jpg, Width: 2560, Height: 1680\n",
      "Image: 3.jpg, Width: 1772, Height: 1112\n",
      "Image: 1.jpg, Width: 960, Height: 671\n"
     ]
    }
   ],
   "source": [
    "# Train Images\n",
    "\n",
    "# Iterate over all images in the folder\n",
    "for image_name in os.listdir(train_folder):\n",
    "    if image_name.endswith((\".jpg\")):  # Check for common image extensions\n",
    "        image_path = os.path.join(train_folder, image_name)\n",
    "        \n",
    "        # Read the image using OpenCV\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is not None:\n",
    "            height, width, channels = img.shape  # Get image size (height, width, channels)\n",
    "            print(f\"Image: {image_name}, Width: {width}, Height: {height}\")\n",
    "        else:\n",
    "            print(f\"Could not read image: {image_name}\")\n",
    "\n",
    "# Test Images\n",
    "\n",
    "# Iterate over all images in the folder\n",
    "for image_name in os.listdir(test_folder):\n",
    "    if image_name.endswith((\".jpg\")):  # Check for common image extensions\n",
    "        image_path = os.path.join(test_folder, image_name)\n",
    "        \n",
    "        # Read the image using OpenCV\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is not None:\n",
    "            height, width, channels = img.shape  # Get image size (height, width, channels)\n",
    "            print(f\"Image: {image_name}, Width: {width}, Height: {height}\")\n",
    "        else:\n",
    "            print(f\"Could not read image: {image_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images and annotations resized and saved.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "resized_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train/resized\"\n",
    "resized_annotations_file = \"2024-fall-ml-3-hw-4-wheres-waldo/resized_annotations.csv\"\n",
    "target_size = (1500, 1000)  # Target size for resizing images\n",
    "\n",
    "# Read the annotations CSV file\n",
    "annotations_df = pd.read_csv(annotations_file)\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(resized_folder, exist_ok=True)\n",
    "\n",
    "# List to store updated bounding boxes\n",
    "updated_annotations = []\n",
    "\n",
    "# Iterate over all images in the annotation file\n",
    "for index, row in annotations_df.iterrows():\n",
    "    image_name = row[\"filename\"]\n",
    "    xmin, ymin, xmax, ymax = row[\"xmin\"], row[\"ymin\"], row[\"xmax\"], row[\"ymax\"]\n",
    "    \n",
    "    # Load the image\n",
    "    image_path = os.path.join(train_folder, image_name)\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    if img is not None:\n",
    "        original_height, original_width = img.shape[:2]\n",
    "        \n",
    "        # Calculate the resizing scale factors\n",
    "        scale_x = target_size[0] / original_width\n",
    "        scale_y = target_size[1] / original_height\n",
    "        \n",
    "        # Resize the image\n",
    "        resized_img = cv2.resize(img, target_size)\n",
    "        \n",
    "        # Adjust bounding boxes based on the scaling factors\n",
    "        xmin_new = int(xmin * scale_x)\n",
    "        ymin_new = int(ymin * scale_y)\n",
    "        xmax_new = int(xmax * scale_x)\n",
    "        ymax_new = int(ymax * scale_y)\n",
    "        \n",
    "        # Save the resized image\n",
    "        resized_image_path = os.path.join(resized_folder, image_name)\n",
    "        cv2.imwrite(resized_image_path, resized_img)\n",
    "        \n",
    "        # Add the updated annotation to the list\n",
    "        updated_annotations.append([image_name, xmin_new, ymin_new, xmax_new, ymax_new])\n",
    "\n",
    "# Save the updated annotations to a new CSV file\n",
    "updated_annotations_df = pd.DataFrame(updated_annotations, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "updated_annotations_df.to_csv(resized_annotations_file, index=False)\n",
    "\n",
    "print(\"Images and annotations resized and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented images and annotations saved to 2024-fall-ml-3-hw-4-wheres-waldo/train/train/resized_augmented_images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import random\n",
    "import albumentations as A\n",
    "\n",
    "# Paths\n",
    "augmented_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train/resized_augmented_images\"  # Folder to save augmented images\n",
    "os.makedirs(augmented_folder, exist_ok=True)\n",
    "\n",
    "# Load annotations\n",
    "annotations = pd.read_csv(resized_annotations_file)\n",
    "\n",
    "# Define augmentation pipeline\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.7),  # Randomly flip the image horizontally\n",
    "        A.RandomBrightnessContrast(p=0.7),  # Adjust brightness and contrast\n",
    "        A.Rotate(limit=15, p=0.5),  # Randomly rotate the image\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'])\n",
    ")\n",
    "\n",
    "# Process each image\n",
    "augmented_annotations = []\n",
    "\n",
    "for _, row in annotations.iterrows():\n",
    "    image_name = row['filename']\n",
    "    x_min, y_min, x_max, y_max = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
    "    \n",
    "    # Load image\n",
    "    image_path = os.path.join(resized_folder, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Image {image_path} not found. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Prepare bounding boxes and labels\n",
    "    bboxes = [[x_min, y_min, x_max, y_max]]\n",
    "    class_labels = [\"waldo\"]\n",
    "    \n",
    "    for i in range(5):\n",
    "        # Apply augmentation\n",
    "        augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "        \n",
    "        # Save augmented image\n",
    "        aug_image_name = f\"aug_{random.randint(1000, 9999)}_{image_name}\"\n",
    "        aug_image_path = os.path.join(augmented_folder, aug_image_name)\n",
    "        cv2.imwrite(aug_image_path, augmented['image'])\n",
    "        \n",
    "        # Save augmented bounding boxes\n",
    "        for bbox, label in zip(augmented['bboxes'], augmented['class_labels']):\n",
    "            x_min, y_min, x_max, y_max = bbox  # Bounding boxes are already in Pascal VOC format\n",
    "            augmented_annotations.append([aug_image_name, x_min, y_min, x_max, y_max])\n",
    "\n",
    "# Save augmented annotations to CSV\n",
    "augmented_csv_path = os.path.join(\"2024-fall-ml-3-hw-4-wheres-waldo\", \"resized_augmented_annotations.csv\")\n",
    "augmented_df = pd.DataFrame(augmented_annotations, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "augmented_df.to_csv(augmented_csv_path, index=False)\n",
    "\n",
    "print(f\"Augmented images and annotations saved to {augmented_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw bounding boxes on train images to check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3201_1.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2033_1.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5179_1.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2931_1.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9117_1.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8364_10.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8737_10.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7219_10.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4439_10.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2537_10.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8993_11.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1464_11.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7386_11.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8090_11.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1034_11.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8297_12.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5363_12.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4748_12.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2674_12.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6200_12.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1501_13.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1365_13.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1416_13.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9870_13.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1150_13.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7245_14.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4548_14.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7915_14.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1475_14.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9644_14.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4632_15.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8174_15.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9123_15.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4818_15.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6663_15.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4782_17.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4584_17.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8530_17.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5747_17.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1352_17.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7818_18.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2638_18.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4045_18.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4045_18.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5856_18.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2980_18.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6450_19.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9205_19.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7915_19.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9318_19.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4110_19.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5970_2.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5655_2.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9181_2.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9278_2.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7444_2.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1565_3.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8868_3.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4977_3.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7623_3.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7788_3.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3834_4.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7014_4.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9991_4.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7139_4.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2416_4.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8191_5.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9330_5.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2768_5.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3682_5.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9535_5.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7443_6.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7070_6.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9023_6.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1484_6.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8689_6.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1712_7.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6054_7.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7448_7.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3791_7.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3762_7.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9228_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4718_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1201_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4268_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9841_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9983_9.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4803_9.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7626_9.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9417_9.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6633_9.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6788_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8522_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8522_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5411_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9978_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1093_8.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7286_10.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9396_10.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3117_10.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9498_10.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4366_10.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7981_16.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7981_16.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1919_16.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8882_16.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6975_16.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6975_16.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4274_16.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9269_20.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7773_20.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8945_20.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6845_20.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7789_20.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6670_21.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1025_21.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9822_21.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9849_21.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6425_21.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8506_22.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1458_22.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1458_22.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4761_22.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4761_22.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3903_22.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3961_22.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2500_23.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5182_23.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1531_23.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2154_23.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2363_23.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1273_24.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1273_24.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8421_24.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8421_24.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1238_24.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1238_24.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5607_24.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5607_24.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5088_24.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5088_24.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5401_25.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5401_25.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2793_25.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4024_25.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4024_25.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6643_25.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6643_25.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5756_25.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5756_25.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2138_26.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3743_26.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3615_26.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3615_26.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5181_26.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9640_26.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3754_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5471_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5824_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8449_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8449_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6275_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9134_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8762_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2870_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2870_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1387_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6111_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6111_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7333_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_6625_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7896_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4080_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5233_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_2781_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_5152_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9357_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4425_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8072_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_4692_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1292_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1292_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7509_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3399_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_1578_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_3625_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_8301_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_9295_27.jpg\n",
      "Annotated image saved to 2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images/aug_7990_27.jpg\n",
      "All bounding boxes have been drawn and saved.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "output_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/annotated_resized_augmented_images\"  # Folder to save images with drawn boxes\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Read the CSV file\n",
    "# Assumes the CSV columns are: filename, xmin, ymin, xmax, ymax\n",
    "annotations = pd.read_csv(augmented_csv_path)\n",
    "\n",
    "# Iterate through each image in the annotations\n",
    "for _, row in annotations.iterrows():\n",
    "    image_name = row[\"filename\"]\n",
    "    x_min, y_min, x_max, y_max = row[\"xmin\"], row[\"ymin\"], row[\"xmax\"], row[\"ymax\"]\n",
    "    \n",
    "    # Load the image\n",
    "    image_path = os.path.join(augmented_folder, image_name)\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image {image_path} not found. Skipping...\")\n",
    "        continue\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Draw the bounding box\n",
    "    # cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (B, G, R), thickness)\n",
    "    cv2.rectangle(image, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 4)\n",
    "    \n",
    "    # Optionally, add a label or text\n",
    "    label = \"Waldo\"\n",
    "    cv2.putText(image, label, (int(x_min), int(y_min) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # Save the image\n",
    "    output_path = os.path.join(output_folder, image_name)\n",
    "    cv2.imwrite(output_path, image)\n",
    "\n",
    "    print(f\"Annotated image saved to {output_path}\")\n",
    "\n",
    "print(\"All bounding boxes have been drawn and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (for model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "yolo_train_dir = \"datasets/yolo_dataset/train\"\n",
    "yolo_val_dir = \"datasets/yolo_dataset/val\"\n",
    "\n",
    "#Saved Predictions\n",
    "yolo_test_dir = \"yolo_test_predictions\"\n",
    "\n",
    "# Create necessary folders\n",
    "os.makedirs(yolo_train_dir, exist_ok=True)\n",
    "os.makedirs(yolo_val_dir, exist_ok=True)\n",
    "os.makedirs(yolo_test_dir, exist_ok=True)\n",
    "\n",
    "# Load annotations\n",
    "annotations = pd.read_csv(augmented_csv_path)\n",
    "\n",
    "# Function to convert annotations to YOLO format\n",
    "def convert_to_yolo_format(row, img_width, img_height):\n",
    "    x_center = (row[\"xmin\"] + row[\"xmax\"]) / 2 / img_width\n",
    "    y_center = (row[\"ymin\"] + row[\"ymax\"]) / 2 / img_height\n",
    "    width = (row[\"xmax\"] - row[\"xmin\"]) / img_width\n",
    "    height = (row[\"ymax\"] - row[\"ymin\"]) / img_height\n",
    "    return f\"0 {x_center} {y_center} {width} {height}\"\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "image_files = annotations[\"filename\"].unique()\n",
    "train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to prepare YOLO format data\n",
    "def prepare_yolo_data(image_list, output_dir):\n",
    "    for img_name in image_list:\n",
    "        img_path = os.path.join(augmented_folder, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img_height, img_width, _ = img.shape\n",
    "\n",
    "        # Filter annotations for this image\n",
    "        image_annotations = annotations[annotations[\"filename\"] == img_name]\n",
    "\n",
    "        # YOLO annotations file\n",
    "        yolo_annotations = []\n",
    "        for _, row in image_annotations.iterrows():\n",
    "            yolo_line = convert_to_yolo_format(row, img_width, img_height)\n",
    "            yolo_annotations.append(yolo_line)\n",
    "\n",
    "        # Save image and annotation\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        shutil.copy(img_path, os.path.join(output_dir, f\"{base_name}.jpg\"))\n",
    "        with open(os.path.join(output_dir, f\"{base_name}.txt\"), \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_annotations))\n",
    "\n",
    "# Prepare training and validation data\n",
    "prepare_yolo_data(train_images, yolo_train_dir)\n",
    "prepare_yolo_data(val_images, yolo_val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv_by_column(input_csv, output_csv, column_name, values_list):\n",
    "    \"\"\"\n",
    "    Filters rows in a CSV file and keeps only those where the specified column's value is in a given list.\n",
    "\n",
    "    Parameters:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to save the filtered CSV file.\n",
    "        column_name (str): Column to filter on.\n",
    "        values_list (list): List of values to keep.\n",
    "    \"\"\"\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df[column_name].isin(values_list)]\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    filtered_df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split annotations into train and val\n",
    "values_list = []\n",
    "directory = \"datasets/yolo_dataset/train\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.jpg'):\n",
    "        values_list.append(filename)\n",
    "\n",
    "# Example usage\n",
    "input_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/resized_augmented_annotations.csv\"  # Replace with your input file path\n",
    "output_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\"  # Replace with your output file path\n",
    "column_name = \"filename\"  # Replace with the column you want to filter\n",
    "\n",
    "filter_csv_by_column(input_csv, output_csv, column_name, values_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "values_list = []\n",
    "directory = \"datasets/yolo_dataset/val\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.jpg'):\n",
    "        values_list.append(filename)\n",
    "\n",
    "# Example usage\n",
    "input_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/resized_augmented_annotations.csv\"  # Replace with your input file path\n",
    "output_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\"  # Replace with your output file path\n",
    "column_name = \"filename\"  # Replace with the column you want to filter\n",
    "\n",
    "filter_csv_by_column(input_csv, output_csv, column_name, values_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "class WaldoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = F.to_tensor(image)\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Read bounding box data, ensuring all are converted to float\n",
    "        box_data = self.img_labels.iloc[idx, 1:5].values\n",
    "        boxes = []\n",
    "        for item in box_data:\n",
    "            try:\n",
    "                boxes.append(float(item))\n",
    "            except ValueError as e:\n",
    "                raise ValueError(f\"Error converting bounding box data to float: {e}\")\n",
    "\n",
    "        # Create tensors\n",
    "        boxes = torch.as_tensor([boxes], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        image, target = self.transform(image, target)\n",
    "\n",
    "        target = F.to_tensor(target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Create the dataset\n",
    "train_dataset = WaldoDataset(annotations_file= \"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\", img_dir=\"datasets/yolo_dataset/train\")\n",
    "test_dataset = WaldoDataset(annotations_file= \"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\", img_dir=\"datasets/yolo_dataset/val\")\n",
    "\n",
    "# Now, you can use this dataset with a DataLoader to train your model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    #collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    #collate_fn=lambda x: tuple(zip(*x))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 750, 500])\n",
      "torch.Size([1, 18, 750, 500])\n",
      "torch.Size([1, 18, 750, 500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleYOLOv3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleYOLOv3, self).__init__()\n",
    "\n",
    "        # Backbone: Feature extractor (simplified)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Add more convolutional layers as needed\n",
    "        )\n",
    "\n",
    "        # Detection head (simplified)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, (5 + num_classes) * 3, 1),  # 3 bounding boxes per cell\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        print(x.size())\n",
    "        x = self.head(x)\n",
    "        print(x.size())\n",
    "        return x \n",
    "\n",
    "# Instantiate and check the model\n",
    "model = SimpleYOLOv3(num_classes=1)\n",
    "input_image = torch.randn(1, 3, 1500, 1000)  # Example batch\n",
    "output = model(input_image)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Compose.__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     evaluate(model, test_loader, device)\n",
      "Cell \u001b[0;32mIn[78], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     34\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape())\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[77], line 46\u001b[0m, in \u001b[0;36mWaldoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m area\n\u001b[1;32m     44\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miscrowd\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m iscrowd\n\u001b[0;32m---> 46\u001b[0m image, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m target \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mto_tensor(target)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "\u001b[0;31mTypeError\u001b[0m: Compose.__call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "model = SimpleYOLOv3(num_classes=1)\n",
    "\n",
    "# IoU calculation\n",
    "def compute_iou(pred_boxes, true_boxes):\n",
    "    # pred_boxes and true_boxes should be in (x_min, y_min, x_max, y_max)\n",
    "    inter_xmin = torch.max(pred_boxes[:, 0], true_boxes[:, 0])\n",
    "    inter_ymin = torch.max(pred_boxes[:, 1], true_boxes[:, 1])\n",
    "    inter_xmax = torch.min(pred_boxes[:, 2], true_boxes[:, 2])\n",
    "    inter_ymax = torch.min(pred_boxes[:, 3], true_boxes[:, 3])\n",
    "\n",
    "    inter_area = torch.clamp(inter_xmax - inter_xmin, min=0) * torch.clamp(inter_ymax - inter_ymin, min=0)\n",
    "    pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "    true_area = (true_boxes[:, 2] - true_boxes[:, 0]) * (true_boxes[:, 3] - true_boxes[:, 1])\n",
    "\n",
    "    union_area = pred_area + true_area - inter_area\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "# Simple IoU loss function\n",
    "def iou_loss(pred_boxes, true_boxes):\n",
    "    iou = compute_iou(pred_boxes, true_boxes)\n",
    "    return 1 - iou.mean()  # We want to maximize IoU, so minimize 1 - IoU\n",
    "\n",
    "# Custom YOLOv3 training loop\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        print(images.shape())\n",
    "        print(images)\n",
    "        print(targets.shape())\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Extract predicted boxes and target boxes (for simplicity, assuming one grid cell)\n",
    "        pred_boxes = predictions[:, :4]  # first 4 are bounding box coordinates\n",
    "        pred_conf = predictions[:, 4]    # 5th is objectness confidence\n",
    "        pred_class = predictions[:, 5:]  # remaining are class predictions\n",
    "\n",
    "        true_boxes = targets[:, :4]  # Ground truth boxes\n",
    "        true_conf = targets[:, 4]    # Objectness confidence\n",
    "        true_class = targets[:, 5:]  # Ground truth class\n",
    "\n",
    "        # Losses\n",
    "        loss_loc = iou_loss(pred_boxes, true_boxes)  # IoU loss\n",
    "        loss_conf = torch.nn.BCEWithLogitsLoss()(pred_conf, true_conf)  # Confidence loss\n",
    "        loss_class = torch.nn.BCEWithLogitsLoss()(pred_class, true_class)  # Classification loss\n",
    "\n",
    "        # Total loss (sum or weighted sum)\n",
    "        loss = loss_loc + loss_conf + loss_class\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training loss: {avg_loss}\")\n",
    "\n",
    "# Evaluation (testing) function\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Extract predicted boxes and target boxes\n",
    "            pred_boxes = predictions[:, :4]\n",
    "            true_boxes = targets[:, :4]\n",
    "\n",
    "            # Calculate IoU for the batch\n",
    "            iou = compute_iou(pred_boxes, true_boxes)\n",
    "            total_iou += iou.mean().item()\n",
    "\n",
    "    avg_iou = total_iou / len(test_loader)\n",
    "    print(f\"Average IoU on test set: {avg_iou}\")\n",
    "\n",
    "# Initialize model, optimizer, and device\n",
    "model = SimpleYOLOv3(num_classes=1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train(model, train_loader, optimizer, device)\n",
    "    evaluate(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO model\n",
    "#model = YOLO(\"yolov5su.pt\")  # Load pretrained weights\n",
    "#model.train(data=\"yolo.yaml\", epochs=15, imgsz=640, pretrained=True, augment=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 210.5ms\n",
      "1: 640x640 (no detections), 210.5ms\n",
      "2: 640x640 (no detections), 210.5ms\n",
      "3: 640x640 (no detections), 210.5ms\n",
      "4: 640x640 (no detections), 210.5ms\n",
      "5: 640x640 (no detections), 210.5ms\n",
      "6: 640x640 (no detections), 210.5ms\n",
      "7: 640x640 (no detections), 210.5ms\n",
      "8: 640x640 (no detections), 210.5ms\n",
      "Speed: 1.5ms preprocess, 210.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolo_test_predictions/train2\u001b[0m\n",
      "0 label saved to yolo_test_predictions/train2/labels\n",
      "Predictions saved to yolo_test_predictions/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\"\n",
    "\n",
    "# Predict on test images\n",
    "test_images = [os.path.join(test_folder, img) for img in os.listdir(test_folder) if img.endswith(\".jpg\")]\n",
    "results = model.predict(source=test_images, save=True, save_txt=True, project=\"yolo_test_predictions\")\n",
    "\n",
    "# Prepare to save the predictions\n",
    "output_csv_path = os.path.join(\"yolo_test_predictions\", \"predictions.csv\")\n",
    "predictions = []\n",
    "\n",
    "# Process results\n",
    "for result in results:\n",
    "    image_name = os.path.basename(result.path)  # Get the image name\n",
    "    if result.boxes is not None and len(result.boxes) > 0:  # Check if there are predictions\n",
    "        # Convert result.boxes to tensor for easier access\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()  # Convert bounding boxes to array\n",
    "        confidences = result.boxes.conf.cpu().numpy()  # Convert confidence scores to array\n",
    "\n",
    "        # Find the index of the box with the highest confidence\n",
    "        best_idx = confidences.argmax()\n",
    "        best_box = boxes[best_idx]\n",
    "        conf = confidences[best_idx]\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        x_min, y_min, x_max, y_max = best_box\n",
    "        predictions.append([image_name, x_min, y_min, x_max, y_max, conf])\n",
    "    else:\n",
    "        # No predictions for this image\n",
    "        predictions.append([image_name, None, None, None, None, None])\n",
    "\n",
    "# Save predictions to CSV\n",
    "df = pd.DataFrame(predictions, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"confidence\"])\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
