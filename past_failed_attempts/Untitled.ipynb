{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0335ad2f-144f-4542-9ff5-215d13846d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd9aa768-0ad8-4708-ae18-dd57ead10659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = device = torch.device(\"cuda\") #mps/cuda\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d54729-cc43-4b46-9964-7909cf4d4791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train\" # Original Train Images\n",
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\" # Original Test Images\n",
    "annotations_file = \"2024-fall-ml-3-hw-4-wheres-waldo/annotations.csv\" # Original Annotations File\n",
    "image_sz = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d556ea6-b893-46af-a18d-b0e5384be368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataset (Train and Test Loaders)\n",
    "class WaldoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transforms=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        #image = torchvision.transforms.ToTensor()(image)  # Convert to tensor\n",
    "        \n",
    "        box_data = self.img_labels.iloc[idx, 1:].values\n",
    "        boxes = [float(item) for item in box_data]\n",
    "        \n",
    "        boxes = torch.as_tensor([boxes], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Set up the dataset and data loaders\n",
    "train_dataset = WaldoDataset(\n",
    "    annotations_file=\"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\",\n",
    "    img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/chunks\",\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((image_sz, image_sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_dataset = WaldoDataset(\n",
    "    annotations_file=\"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\",\n",
    "    img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/val\",\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((image_sz, image_sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: list(zip(*x))\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: list(zip(*x))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f4cafc-6531-47df-a4db-ddee289dd75b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box predictions:  tensor([[ 0.0912, -0.0641,  0.0536, -0.0539]], grad_fn=<AddmmBackward0>)\n",
      "Loss:  132.49331665039062\n",
      "Bounding box predictions (inference):  tensor([[ 0.0912, -0.0641,  0.0536, -0.0539]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleBBoxModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleBBoxModel, self).__init__()\n",
    "        # Define a simple CNN architecture\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Define a fully connected layer to output 4 values for the bounding box\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 128)  # Flattening 64 channels of 64x64 feature maps\n",
    "        self.fc2 = nn.Linear(128, 4)  # Outputting the 4 bounding box coordinates\n",
    "\n",
    "        # Define loss function (L1 Loss for bounding box regression)\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # Pass the input through the convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Pooling to reduce the spatial dimensions\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)  # Pooling again\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)  # Final pooling\n",
    "        \n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        bbox = self.fc2(x)  # Output 4 values for the bounding box\n",
    "        \n",
    "        # Compute loss if targets are provided\n",
    "        if targets is not None:\n",
    "            loss = self.criterion(bbox, targets)\n",
    "            return bbox, loss\n",
    "        \n",
    "        return bbox\n",
    "\n",
    "# Example usage:\n",
    "model = SimpleBBoxModel()\n",
    "\n",
    "# Input a batch of 512x512 RGB images (batch_size=1)\n",
    "images = torch.randn(1, 3, 512, 512)  # Random images for demonstration\n",
    "targets = torch.tensor([[50.0, 60.0, 200.0, 220.0]])  # Example target bounding box for batch\n",
    "\n",
    "# Forward pass with targets (training mode)\n",
    "output, loss = model(images, targets)\n",
    "print(\"Bounding box predictions: \", output)\n",
    "print(\"Loss: \", loss.item())\n",
    "\n",
    "# Forward pass without targets (inference mode)\n",
    "output = model(images)\n",
    "print(\"Bounding box predictions (inference): \", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4110dfda-651c-4cf0-90d9-6d7d8f80bced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleBBoxModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 113\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete. Best Validation IoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Set up optimizer, scheduler, and device\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2).to(device)\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleBBoxModel\u001b[49m()\n\u001b[1;32m    114\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m    115\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleBBoxModel' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "# Define IoU computation\n",
    "def calculate_iou(pred_boxes, target_boxes):\n",
    "    \"\"\"\n",
    "    Computes IoU between predicted and target boxes.\n",
    "    Args:\n",
    "        pred_boxes (Tensor): Predicted boxes, shape (N, 4).\n",
    "        target_boxes (Tensor): Target boxes, shape (M, 4).\n",
    "    Returns:\n",
    "        IoU scores: Tensor of shape (N, M).\n",
    "    \"\"\"\n",
    "    return ops.box_iou(pred_boxes, target_boxes)\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = torch.stack([img.to(device) for img in images])\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        losses = model(images, targets)\n",
    "\n",
    "        # Ensure that losses is a dictionary and sum all loss components\n",
    "        if isinstance(losses, dict):\n",
    "            total_loss = sum(loss for loss in losses.values())\n",
    "        else:\n",
    "            raise ValueError(\"Expected losses to be a dictionary\")\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_iou = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Validation\", leave=False):\n",
    "            # Move data to device\n",
    "            images = torch.stack([img.to(device) for img in images])\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = model(images)  # During evaluation, model outputs predictions\n",
    "            \n",
    "            # Compute IoU between predicted and target boxes\n",
    "            for output, target in zip(outputs, targets):\n",
    "                pred_boxes = output[\"boxes\"].detach()\n",
    "                target_boxes = target[\"boxes\"]\n",
    "\n",
    "                if len(pred_boxes) > 0 and len(target_boxes) > 0:\n",
    "                    iou = box_iou(pred_boxes, target_boxes).mean().item()\n",
    "                    total_iou += iou\n",
    "                else:\n",
    "                    total_iou += 0  # No predictions or no targets\n",
    "\n",
    "                num_samples += 1\n",
    "\n",
    "    # Calculate average IoU\n",
    "    avg_iou = total_iou / num_samples if num_samples > 0 else 0.0\n",
    "\n",
    "    return avg_iou\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, optimizer, scheduler, checkpoint_path):\n",
    "    best_iou = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train the model\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "\n",
    "        # Validate the model\n",
    "        val_iou = evaluate(model, val_loader, device)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_iou > best_iou:\n",
    "            best_iou = val_iou\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model saved with IoU: {best_iou:.4f}\")\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation IoU: {val_iou:.4f}\")\n",
    "\n",
    "    print(f\"Training complete. Best Validation IoU: {best_iou:.4f}\")\n",
    "\n",
    "\n",
    "# Set up optimizer, scheduler, and device\n",
    "#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2).to(device)\n",
    "model = SimpleBBoxModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "checkpoint_path = \"best_model.pth\"\n",
    "num_epochs = 10\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    val_data_loader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    checkpoint_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa928c95-c14d-49d0-b87b-75b54db33e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
