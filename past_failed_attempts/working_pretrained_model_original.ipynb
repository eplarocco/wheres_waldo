{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Wheres Waldo?\n",
    "### Name: Eileanor LaRocco\n",
    "In this assignment, you will develop an object detection algorithm to locate Waldo in a set of images. You will develop a model to detect the bounding box around Waldo. Your final task is to submit your predictions on Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = device = torch.device(\"cuda\") #mps/cuda\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train\" # Original Train Images\n",
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\" # Original Test Images\n",
    "annotations_file = \"2024-fall-ml-3-hw-4-wheres-waldo/annotations.csv\" # Original Annotations File\n",
    "image_sz = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataset (Train and Test Loaders)\n",
    "class WaldoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transforms=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        #image = torchvision.transforms.ToTensor()(image)  # Convert to tensor\n",
    "        \n",
    "        box_data = self.img_labels.iloc[idx, 1:].values\n",
    "        boxes = [float(item) for item in box_data]\n",
    "        \n",
    "        boxes = torch.as_tensor([boxes], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Set up the dataset and data loaders\n",
    "train_dataset = WaldoDataset(\n",
    "    annotations_file=\"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\",\n",
    "    img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/chunks\",\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((image_sz, image_sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_dataset = WaldoDataset(\n",
    "    annotations_file=\"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\",\n",
    "    img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/val\",\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((image_sz, image_sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: list(zip(*x))\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: list(zip(*x))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[-1.0567e-01,  2.1472e-03,  1.9529e-02,  4.7458e-03],\n",
      "        [-1.3404e-01,  6.8100e-03,  6.6289e-03,  5.6924e-03],\n",
      "        [-5.4605e-02,  5.4930e-02, -6.4145e-02,  2.5382e-02],\n",
      "        [-3.8036e-02, -3.7361e-02, -2.1742e-02, -6.5491e-02],\n",
      "        [-1.9463e-02, -2.1646e-02, -4.8179e-02,  2.9291e-02],\n",
      "        [ 8.7323e-04, -2.7896e-02, -1.4111e-02,  1.4439e-02],\n",
      "        [-3.3448e-02, -1.6926e-02, -5.6510e-02,  1.4538e-02],\n",
      "        [ 1.4941e-04, -3.2483e-02,  1.9012e-02,  3.6338e-03],\n",
      "        [-5.9636e-03, -2.6276e-02,  1.2019e-02, -1.1427e-02],\n",
      "        [-4.2636e-02, -2.7052e-02, -1.3361e-02, -1.3018e-02],\n",
      "        [-6.2829e-02,  6.5848e-02, -3.7687e-02, -1.4809e-02],\n",
      "        [ 3.1937e-02, -5.4429e-02, -7.1939e-02, -3.7604e-02],\n",
      "        [ 7.3745e-03, -1.6996e-02,  5.8366e-03,  3.6775e-02],\n",
      "        [ 1.3459e-02, -2.0028e-02, -1.9076e-02,  1.8829e-03],\n",
      "        [-7.8496e-03, -7.0453e-03,  2.3613e-02, -5.9791e-04],\n",
      "        [-1.6687e-01,  2.1340e-02,  2.9041e-02,  3.8045e-02],\n",
      "        [-5.9372e-02, -1.8786e-02, -5.7593e-03,  8.9460e-02],\n",
      "        [-5.1852e-02, -8.1759e-02,  2.3682e-02, -9.5677e-02],\n",
      "        [-4.9379e-02, -2.4662e-02, -6.6212e-03, -3.1205e-02],\n",
      "        [ 3.6286e-03,  1.4262e-02, -4.0978e-02,  1.4058e-02],\n",
      "        [-2.5838e-02, -2.7247e-02, -3.0205e-03, -1.4133e-02],\n",
      "        [ 3.2424e-03, -6.7760e-03,  1.6039e-02, -2.5203e-02],\n",
      "        [-4.5722e-02, -2.9312e-02, -3.2407e-02,  3.8650e-02],\n",
      "        [-1.2554e-02, -5.8459e-02,  3.7687e-03, -2.7166e-02],\n",
      "        [ 2.0845e-03, -3.7772e-04,  3.7615e-03, -6.2411e-02],\n",
      "        [-7.3484e-02, -5.1655e-02, -8.6060e-03, -4.2829e-02],\n",
      "        [-2.6215e-02, -2.4290e-02, -4.2565e-02,  1.3194e-02],\n",
      "        [-6.6899e-03,  1.0982e-02, -1.8050e-03,  3.9907e-02],\n",
      "        [-1.1217e-02, -2.9633e-02,  2.7276e-02, -3.3670e-02],\n",
      "        [ 5.1540e-03,  1.0309e-03, -1.4613e-02,  2.3026e-02],\n",
      "        [-4.4781e-02, -2.0443e-02, -1.0905e-02,  5.6715e-02],\n",
      "        [-2.6027e-02, -4.8190e-02,  8.4596e-03, -6.0966e-02],\n",
      "        [-1.2010e-02, -1.2129e-02,  1.4569e-02, -4.6900e-02],\n",
      "        [-3.2178e-02, -5.1561e-02,  3.6269e-02,  2.5929e-02],\n",
      "        [-6.2347e-02, -5.7879e-02, -1.1817e-02, -6.5671e-03],\n",
      "        [ 8.1589e-03, -3.2298e-02,  3.1846e-02, -3.4855e-02],\n",
      "        [-4.1976e-02, -6.3394e-02, -3.3571e-02,  1.3302e-02],\n",
      "        [-1.7721e-02, -1.2301e-02, -2.7934e-02,  1.6670e-02],\n",
      "        [ 3.2661e-02, -2.5512e-02, -6.3868e-02,  9.6209e-03],\n",
      "        [-1.8666e-02, -6.3846e-02,  1.8144e-02, -7.1348e-03],\n",
      "        [-4.8560e-02, -7.4423e-02,  8.3395e-03, -2.6365e-02],\n",
      "        [-4.9517e-02,  4.8997e-02, -1.0568e-02,  5.1729e-02],\n",
      "        [-1.2275e-02, -2.8957e-02,  1.0721e-02, -6.3298e-02],\n",
      "        [-1.3901e-02, -1.9483e-02, -6.3268e-03,  2.5592e-02],\n",
      "        [ 4.5525e-02,  7.5399e-03,  1.2403e-02,  3.3517e-02],\n",
      "        [-4.7658e-02, -1.4970e-02,  2.4875e-02,  2.2459e-02],\n",
      "        [-2.2263e-02, -2.6465e-02,  1.7036e-02, -3.9791e-02],\n",
      "        [-1.4786e-01,  9.4193e-03,  3.4853e-02,  3.2954e-02],\n",
      "        [-3.5862e-02, -4.6515e-02, -1.2868e-02, -4.3624e-02],\n",
      "        [ 3.2031e-02,  2.8918e-03, -2.8293e-02,  1.0870e-04],\n",
      "        [-2.2626e-02, -3.0458e-02,  5.1740e-02, -5.5702e-02],\n",
      "        [-4.4411e-02, -2.1111e-02, -1.9502e-02,  1.6185e-02],\n",
      "        [-1.7622e-02, -1.6653e-02,  3.7953e-02,  2.2953e-02],\n",
      "        [-5.6063e-02, -6.6398e-02, -2.2692e-02,  1.4287e-02],\n",
      "        [-2.0545e-02, -6.3583e-05,  2.3252e-03,  3.4202e-02],\n",
      "        [ 1.3996e-02, -2.4149e-02, -4.6568e-03,  1.9590e-02],\n",
      "        [ 2.4764e-02, -3.0829e-02, -6.6888e-03,  2.4485e-02],\n",
      "        [ 1.7124e-02,  9.5621e-03,  2.4685e-02,  3.1759e-02],\n",
      "        [-5.1770e-02, -1.0188e-02,  1.7975e-02, -9.5850e-02],\n",
      "        [-1.2666e-02, -2.0102e-02,  1.9964e-02,  4.3379e-02],\n",
      "        [-6.2586e-02, -1.8138e-02,  1.2072e-02,  4.5356e-03],\n",
      "        [ 6.1630e-02, -3.6167e-02, -3.3916e-02,  3.0374e-02],\n",
      "        [-1.5314e-02, -4.5398e-02, -5.9121e-03,  6.8438e-03],\n",
      "        [-7.4909e-02, -1.4811e-02,  8.4743e-03,  6.9587e-03],\n",
      "        [ 1.6435e-02, -1.2691e-02,  1.7742e-02,  5.6641e-02],\n",
      "        [-5.0805e-02, -6.2700e-02,  3.1012e-02, -4.5063e-02],\n",
      "        [ 5.1174e-02, -1.3323e-02, -3.9746e-02,  9.0785e-04],\n",
      "        [-2.6869e-02,  1.1227e-02, -3.9048e-02,  5.5053e-02],\n",
      "        [ 4.4444e-03, -2.1749e-02, -6.8101e-03,  5.5392e-03],\n",
      "        [-2.0080e-02, -1.8604e-02,  1.5910e-02, -7.0029e-02],\n",
      "        [ 1.5871e-02, -3.8347e-02, -1.3379e-02,  1.9104e-02],\n",
      "        [-1.0946e-02, -8.2877e-04, -1.5546e-02,  1.9381e-02],\n",
      "        [-3.3679e-02, -6.2874e-02,  2.7890e-02,  3.1648e-02],\n",
      "        [ 4.2818e-02, -1.9091e-02,  9.9991e-03,  1.2266e-02],\n",
      "        [-2.0561e-02, -2.1409e-02,  1.2107e-02, -5.3051e-02],\n",
      "        [ 3.0096e-02,  2.9382e-03,  7.6312e-03,  3.5080e-02],\n",
      "        [ 2.3565e-03, -2.1199e-02,  1.4387e-02, -7.2549e-02],\n",
      "        [ 5.1868e-02, -1.6866e-03,  1.6131e-02,  5.7865e-02],\n",
      "        [-5.1384e-02, -2.5421e-02,  8.0661e-03,  8.6677e-03],\n",
      "        [-6.3624e-02, -1.7634e-02,  1.2980e-02, -3.1686e-03],\n",
      "        [ 9.2765e-03,  1.0967e-03,  1.1018e-02, -6.1375e-02],\n",
      "        [-2.9228e-02, -3.6801e-02,  8.6098e-03,  2.0115e-03],\n",
      "        [ 1.2216e-02, -1.9342e-02,  3.1495e-03,  2.0742e-02],\n",
      "        [-1.3743e-02,  1.6021e-02,  3.2536e-02, -4.5844e-03],\n",
      "        [-6.0897e-03, -2.2000e-02,  4.7624e-02,  6.6468e-05],\n",
      "        [-3.9412e-02,  4.3144e-03,  2.7911e-02, -3.3085e-02],\n",
      "        [ 5.4041e-03, -3.0889e-02, -3.3871e-02, -6.1368e-02],\n",
      "        [-2.1409e-02, -1.8368e-02,  3.1042e-02, -1.0605e-02],\n",
      "        [-6.4368e-03, -1.7004e-02,  3.6140e-03,  4.8875e-02],\n",
      "        [ 1.6256e-02,  1.8081e-03, -1.2805e-02,  6.0352e-02],\n",
      "        [-4.2900e-02,  1.8644e-02, -8.7415e-03,  2.4295e-02],\n",
      "        [-1.6303e-02, -6.6215e-02, -1.0089e-01,  1.7265e-03],\n",
      "        [ 2.4216e-02, -1.4748e-02,  4.4572e-03, -8.7558e-04],\n",
      "        [ 2.5487e-03, -5.2317e-02, -1.7415e-02, -8.0498e-03],\n",
      "        [-4.4180e-02,  5.4324e-03,  1.4729e-03, -1.9562e-02],\n",
      "        [-4.7632e-02, -5.1425e-02, -6.4357e-03, -5.1595e-02],\n",
      "        [ 2.0316e-02, -4.8403e-02, -7.3742e-02, -3.8000e-02],\n",
      "        [-1.8366e-02, -3.2444e-02, -3.7493e-04, -4.9907e-02],\n",
      "        [-9.5489e-03,  1.0832e-02,  1.4504e-02,  6.0300e-02],\n",
      "        [-3.7633e-02, -1.3516e-02,  4.7926e-02,  2.5052e-02],\n",
      "        [-3.3011e-02, -1.5443e-03,  5.8933e-03, -1.5822e-02],\n",
      "        [-7.0655e-02, -4.4874e-02,  1.8305e-02, -9.9592e-02],\n",
      "        [ 3.3243e-02, -1.5083e-02,  9.4451e-03,  8.7013e-03],\n",
      "        [ 4.3840e-02, -1.9975e-02,  1.8862e-02,  2.7634e-02],\n",
      "        [ 3.7667e-03,  1.4864e-02,  3.2953e-02,  1.8608e-02],\n",
      "        [ 4.8076e-03, -1.5659e-02, -3.0838e-03,  3.2055e-02],\n",
      "        [ 1.2565e-02, -7.5984e-03,  2.6373e-02,  1.4420e-02],\n",
      "        [-9.1059e-02,  1.7017e-02, -2.1005e-02,  2.3293e-02],\n",
      "        [-3.8748e-02,  4.1444e-02,  1.2153e-02, -2.6289e-02],\n",
      "        [-2.6365e-02, -2.1208e-02, -2.4379e-03,  4.8276e-02],\n",
      "        [-3.8768e-02, -1.9227e-02, -1.2173e-02, -7.3011e-02],\n",
      "        [ 3.4391e-02, -2.0224e-02, -1.6760e-02,  9.6518e-03],\n",
      "        [-2.4610e-02, -1.9569e-02, -4.2571e-03, -1.1189e-02],\n",
      "        [-7.6530e-02, -1.5139e-02, -3.3961e-03,  4.1154e-02],\n",
      "        [-4.1764e-02, -9.9794e-03,  1.5297e-02,  6.2359e-02],\n",
      "        [-3.1456e-02, -7.0170e-02, -8.2334e-03, -2.3300e-02],\n",
      "        [ 4.2460e-02, -1.8710e-02,  3.3240e-02,  3.0377e-02],\n",
      "        [-1.3063e-01, -4.9208e-02,  2.6536e-02,  1.7565e-02],\n",
      "        [-9.3963e-02, -4.4568e-02,  1.3592e-02, -9.4157e-02],\n",
      "        [ 6.1988e-03, -3.6780e-02, -1.4165e-02,  8.6178e-03],\n",
      "        [-1.1717e-02,  2.2208e-02,  7.4337e-03,  5.8961e-02],\n",
      "        [ 3.5257e-02, -2.1338e-02,  1.7761e-02,  3.0003e-02],\n",
      "        [ 5.2743e-03, -3.6759e-04,  4.1754e-02, -4.2948e-03],\n",
      "        [-4.6225e-03, -1.2678e-02, -2.7564e-02,  1.8251e-02],\n",
      "        [-8.4931e-02, -3.8963e-02,  4.0418e-03,  4.8592e-03],\n",
      "        [-6.5842e-03, -4.7609e-02, -7.1841e-03,  1.3963e-02],\n",
      "        [-1.6552e-02,  6.4575e-03,  2.7739e-02,  3.1001e-02],\n",
      "        [ 9.3794e-03, -7.3208e-03, -8.7025e-03,  7.8104e-03],\n",
      "        [-8.7972e-03, -3.2064e-02,  1.3647e-02, -5.4403e-02],\n",
      "        [-1.4927e-02, -4.5893e-03, -1.8966e-02,  9.0864e-03],\n",
      "        [ 2.2346e-02, -3.8483e-02,  3.7509e-02, -5.2291e-02],\n",
      "        [ 1.1021e-02, -1.2868e-02,  1.9463e-02,  3.0771e-02],\n",
      "        [-4.1974e-02, -1.6101e-05,  2.2990e-02,  6.5056e-02],\n",
      "        [-6.1175e-03, -8.4303e-03,  7.5419e-03,  1.6325e-02],\n",
      "        [-1.9006e-02, -7.5574e-03,  5.0561e-02,  3.7551e-02],\n",
      "        [-2.2142e-02, -2.1285e-02, -1.4440e-02, -5.7745e-02],\n",
      "        [-7.5797e-02, -1.8842e-02, -8.6344e-03,  2.6197e-02],\n",
      "        [ 2.2007e-02, -1.8663e-02, -6.5232e-03,  2.2192e-02],\n",
      "        [-4.6087e-02,  1.8495e-02,  1.1605e-02,  3.0467e-02],\n",
      "        [-9.7570e-02,  3.4217e-02,  3.5134e-03, -3.2615e-02],\n",
      "        [ 2.1802e-02, -2.3152e-02,  6.9012e-03,  2.8706e-02],\n",
      "        [ 1.1740e-03, -3.4786e-02, -2.2011e-02,  3.6686e-02],\n",
      "        [-8.3380e-04, -2.7080e-02, -3.7033e-03,  3.4589e-02],\n",
      "        [ 1.5344e-02, -4.2027e-02,  2.1110e-03,  1.3923e-02],\n",
      "        [-3.7467e-03, -6.0905e-03,  1.6626e-02,  3.2702e-02],\n",
      "        [-5.0667e-02,  6.0841e-02,  5.1357e-03,  3.4350e-02],\n",
      "        [-2.0087e-03, -3.0516e-02,  1.9860e-02, -5.9693e-02],\n",
      "        [-1.6543e-02, -3.7801e-02, -2.3061e-02,  4.5117e-02],\n",
      "        [-9.5973e-02, -6.1822e-02,  1.0989e-02, -1.8961e-02],\n",
      "        [-8.4054e-03, -4.4264e-02,  2.4405e-02, -3.2325e-02],\n",
      "        [ 1.7854e-03, -2.5506e-02, -2.6426e-02,  1.3736e-02],\n",
      "        [ 2.5926e-02, -1.1948e-02,  2.3480e-02,  3.4432e-02],\n",
      "        [ 2.3541e-03, -1.2059e-02,  1.4207e-02,  2.5607e-02],\n",
      "        [-7.2792e-03,  7.1962e-03,  3.5389e-02,  2.5054e-02],\n",
      "        [ 2.4405e-02, -5.5915e-03,  2.2645e-02,  1.2451e-02],\n",
      "        [ 1.5284e-02,  9.7854e-04,  1.1734e-02,  1.0040e-02],\n",
      "        [-6.4543e-03,  6.2674e-02,  3.7251e-02,  2.9337e-02],\n",
      "        [ 9.8544e-03,  6.7949e-05, -1.1471e-02,  2.0503e-02],\n",
      "        [ 2.6526e-02, -4.9188e-02,  1.0148e-02, -1.5860e-02],\n",
      "        [ 4.6326e-02, -3.7211e-02,  1.8154e-02,  2.0690e-02],\n",
      "        [ 8.5224e-03,  3.4709e-03, -1.4459e-02,  1.2118e-02],\n",
      "        [-2.1441e-02, -1.6475e-02,  1.7611e-02,  2.8881e-02],\n",
      "        [ 3.4181e-02,  3.3926e-03,  3.7236e-03,  2.0230e-02],\n",
      "        [-4.2308e-02, -1.9044e-02,  1.9232e-02, -4.8006e-02],\n",
      "        [-1.6470e-03, -2.1922e-02,  1.1729e-02,  4.6607e-02],\n",
      "        [-1.3554e-02, -3.5144e-02, -3.4002e-03,  2.2464e-02],\n",
      "        [ 2.4211e-02, -9.6868e-03, -2.6345e-02,  1.8277e-02],\n",
      "        [-1.1007e-02, -7.5170e-02, -6.7737e-03, -5.2661e-02],\n",
      "        [-2.2597e-02, -6.4151e-03, -2.2127e-02, -8.8860e-03],\n",
      "        [-4.2500e-02, -4.0105e-02, -1.1416e-02, -4.4264e-02],\n",
      "        [-7.3467e-03, -2.0992e-02,  2.8515e-02,  3.8917e-02],\n",
      "        [ 1.0740e-03, -1.0888e-02, -5.3265e-03,  3.2915e-02],\n",
      "        [-4.2739e-02, -8.3105e-03,  1.0745e-02, -3.2158e-02],\n",
      "        [ 5.0585e-02, -1.0569e-02,  1.7366e-02,  1.8403e-02],\n",
      "        [-2.9857e-02, -2.2122e-02,  1.5394e-02, -3.6767e-03],\n",
      "        [ 7.0400e-03,  2.7849e-02,  1.2123e-02,  2.9284e-02],\n",
      "        [ 2.4260e-02,  3.0206e-03, -2.9518e-03,  4.4420e-02],\n",
      "        [-6.4357e-02,  6.9105e-03,  8.6354e-03, -4.3750e-03],\n",
      "        [-2.5708e-03,  2.4411e-03, -1.0834e-02, -5.9031e-02],\n",
      "        [ 1.9835e-02,  2.7462e-03,  2.2584e-02,  1.0291e-02],\n",
      "        [ 3.6030e-02, -5.1742e-02, -8.5374e-03,  2.0479e-02],\n",
      "        [-6.9121e-04, -2.5524e-02,  6.6055e-03,  1.7604e-02],\n",
      "        [ 2.0746e-02, -3.0557e-02,  1.6229e-02,  3.0637e-03],\n",
      "        [-1.5915e-02,  1.9930e-02, -2.3909e-02,  1.1458e-02],\n",
      "        [-2.2810e-02, -1.6009e-03, -3.2100e-02,  2.6858e-02],\n",
      "        [ 2.9137e-02, -2.4307e-02, -8.6695e-03,  1.0081e-02],\n",
      "        [ 2.8942e-02,  8.0842e-03,  3.4384e-02,  2.3179e-02],\n",
      "        [ 1.8523e-02, -2.3957e-03,  1.8449e-02,  3.8686e-02],\n",
      "        [ 2.1671e-02, -3.1103e-02,  1.0235e-02, -5.5269e-02],\n",
      "        [-1.5234e-02, -1.2887e-02,  1.2355e-02,  3.1995e-02],\n",
      "        [ 1.1555e-02, -1.9837e-02,  2.2577e-02,  2.9967e-02],\n",
      "        [ 2.4561e-02, -1.4049e-02,  1.5034e-02,  9.5720e-03],\n",
      "        [-1.0262e-02, -2.7673e-02, -3.5397e-03, -2.8670e-02],\n",
      "        [ 2.7667e-02, -8.6396e-03,  1.6877e-02,  2.6501e-02],\n",
      "        [-3.9106e-02, -1.9740e-03,  1.5157e-02, -1.1098e-01],\n",
      "        [-2.5695e-02, -5.5463e-02, -9.4756e-02, -5.0277e-02],\n",
      "        [ 1.0572e-02, -5.0066e-02,  1.5596e-02,  2.2258e-02],\n",
      "        [-3.9050e-02, -4.7477e-02,  2.0499e-02, -5.5615e-02],\n",
      "        [ 2.6348e-02, -4.2752e-02,  4.0132e-02,  1.4704e-02],\n",
      "        [-1.2425e-02, -4.3821e-02,  8.1519e-03,  2.0643e-02]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), 'labels': tensor([[-1.4752e-02, -1.3626e-01],\n",
      "        [ 2.9242e-02, -2.4942e-01],\n",
      "        [ 9.4670e-02, -1.1923e-01],\n",
      "        [-3.5648e-02, -1.7780e-01],\n",
      "        [ 1.7374e-02, -8.8292e-02],\n",
      "        [ 1.2564e-02, -5.6611e-02],\n",
      "        [-3.6017e-02, -1.1857e-01],\n",
      "        [ 2.9556e-02, -1.0299e-01],\n",
      "        [-2.3374e-02, -8.6615e-02],\n",
      "        [ 2.4238e-02, -8.2303e-02],\n",
      "        [ 6.4295e-02, -1.6159e-01],\n",
      "        [-5.2794e-02, -1.4830e-01],\n",
      "        [ 3.6605e-03, -8.4045e-02],\n",
      "        [-1.2681e-03, -8.9662e-02],\n",
      "        [-3.0158e-03, -9.6619e-02],\n",
      "        [ 2.0391e-02, -1.9967e-01],\n",
      "        [-1.7101e-02, -1.1685e-01],\n",
      "        [-1.5086e-02, -9.4695e-02],\n",
      "        [ 6.6648e-02, -1.2239e-01],\n",
      "        [-1.1891e-02, -5.3417e-02],\n",
      "        [ 8.6883e-03, -7.0415e-02],\n",
      "        [-7.7302e-03, -8.7787e-02],\n",
      "        [-2.7909e-02, -1.2268e-01],\n",
      "        [-2.4471e-02, -1.2490e-01],\n",
      "        [-1.7143e-02, -1.0915e-01],\n",
      "        [ 3.5701e-02, -1.4880e-01],\n",
      "        [ 2.3667e-03, -7.5084e-02],\n",
      "        [-2.0167e-02, -1.2036e-01],\n",
      "        [-3.7486e-02, -4.4174e-02],\n",
      "        [-8.9359e-03, -6.2952e-02],\n",
      "        [-1.0388e-02, -9.8110e-02],\n",
      "        [-2.0324e-02, -1.1137e-01],\n",
      "        [-2.1692e-02, -5.0657e-02],\n",
      "        [-1.0955e-02, -9.9486e-02],\n",
      "        [ 3.0003e-02, -1.1308e-01],\n",
      "        [-1.4328e-02, -5.9416e-02],\n",
      "        [ 1.7004e-02, -1.0406e-01],\n",
      "        [-2.6580e-02, -1.6507e-01],\n",
      "        [-2.4310e-02, -7.2760e-02],\n",
      "        [ 3.3474e-02, -1.0927e-01],\n",
      "        [ 3.6991e-03, -1.1320e-01],\n",
      "        [-8.4135e-03, -1.1613e-01],\n",
      "        [-2.4387e-02, -8.4800e-02],\n",
      "        [-2.1499e-02, -1.8953e-01],\n",
      "        [-2.6557e-02, -6.4474e-02],\n",
      "        [-3.8788e-02, -1.3346e-01],\n",
      "        [-2.2864e-02, -9.0620e-02],\n",
      "        [-1.0135e-02, -1.8300e-01],\n",
      "        [-1.2311e-02, -1.0515e-01],\n",
      "        [-7.5042e-02, -1.1391e-01],\n",
      "        [ 1.5009e-02, -1.2586e-01],\n",
      "        [ 6.5870e-02, -1.1043e-01],\n",
      "        [-1.8299e-03, -1.1672e-01],\n",
      "        [ 2.9028e-02, -1.1393e-01],\n",
      "        [-1.3930e-02, -8.1336e-02],\n",
      "        [ 1.8174e-02, -6.3254e-02],\n",
      "        [-4.2088e-02, -1.2139e-01],\n",
      "        [ 9.7646e-03, -7.8861e-02],\n",
      "        [-6.0713e-02, -1.2008e-01],\n",
      "        [-2.2369e-03, -9.6504e-02],\n",
      "        [-2.0751e-02, -7.9883e-02],\n",
      "        [ 1.7699e-02, -1.3742e-01],\n",
      "        [ 2.2220e-02, -8.1772e-02],\n",
      "        [ 4.4514e-02, -7.7997e-02],\n",
      "        [ 8.5622e-03, -7.6861e-02],\n",
      "        [-1.4589e-02, -1.2310e-01],\n",
      "        [-7.4739e-02, -8.8894e-02],\n",
      "        [ 3.1224e-02, -1.4265e-01],\n",
      "        [-1.3968e-03, -7.0611e-02],\n",
      "        [-5.1059e-02, -6.1531e-02],\n",
      "        [ 3.2802e-03, -5.0724e-02],\n",
      "        [ 3.0836e-02, -1.2384e-01],\n",
      "        [-2.8017e-02, -8.1296e-02],\n",
      "        [ 3.9545e-02, -4.8709e-02],\n",
      "        [-5.8494e-02, -6.3467e-02],\n",
      "        [-2.5862e-02, -7.0561e-02],\n",
      "        [-1.7820e-02, -8.2395e-02],\n",
      "        [-1.8739e-02, -8.5923e-02],\n",
      "        [-3.0268e-02, -9.8472e-02],\n",
      "        [-3.7408e-02, -1.3428e-01],\n",
      "        [-1.9886e-02, -9.9259e-02],\n",
      "        [-3.5104e-03, -7.0115e-02],\n",
      "        [ 2.3265e-02, -6.1335e-02],\n",
      "        [-3.9800e-02, -1.4032e-01],\n",
      "        [ 5.1041e-02, -1.0688e-01],\n",
      "        [ 3.3985e-02, -1.0881e-01],\n",
      "        [-7.8297e-02, -1.3571e-01],\n",
      "        [ 2.1784e-02, -9.7585e-02],\n",
      "        [ 5.7539e-03, -7.9279e-02],\n",
      "        [ 1.1749e-02, -8.5373e-02],\n",
      "        [ 3.6045e-02, -5.7298e-02],\n",
      "        [-2.1367e-03, -1.2873e-01],\n",
      "        [ 2.4086e-02, -6.7115e-02],\n",
      "        [ 5.5634e-02, -8.5218e-02],\n",
      "        [ 2.1191e-02, -9.3431e-02],\n",
      "        [ 8.1924e-03, -6.8981e-02],\n",
      "        [-2.6105e-02, -7.6520e-02],\n",
      "        [-1.9741e-02, -8.4725e-02],\n",
      "        [-2.2103e-02, -1.0600e-01],\n",
      "        [-3.3715e-03, -9.1378e-02],\n",
      "        [ 3.3449e-02, -9.9986e-02],\n",
      "        [-6.6015e-02, -5.6933e-02],\n",
      "        [-5.2363e-03, -5.1809e-02],\n",
      "        [ 2.1162e-02, -5.5446e-02],\n",
      "        [-1.4388e-02, -7.3979e-02],\n",
      "        [-3.3651e-02, -6.2115e-02],\n",
      "        [-1.1313e-02, -4.3240e-02],\n",
      "        [ 7.8066e-02, -1.5129e-01],\n",
      "        [ 4.6481e-02, -6.7575e-02],\n",
      "        [ 1.6688e-02, -7.0904e-02],\n",
      "        [ 2.5013e-03, -4.9603e-02],\n",
      "        [-6.7104e-04, -6.7726e-02],\n",
      "        [-9.2443e-03, -7.0973e-02],\n",
      "        [-1.5771e-02, -1.1245e-01],\n",
      "        [ 2.2380e-02, -1.0588e-01],\n",
      "        [ 3.8277e-02, -9.6813e-02],\n",
      "        [-5.4224e-03, -5.3554e-02],\n",
      "        [ 5.7160e-02, -1.2415e-01],\n",
      "        [ 1.4975e-02, -4.6307e-02],\n",
      "        [ 5.0783e-02, -7.7792e-02],\n",
      "        [-1.6290e-02, -1.1897e-01],\n",
      "        [ 1.1444e-02, -5.6014e-02],\n",
      "        [-2.9606e-02, -1.1591e-01],\n",
      "        [-1.0490e-03, -2.8929e-02],\n",
      "        [ 3.4290e-02, -1.0342e-01],\n",
      "        [-8.3449e-03, -7.5425e-02],\n",
      "        [ 1.2220e-03, -1.0143e-01],\n",
      "        [-4.1994e-03, -6.7234e-02],\n",
      "        [-3.7259e-02, -4.5302e-02],\n",
      "        [-3.9331e-03, -4.7144e-02],\n",
      "        [ 6.2728e-03, -8.2724e-02],\n",
      "        [ 6.8195e-03, -6.4621e-02],\n",
      "        [-7.5991e-03, -1.1943e-01],\n",
      "        [-3.5418e-02, -8.2864e-02],\n",
      "        [-1.2172e-02, -7.8861e-02],\n",
      "        [-2.3899e-02, -7.7064e-02],\n",
      "        [ 6.4619e-02, -1.3330e-01],\n",
      "        [ 6.7426e-03, -7.8279e-02],\n",
      "        [ 4.2763e-03, -8.8357e-02],\n",
      "        [ 8.6620e-02, -1.7081e-01],\n",
      "        [-1.5465e-02, -5.4183e-02],\n",
      "        [-8.9480e-03, -7.9068e-02],\n",
      "        [ 7.9970e-03, -6.1125e-02],\n",
      "        [ 1.6027e-02, -3.8454e-02],\n",
      "        [-2.6732e-02, -6.7798e-02],\n",
      "        [-2.9038e-02, -1.0110e-01],\n",
      "        [-2.3184e-02, -7.5345e-02],\n",
      "        [ 2.7944e-02, -8.3933e-02],\n",
      "        [ 7.5174e-02, -1.1281e-01],\n",
      "        [-2.2140e-02, -1.5242e-01],\n",
      "        [ 3.4158e-02, -8.5544e-02],\n",
      "        [ 1.3008e-02, -4.7495e-02],\n",
      "        [-5.7541e-03, -6.9919e-02],\n",
      "        [-1.8929e-02, -7.6933e-02],\n",
      "        [-7.0533e-03, -3.9060e-02],\n",
      "        [-2.1142e-02, -6.5575e-02],\n",
      "        [-2.0212e-02, -7.5922e-02],\n",
      "        [-4.2584e-03, -3.9532e-02],\n",
      "        [-8.4999e-03, -7.2249e-02],\n",
      "        [-1.0665e-02, -6.6838e-02],\n",
      "        [-1.5966e-02, -5.7937e-02],\n",
      "        [-8.6272e-03, -1.0078e-01],\n",
      "        [-3.4621e-02, -4.9382e-02],\n",
      "        [-1.9243e-03, -1.0003e-01],\n",
      "        [ 6.9192e-03, -7.9771e-02],\n",
      "        [ 6.5224e-02, -7.9015e-02],\n",
      "        [ 1.9145e-02, -5.8625e-02],\n",
      "        [-2.5047e-03, -8.0645e-02],\n",
      "        [ 5.8600e-03, -1.1184e-01],\n",
      "        [ 3.3836e-02, -7.8204e-02],\n",
      "        [ 2.8838e-02, -1.0988e-01],\n",
      "        [ 1.6974e-02, -8.4467e-02],\n",
      "        [-4.3642e-02, -9.2189e-02],\n",
      "        [-1.4590e-02, -3.8840e-02],\n",
      "        [ 7.0634e-02, -1.2589e-01],\n",
      "        [-1.6102e-02, -1.0522e-01],\n",
      "        [-3.0035e-02, -7.9622e-02],\n",
      "        [ 4.4114e-02, -1.2401e-01],\n",
      "        [-1.5396e-02, -1.1575e-01],\n",
      "        [ 2.3945e-02, -6.6272e-02],\n",
      "        [ 1.7602e-02, -3.2761e-02],\n",
      "        [ 1.1871e-02, -4.5189e-02],\n",
      "        [-9.1640e-04, -5.6990e-02],\n",
      "        [ 4.4507e-02, -1.4387e-01],\n",
      "        [-2.6936e-05, -8.8708e-02],\n",
      "        [ 2.1943e-02, -3.5716e-02],\n",
      "        [-3.2443e-02, -4.7795e-02],\n",
      "        [-8.8138e-04, -4.5071e-02],\n",
      "        [ 2.3282e-02, -8.5981e-02],\n",
      "        [-2.7480e-02, -7.4016e-02],\n",
      "        [ 1.6333e-02, -9.4547e-02],\n",
      "        [ 2.5509e-02, -3.2097e-02],\n",
      "        [ 2.9842e-02, -7.0274e-02],\n",
      "        [-5.6746e-03, -4.5296e-02],\n",
      "        [ 1.2504e-03, -8.1801e-02],\n",
      "        [-7.3622e-03, -7.7816e-02],\n",
      "        [ 1.7972e-02, -6.1932e-02],\n",
      "        [ 2.0151e-02, -8.4924e-02],\n",
      "        [ 2.5326e-02, -9.4363e-02],\n",
      "        [-3.3793e-02, -9.7290e-02]], device='cuda:0', grad_fn=<AddmmBackward0>), 'rpn_cls_logits': tensor([[[[ 0.0096, -0.1907,  0.0127,  ...,  0.0657,  0.0318, -0.0227],\n",
      "          [-0.2072, -0.2771, -0.1322,  ..., -0.0825, -0.0555, -0.0758],\n",
      "          [-0.1612, -0.2924, -0.2464,  ..., -0.1662, -0.1627, -0.0923],\n",
      "          ...,\n",
      "          [-0.0689, -0.0956, -0.1162,  ..., -0.1275, -0.0775, -0.0641],\n",
      "          [-0.0592, -0.0846, -0.1263,  ..., -0.0999, -0.0610, -0.0358],\n",
      "          [-0.1071, -0.1620, -0.1803,  ..., -0.1453, -0.1317, -0.0727]],\n",
      "\n",
      "         [[-0.0521, -0.1585, -0.1289,  ..., -0.0340, -0.0203, -0.0841],\n",
      "          [ 0.0462, -0.1740, -0.1017,  ..., -0.0045, -0.0114,  0.0139],\n",
      "          [-0.0958, -0.0651, -0.0115,  ...,  0.0097, -0.0019,  0.0532],\n",
      "          ...,\n",
      "          [-0.0084, -0.0744,  0.0474,  ..., -0.0216, -0.0026,  0.0602],\n",
      "          [-0.0164,  0.0306,  0.0767,  ...,  0.0117, -0.0237,  0.0164],\n",
      "          [-0.0339,  0.0096,  0.0224,  ..., -0.0132, -0.0078,  0.0869]],\n",
      "\n",
      "         [[ 0.1188,  0.1599,  0.0996,  ...,  0.0983,  0.1215,  0.0717],\n",
      "          [ 0.1891, -0.0566,  0.1056,  ...,  0.1119,  0.0458,  0.0765],\n",
      "          [ 0.1801,  0.0239, -0.0054,  ...,  0.0441, -0.0174,  0.0476],\n",
      "          ...,\n",
      "          [ 0.1560,  0.0465,  0.0876,  ...,  0.0903,  0.0503,  0.0726],\n",
      "          [ 0.1637,  0.1092,  0.1199,  ...,  0.1651,  0.1300,  0.1503],\n",
      "          [ 0.0529,  0.0409,  0.0411,  ...,  0.0749, -0.0045,  0.0906]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0825,  0.0839,  0.1672,  ...,  0.0548,  0.0627,  0.1241],\n",
      "          [ 0.0606,  0.1395,  0.2426,  ...,  0.0894,  0.1476,  0.0820],\n",
      "          [ 0.1091,  0.1425,  0.1386,  ...,  0.1163,  0.0780,  0.1016],\n",
      "          ...,\n",
      "          [ 0.0504,  0.0463,  0.1197,  ...,  0.1627,  0.0548,  0.1187],\n",
      "          [ 0.0793,  0.0561,  0.1090,  ...,  0.0823,  0.1014,  0.1949],\n",
      "          [ 0.0091,  0.0889,  0.0866,  ...,  0.0577,  0.0197,  0.1005]],\n",
      "\n",
      "         [[-0.0526,  0.0106,  0.1141,  ...,  0.0341,  0.1155,  0.0961],\n",
      "          [ 0.0253,  0.0685,  0.0696,  ...,  0.0831,  0.0754,  0.0545],\n",
      "          [ 0.0239,  0.0364,  0.0920,  ...,  0.0045,  0.0186,  0.0630],\n",
      "          ...,\n",
      "          [ 0.0276,  0.0941,  0.0255,  ...,  0.0370,  0.0410,  0.0354],\n",
      "          [ 0.0689,  0.0023,  0.0194,  ...,  0.0153,  0.0321,  0.0290],\n",
      "          [ 0.0758,  0.0861,  0.0660,  ...,  0.1064,  0.0188,  0.0053]],\n",
      "\n",
      "         [[-0.0816, -0.0492, -0.0526,  ..., -0.0298, -0.0171, -0.0277],\n",
      "          [-0.0401, -0.0024,  0.0288,  ...,  0.0391, -0.0061,  0.0160],\n",
      "          [-0.0527, -0.0201, -0.0899,  ...,  0.0165,  0.0615, -0.0208],\n",
      "          ...,\n",
      "          [ 0.0013, -0.0144, -0.0306,  ...,  0.0148, -0.0269, -0.0496],\n",
      "          [ 0.0297,  0.0203, -0.0611,  ..., -0.0394, -0.0261, -0.0289],\n",
      "          [-0.0165,  0.0321,  0.0775,  ...,  0.0503,  0.0299,  0.0050]]]],\n",
      "       device='cuda:0', grad_fn=<ConvolutionBackward0>), 'rpn_bbox_preds': tensor([[[[ 0.1128,  0.0509,  0.0185,  ...,  0.0816,  0.0729,  0.1299],\n",
      "          [-0.0760,  0.0313,  0.1091,  ...,  0.0726,  0.0782,  0.1026],\n",
      "          [ 0.0686,  0.1632,  0.2464,  ...,  0.1301,  0.1533,  0.1678],\n",
      "          ...,\n",
      "          [ 0.0816,  0.1290,  0.1819,  ...,  0.1594,  0.2021,  0.1201],\n",
      "          [ 0.0376,  0.1135,  0.2075,  ...,  0.1309,  0.1780,  0.1493],\n",
      "          [ 0.0614,  0.0611, -0.0150,  ...,  0.0099,  0.0496,  0.0313]],\n",
      "\n",
      "         [[ 0.0055,  0.0614, -0.0053,  ...,  0.0455,  0.0355,  0.0248],\n",
      "          [ 0.1422,  0.0517,  0.1746,  ...,  0.0552,  0.0856,  0.0473],\n",
      "          [ 0.1892,  0.1143, -0.0354,  ...,  0.0316,  0.0559, -0.0292],\n",
      "          ...,\n",
      "          [ 0.1057,  0.0266,  0.0198,  ...,  0.0300,  0.0263,  0.0244],\n",
      "          [ 0.0735,  0.0478,  0.0243,  ...,  0.0180, -0.0362, -0.0152],\n",
      "          [ 0.1120,  0.0317,  0.0581,  ...,  0.0809, -0.0161, -0.0239]],\n",
      "\n",
      "         [[-0.0620, -0.1305,  0.0185,  ..., -0.0022, -0.0503,  0.0112],\n",
      "          [-0.0629, -0.0316, -0.1055,  ..., -0.0219,  0.0209,  0.0096],\n",
      "          [-0.0962, -0.1142, -0.1875,  ..., -0.0294, -0.0230, -0.0491],\n",
      "          ...,\n",
      "          [-0.0353,  0.0201, -0.0320,  ..., -0.0354, -0.0137, -0.0344],\n",
      "          [ 0.0124,  0.0496, -0.0457,  ..., -0.0460, -0.0396, -0.0495],\n",
      "          [ 0.0356,  0.0082, -0.0222,  ...,  0.0354,  0.0705, -0.0619]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0714, -0.0350,  0.0319,  ..., -0.0016, -0.0637, -0.0231],\n",
      "          [ 0.1310,  0.2111,  0.0203,  ..., -0.0073, -0.0253, -0.1267],\n",
      "          [ 0.0446,  0.0186, -0.0048,  ...,  0.0047,  0.0197, -0.0371],\n",
      "          ...,\n",
      "          [ 0.0549,  0.0173, -0.0105,  ..., -0.0012, -0.0220, -0.0460],\n",
      "          [ 0.0417,  0.0232, -0.0422,  ...,  0.0348,  0.0595,  0.0078],\n",
      "          [-0.0377, -0.0570, -0.0203,  ..., -0.0626, -0.0331, -0.1188]],\n",
      "\n",
      "         [[-0.0997, -0.1387, -0.2008,  ..., -0.0534, -0.0527, -0.1099],\n",
      "          [-0.0883, -0.2093, -0.1324,  ...,  0.0243, -0.0623, -0.0296],\n",
      "          [-0.0565, -0.1037, -0.0157,  ..., -0.0118, -0.0380,  0.0524],\n",
      "          ...,\n",
      "          [-0.0519, -0.0672, -0.0565,  ..., -0.0259, -0.0627,  0.0097],\n",
      "          [-0.0520, -0.1289, -0.0544,  ..., -0.0940, -0.1786, -0.0126],\n",
      "          [ 0.0022, -0.0281, -0.0024,  ..., -0.0470, -0.0615, -0.0062]],\n",
      "\n",
      "         [[ 0.0320,  0.0027,  0.0753,  ...,  0.0262,  0.0113,  0.0066],\n",
      "          [ 0.1618,  0.0284, -0.0136,  ..., -0.0088,  0.0358, -0.0315],\n",
      "          [ 0.0710,  0.0667, -0.0076,  ...,  0.0428,  0.0398, -0.0533],\n",
      "          ...,\n",
      "          [ 0.0106,  0.0016, -0.0008,  ..., -0.0013,  0.0443, -0.0133],\n",
      "          [ 0.0867,  0.0509,  0.0134,  ..., -0.0301,  0.0048, -0.0301],\n",
      "          [ 0.0611,  0.0147,  0.0747,  ...,  0.0985,  0.0981,  0.0276]]]],\n",
      "       device='cuda:0', grad_fn=<ConvolutionBackward0>), 'proposals': tensor([[  0.0000,  20.9106,  39.5696,  29.5505,  47.1765],\n",
      "        [  0.0000,  19.3097,  20.1488,  29.4120,  28.5736],\n",
      "        [  0.0000,  52.4234, 100.3388,  61.0480, 109.1028],\n",
      "        [  0.0000,  19.9923,   4.4442,  29.4005,  14.0007],\n",
      "        [  0.0000, 149.0256,  38.2686, 158.1111,  46.1664],\n",
      "        [  0.0000,  68.7530, 134.3486,  77.3238, 142.9755],\n",
      "        [  0.0000, 161.9411,  13.3254, 182.9666,  60.0708],\n",
      "        [  0.0000,  84.6709, 117.5531,  93.7473, 127.2224],\n",
      "        [  0.0000, 229.6204, 101.1897, 237.8729, 110.1786],\n",
      "        [  0.0000, 117.0718,  38.4394, 125.9780,  46.4487],\n",
      "        [  0.0000,  67.6260, 100.7656,  77.3684, 109.3347],\n",
      "        [  0.0000,  36.5531,   4.2840,  45.2627,  14.5336],\n",
      "        [  0.0000,  36.9532, 197.4340,  45.8838, 205.9142],\n",
      "        [  0.0000, 176.9648,  16.6479, 198.3351,  63.6020],\n",
      "        [  0.0000, 229.8335,  85.1578, 237.8897,  94.3651],\n",
      "        [  0.0000,  20.0578,  25.0907,  30.6548,  48.6159],\n",
      "        [  0.0000,  34.0343,  99.0421,  55.1855, 143.7353],\n",
      "        [  0.0000, 228.8670, 229.4944, 237.7293, 238.3051],\n",
      "        [  0.0000,  68.5472, 118.2605,  77.7435, 127.3132],\n",
      "        [  0.0000,   0.0000,   0.0000,  27.0313,  28.8768],\n",
      "        [  0.0000, 100.9159,  38.6011, 109.5860,  46.5762],\n",
      "        [  0.0000, 228.9337, 116.9997, 237.4318, 125.8877],\n",
      "        [  0.0000, 145.8688,  15.7883, 167.2630,  63.2521],\n",
      "        [  0.0000, 229.6192,  38.0284, 237.9450,  46.0926],\n",
      "        [  0.0000, 229.3360, 148.9825, 237.5479, 157.5988],\n",
      "        [  0.0000,  65.6166, 110.9231,  86.1462, 157.5748],\n",
      "        [  0.0000, 132.6814,  38.4146, 141.9292,  45.9408],\n",
      "        [  0.0000,  37.4029,  38.9770,  46.1350,  46.4264],\n",
      "        [  0.0000, 245.4831,  84.9704, 253.0964,  94.0573],\n",
      "        [  0.0000, 165.3816,  38.3924, 173.9946,  46.9442],\n",
      "        [  0.0000,  36.8711, 117.6570,  45.5402, 125.6981],\n",
      "        [  0.0000,   0.0000,   7.3605,  16.9052,  53.5086],\n",
      "        [  0.0000, 245.0666, 116.7868, 252.6749, 125.1228],\n",
      "        [  0.0000, 178.5562, 128.5171, 199.2510, 175.1680],\n",
      "        [  0.0000, 114.3783,  17.9630, 134.9724,  65.0041],\n",
      "        [  0.0000, 240.3618,  82.6165, 260.0356,  92.5977],\n",
      "        [  0.0000, 130.9546,  16.2696, 151.3089,  64.0015],\n",
      "        [  0.0000,  37.3505,  21.0706,  46.2498,  30.1871],\n",
      "        [  0.0000, 180.8242,   4.3037, 189.0949,  13.6234],\n",
      "        [  0.0000,  81.3097, 111.6351, 100.4360, 156.4090],\n",
      "        [  0.0000,  97.3245,  16.1067, 118.6518,  66.9374],\n",
      "        [  0.0000,  37.0793, 101.1355,  45.9203, 110.1056],\n",
      "        [  0.0000, 239.8349, 162.0882, 260.5769, 172.0490],\n",
      "        [  0.0000,  36.1955,  14.8534,  54.9546,  61.9469],\n",
      "        [  0.0000, 100.6287, 100.1004, 109.1384, 109.3807],\n",
      "        [  0.0000,  20.8011,  54.6010,  29.9211,  62.2806],\n",
      "        [  0.0000, 229.4597, 165.2081, 237.8040, 173.6384],\n",
      "        [  0.0000,  17.6006,  22.1712,  37.7851,  68.1758],\n",
      "        [  0.0000, 228.6822,  20.8912, 237.7120,  29.5292],\n",
      "        [  0.0000,  48.4902,   2.5942,  69.5782,  49.0440],\n",
      "        [  0.0000,  20.5060, 245.7171,  28.9250, 253.7106],\n",
      "        [  0.0000,  52.7689, 117.8245,  61.2756, 126.4349],\n",
      "        [  0.0000,  20.4163, 149.9778,  29.2062, 158.1222],\n",
      "        [  0.0000,  48.8688, 113.5290,  69.9185, 160.7140],\n",
      "        [  0.0000,  37.2673, 133.8551,  46.1600, 142.1014],\n",
      "        [  0.0000,  84.6285, 133.8666,  93.1744, 142.0397],\n",
      "        [  0.0000,  50.3092,  13.5106,  70.0736,  63.7276],\n",
      "        [  0.0000,  37.3407, 149.3115,  45.5772, 157.6169],\n",
      "        [  0.0000, 245.4709, 228.8310, 253.3023, 238.1748],\n",
      "        [  0.0000,  34.0051, 176.5954,  55.5706, 223.5369],\n",
      "        [  0.0000,  20.5997, 134.1048,  29.4258, 142.2139],\n",
      "        [  0.0000, 132.8149,   4.3166, 141.4654,  13.5051],\n",
      "        [  0.0000, 143.6127,  32.1908, 165.3712,  79.9815],\n",
      "        [  0.0000, 211.7818,  85.9573, 221.1319,  93.6743],\n",
      "        [  0.0000,  36.8285, 165.2504,  45.6976, 173.1889],\n",
      "        [  0.0000,   4.8602,  38.7205,  12.4270,  45.8797],\n",
      "        [  0.0000,  52.9662,   4.2927,  61.4732,  13.6553],\n",
      "        [  0.0000,  48.0012,  96.8419,  69.1778, 147.3715],\n",
      "        [  0.0000, 161.2445,  32.6505, 182.3758,  77.7372],\n",
      "        [  0.0000, 244.9579, 148.6082, 252.8838, 157.2854],\n",
      "        [  0.0000, 149.3285,  53.8193, 157.9743,  62.0932],\n",
      "        [  0.0000,  84.6963, 100.6637,  93.5331, 110.3196],\n",
      "        [  0.0000, 160.8516, 126.5531, 182.9729, 175.0266],\n",
      "        [  0.0000,  52.9263, 164.5067,  61.1633, 173.0948],\n",
      "        [  0.0000, 245.4795, 164.9380, 253.2762, 173.4250],\n",
      "        [  0.0000,  53.5233,  37.4559,  62.1634,  45.5483],\n",
      "        [  0.0000, 239.1770, 146.1648, 260.0452, 156.2982],\n",
      "        [  0.0000, 100.8264, 116.8100, 109.9911, 126.8755],\n",
      "        [  0.0000,  20.5136, 117.8641,  29.1011, 126.2179],\n",
      "        [  0.0000,  18.6818,  46.5685,  29.1939,  67.8446],\n",
      "        [  0.0000, 224.4047, 127.2681, 246.0787, 174.0510],\n",
      "        [  0.0000, 128.0760,  33.8403, 150.0845,  80.5850],\n",
      "        [  0.0000,  63.9626, 130.9301,  86.2647, 178.6199],\n",
      "        [  0.0000, 223.3679,  77.8817, 245.2600, 123.2563],\n",
      "        [  0.0000,  33.8859, 208.5271,  55.5032, 257.6681],\n",
      "        [  0.0000,  28.5945, 226.9902,  50.7258, 274.2665],\n",
      "        [  0.0000,  18.3408,   0.0000,  29.1293,  21.1871],\n",
      "        [  0.0000,  53.5233, 228.3073,  62.0247, 236.8356],\n",
      "        [  0.0000,  36.9977, 181.2639,  45.5735, 189.6405],\n",
      "        [  0.0000, 100.4073, 133.0112, 109.5624, 141.5116],\n",
      "        [  0.0000, 196.1220,  85.1413, 205.0705,  93.4843],\n",
      "        [  0.0000, 116.5543,   4.2916, 125.3142,  13.3377],\n",
      "        [  0.0000, 116.6336,  69.2889, 125.3458,  77.4876],\n",
      "        [  0.0000, 212.3235, 212.9837, 221.4100, 221.7453],\n",
      "        [  0.0000, 208.3095, 115.0438, 229.2542, 163.3879],\n",
      "        [  0.0000, 196.4711, 229.4407, 205.5848, 237.9389],\n",
      "        [  0.0000, 164.7683,   4.5920, 173.4026,  12.8147],\n",
      "        [  0.0000, 228.9962, 197.1068, 237.4961, 205.5303],\n",
      "        [  0.0000,  37.2276,  53.8646,  45.9640,  61.2884],\n",
      "        [  0.0000,  20.4504,  70.1311,  29.3891,  77.9544],\n",
      "        [  0.0000, 212.5432, 133.4094, 221.6750, 141.5716],\n",
      "        [  0.0000, 240.1210, 241.7971, 260.7355, 252.4722],\n",
      "        [  0.0000, 180.8700,  38.1679, 189.6724,  46.2709],\n",
      "        [  0.0000,  68.7300, 164.8742,  77.3111, 173.4905],\n",
      "        [  0.0000,  36.6070,  85.0597,  45.6062,  93.3234],\n",
      "        [  0.0000,  84.9502,  37.9922,  93.4519,  45.9930],\n",
      "        [  0.0000,  68.8952,  38.0465,  77.3921,  45.7298],\n",
      "        [  0.0000,  50.4881, 109.8216,  62.6372, 130.1911],\n",
      "        [  0.0000,  45.9425, 226.3268,  68.4039, 271.6945],\n",
      "        [  0.0000,  20.4372, 166.1255,  29.3471, 174.2739],\n",
      "        [  0.0000, 196.1685, 245.2665, 204.6568, 253.4960],\n",
      "        [  0.0000, 143.7567,  50.4888, 165.1068,  97.1324],\n",
      "        [  0.0000, 229.4447, 181.0651, 237.3839, 189.4666],\n",
      "        [  0.0000,  30.1372, 113.7896,  51.0706, 124.3836],\n",
      "        [  0.0000, 180.6296, 117.0911, 189.2198, 125.8481],\n",
      "        [  0.0000, 131.6728,  20.7890, 141.3681,  30.1282],\n",
      "        [  0.0000,  68.3856, 196.7626,  77.2731, 205.4862],\n",
      "        [  0.0000, 210.7930,  94.1249, 221.5299, 115.5410],\n",
      "        [  0.0000, 211.6597, 236.3522, 222.2259, 257.5741],\n",
      "        [  0.0000, 208.3749, 146.5700, 230.1986, 193.7481],\n",
      "        [  0.0000,  34.3839,  34.6202,  54.9755,  81.2151],\n",
      "        [  0.0000,  84.3642, 196.6293,  93.0313, 205.2595],\n",
      "        [  0.0000, 229.1289,  69.2204, 237.6872,  77.7643],\n",
      "        [  0.0000,  84.4726,  68.9181,  93.0279,  77.1345],\n",
      "        [  0.0000, 212.8176, 102.4361, 221.2466, 110.3994],\n",
      "        [  0.0000, 212.6815,  38.0120, 221.3294,  46.0723],\n",
      "        [  0.0000,  29.9912, 146.2844,  50.6996, 156.6456],\n",
      "        [  0.0000,  53.3800, 212.4708,  61.7929, 221.1080],\n",
      "        [  0.0000, 244.9254, 132.9008, 252.7578, 141.6831],\n",
      "        [  0.0000, 192.0791,  68.3036, 213.1596, 115.2548],\n",
      "        [  0.0000,  80.0365, 209.7280, 101.5652, 258.2108],\n",
      "        [  0.0000,  49.6493, 128.4741,  70.5102, 176.9802],\n",
      "        [  0.0000, 197.2416, 117.5430, 205.7057, 126.1499],\n",
      "        [  0.0000, 164.7458, 149.9035, 173.6305, 158.2761],\n",
      "        [  0.0000,  20.6615,  86.3256,  29.7273,  94.0469],\n",
      "        [  0.0000, 240.4056, 160.0777, 260.7907, 206.2644],\n",
      "        [  0.0000, 206.2020,  83.1220, 228.8906, 130.1699],\n",
      "        [  0.0000, 192.1688,  16.7289, 214.2384,  65.3184],\n",
      "        [  0.0000,  34.2569,  78.4931,  55.3059, 127.1509],\n",
      "        [  0.0000,  67.0139, 110.6497,  78.0690, 130.3837],\n",
      "        [  0.0000, 164.8280,  53.3819, 173.5900,  61.7093],\n",
      "        [  0.0000, 211.0158, 172.9937, 221.2771, 194.9056],\n",
      "        [  0.0000, 196.2784, 196.6831, 205.2631, 205.1858],\n",
      "        [  0.0000, 132.3195, 148.9854, 141.3270, 157.4074],\n",
      "        [  0.0000, 181.0368, 149.5611, 189.9814, 157.9825],\n",
      "        [  0.0000,  31.8687,  98.4845,  51.3029, 108.6637],\n",
      "        [  0.0000, 240.2986, 144.0777, 259.9080, 190.5559],\n",
      "        [  0.0000, 116.6358,  53.1788, 125.9775,  61.5095],\n",
      "        [  0.0000, 210.4131,  94.8680, 230.0157, 144.7463],\n",
      "        [  0.0000, 228.8482,  53.1426, 237.6031,  61.4551],\n",
      "        [  0.0000, 207.9576,  16.3385, 229.9304,  66.5238],\n",
      "        [  0.0000, 180.5449, 165.0478, 189.0625, 173.2324],\n",
      "        [  0.0000, 133.0042,  54.2313, 141.8454,  61.9490],\n",
      "        [  0.0000,  30.1794,  82.6802,  49.2091,  92.5599],\n",
      "        [  0.0000,  68.6788,  52.7036,  77.2119,  61.2268],\n",
      "        [  0.0000,  45.4644, 195.0159,  65.1973, 205.1643],\n",
      "        [  0.0000,  34.5093,  67.1226,  55.8536, 111.3005],\n",
      "        [  0.0000, 101.0838,  84.7436, 109.6771,  93.3987],\n",
      "        [  0.0000,  84.5341, 212.8962,  92.9231, 221.2868],\n",
      "        [  0.0000,  64.2398, 177.5711,  85.7486, 224.6067],\n",
      "        [  0.0000, 101.0964,  69.2120, 109.5953,  77.5024],\n",
      "        [  0.0000, 180.3953, 133.8937, 189.8699, 142.5385],\n",
      "        [  0.0000,  53.1653,  53.0507,  61.5682,  61.1377],\n",
      "        [  0.0000,  13.9773, 226.3534,  36.7900, 272.5192],\n",
      "        [  0.0000,  29.3347, 195.2150,  48.0618, 205.3411],\n",
      "        [  0.0000, 212.9184, 197.5157, 221.0877, 205.8310],\n",
      "        [  0.0000,  78.6161, 131.3848, 100.8637, 177.9210],\n",
      "        [  0.0000,   4.7312,  54.1671,  12.4040,  61.7434],\n",
      "        [  0.0000, 224.0930,  96.2635, 245.5317, 139.6682],\n",
      "        [  0.0000, 213.3022, 149.7708, 221.6380, 157.7560],\n",
      "        [  0.0000, 191.1730, 130.2356, 210.6984, 140.9408],\n",
      "        [  0.0000, 109.9345,  49.9665, 132.9788,  98.0211],\n",
      "        [  0.0000, 245.3417,  37.1881, 253.4109,  45.5425],\n",
      "        [  0.0000, 116.4626, 100.4522, 125.0557, 109.1217],\n",
      "        [  0.0000,  77.5812, 114.1148,  98.6097, 125.8712],\n",
      "        [  0.0000,  33.1720, 127.3880,  54.4839, 175.7267],\n",
      "        [  0.0000,  94.2466,  99.3605, 113.8176, 109.3616],\n",
      "        [  0.0000, 213.4433, 118.5053, 222.2114, 126.7885],\n",
      "        [  0.0000, 223.2643, 146.8142, 242.5818, 157.3463],\n",
      "        [  0.0000,  52.4949,  84.6296,  60.9881,  93.3825],\n",
      "        [  0.0000, 127.8282, 115.5628, 150.0575, 164.0789],\n",
      "        [  0.0000, 196.4326, 165.0202, 205.0946, 173.1364],\n",
      "        [  0.0000,  95.8453, 146.6130, 117.9323, 193.8091],\n",
      "        [  0.0000,  77.4823,  98.7019,  97.5420, 108.8627],\n",
      "        [  0.0000, 160.2828,  34.6520, 179.1355,  44.9216],\n",
      "        [  0.0000,  84.6509, 149.1725,  93.2337, 157.3921],\n",
      "        [  0.0000,  53.3201,  69.3410,  61.9566,  77.2219],\n",
      "        [  0.0000, 176.1251, 146.6663, 197.5935, 192.1330],\n",
      "        [  0.0000, 237.8182, 130.4731, 259.0371, 140.4448],\n",
      "        [  0.0000, 178.7890, 140.4425, 188.9758, 164.3499],\n",
      "        [  0.0000, 196.2490, 133.1937, 205.2950, 142.0208],\n",
      "        [  0.0000,  68.8176, 149.8136,  77.4177, 158.0098],\n",
      "        [  0.0000, 132.2387, 228.6057, 140.8569, 237.4124],\n",
      "        [  0.0000,  84.3328, 165.2310,  93.0438, 173.5006],\n",
      "        [  0.0000, 229.3363, 244.9968, 237.4638, 253.7015],\n",
      "        [  0.0000, 100.3317,   4.3352, 109.1701,  13.0383],\n",
      "        [  0.0000, 143.6960, 129.7345, 166.1218, 177.4207],\n",
      "        [  0.0000, 191.2924, 209.6900, 213.9014, 257.2906],\n",
      "        [  0.0000,  33.1582, 192.4516,  55.0726, 241.3446],\n",
      "        [  0.0000,  79.9135,  16.7478, 101.7073,  65.9625]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.ops import RoIAlign, nms\n",
    "import torchvision.ops as ops\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, num_classes=91):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        \n",
    "        # Use pretrained ResNet50 as the backbone\n",
    "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Use layers up to the last conv layer\n",
    "        \n",
    "        # Freeze the initial layers to retain pretrained weights\n",
    "        for param in list(self.backbone.parameters())[:6]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # RPN (Region Proposal Network)\n",
    "        self.rpn_conv = nn.Conv2d(2048, 512, kernel_size=3, padding=1)\n",
    "        self.rpn_cls = nn.Conv2d(512, 9 * 2, kernel_size=1)  # 9 anchors, 2 classes (object or background)\n",
    "        self.rpn_reg = nn.Conv2d(512, 9 * 4, kernel_size=1)  # 9 anchors, 4 bbox coordinates\n",
    "        \n",
    "        # RoI Align\n",
    "        self.roi_align = RoIAlign((7, 7), spatial_scale=1.0 / 16, sampling_ratio=2)\n",
    "        \n",
    "        # Fully connected layers for classification and bounding box regression\n",
    "        self.fc_class = nn.Linear(2048 * 7 * 7, 1024)\n",
    "        self.fc_bbox = nn.Linear(1024, 4)\n",
    "        self.fc_class_predict = nn.Linear(1024, num_classes)  # Class prediction layer (including background)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: Extract features\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Step 2: Generate RPN outputs\n",
    "        rpn_feat = F.relu(self.rpn_conv(features))\n",
    "        rpn_cls_logits = self.rpn_cls(rpn_feat)\n",
    "        rpn_bbox_preds = self.rpn_reg(rpn_feat)\n",
    "\n",
    "        # Step 3: Generate anchors (unchanged)\n",
    "        feature_size = (features.size(2), features.size(3))  # (height, width) of feature map\n",
    "        anchors = self.generate_anchors(feature_size, stride=16, device=x.device)\n",
    "\n",
    "        # Step 4: Generate proposals from RPN outputs\n",
    "        proposals = self.generate_proposals(rpn_cls_logits, rpn_bbox_preds, anchors, x.size()[2:], device=x.device)\n",
    "\n",
    "        # Step 5: Perform RoI Align\n",
    "        roi_features = self.roi_align(features, proposals)\n",
    "\n",
    "        # Step 6: Classify and regress bounding boxes for each proposal\n",
    "        roi_flattened = roi_features.view(roi_features.size(0), -1)\n",
    "        fc_class_out = F.relu(self.fc_class(roi_flattened))  # Use fc_class\n",
    "        bbox = self.fc_bbox(fc_class_out)  # Use fc_bbox\n",
    "        class_preds = self.fc_class_predict(fc_class_out)  # Class predictions\n",
    "\n",
    "        # Step 7: Return all necessary components for loss computation\n",
    "        output = {\n",
    "            \"boxes\": bbox,  # Predicted bounding boxes\n",
    "            \"labels\": class_preds,  # Predicted class scores\n",
    "            \"rpn_cls_logits\": rpn_cls_logits,  # RPN classification logits\n",
    "            \"rpn_bbox_preds\": rpn_bbox_preds,  # RPN bounding box predictions\n",
    "            \"proposals\": proposals  # Region proposals\n",
    "        }\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_loss(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Compute the total loss for the Faster R-CNN model.\n",
    "        The total loss is the sum of RPN loss, classification loss, and bbox regression loss.\n",
    "        \"\"\"\n",
    "        # Example loss computation (you can modify it according to your needs)\n",
    "        rpn_cls_loss = F.cross_entropy(outputs[\"rpn_cls_logits\"], targets[\"rpn_cls_labels\"])\n",
    "        rpn_bbox_loss = F.smooth_l1_loss(outputs[\"rpn_bbox_preds\"], targets[\"rpn_bbox_targets\"])\n",
    "        bbox_loss = F.smooth_l1_loss(outputs[\"boxes\"], targets[\"boxes\"])\n",
    "        class_loss = F.cross_entropy(outputs[\"labels\"], targets[\"labels\"])\n",
    "\n",
    "        total_loss = rpn_cls_loss + rpn_bbox_loss + bbox_loss + class_loss\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def generate_anchors(self, feature_size, stride, device):\n",
    "        \"\"\"\n",
    "        Generate anchors for the feature map. This assumes that there are 9 different anchor\n",
    "        sizes and aspect ratios.\n",
    "        \"\"\"\n",
    "        base_size = 16\n",
    "        scales = [2.0, 1.0, 0.5]  # Example scales\n",
    "        aspect_ratios = [0.5, 1.0, 2.0]  # Example aspect ratios\n",
    "        anchors = []\n",
    "\n",
    "        for y in range(feature_size[0]):\n",
    "            for x in range(feature_size[1]):\n",
    "                cx = x * stride + stride / 2  # Center x\n",
    "                cy = y * stride + stride / 2  # Center y\n",
    "                for scale in scales:\n",
    "                    for ratio in aspect_ratios:\n",
    "                        w = base_size * scale * (ratio ** 0.5)\n",
    "                        h = base_size * scale / (ratio ** 0.5)\n",
    "                        anchors.append([cx - w / 2, cy - h / 2, cx + w / 2, cy + h / 2])\n",
    "\n",
    "        anchors = torch.tensor(anchors, dtype=torch.float32, device=device)\n",
    "        return anchors\n",
    "\n",
    "    def generate_proposals(self, rpn_cls_logits, rpn_bbox_preds, anchors, image_size, device, top_n=200):\n",
    "        \"\"\"\n",
    "        Generate proposals from the RPN by decoding the classification and bbox regression outputs.\n",
    "        \"\"\"\n",
    "        batch_size = rpn_cls_logits.size(0)\n",
    "        proposals = []\n",
    "        batch_indices = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            cls_logits = rpn_cls_logits[b].permute(1, 2, 0).reshape(-1, 2)\n",
    "            bbox_preds = rpn_bbox_preds[b].permute(1, 2, 0).reshape(-1, 4)\n",
    "\n",
    "            # Apply softmax to classification logits\n",
    "            scores = F.softmax(cls_logits, dim=-1)[:, 1]  # Keep \"objectness\" score (class 1)\n",
    "\n",
    "            # Decode predicted deltas to proposals\n",
    "            decoded_boxes = self.decode_boxes(bbox_preds, anchors)\n",
    "\n",
    "            # Clip proposals to image boundaries\n",
    "            decoded_boxes = ops.clip_boxes_to_image(decoded_boxes, image_size)\n",
    "\n",
    "            # Add batch index to proposals (RoIAlign expects this)\n",
    "            batch_indices_for_image = torch.full((decoded_boxes.size(0), 1), b, dtype=torch.int64, device=device)\n",
    "            proposals_with_batch = torch.cat([batch_indices_for_image, decoded_boxes], dim=1)\n",
    "\n",
    "            # Keep top N proposals based on scores\n",
    "            keep = scores.argsort(descending=True)[:top_n]\n",
    "            proposals.append(proposals_with_batch[keep])  # Keep top N proposals for this image\n",
    "            batch_indices.append(scores[keep])\n",
    "\n",
    "        proposals = torch.cat(proposals, dim=0)\n",
    "        return proposals\n",
    "\n",
    "\n",
    "\n",
    "    def decode_boxes(self, deltas, anchors):\n",
    "        \"\"\"\n",
    "        Decode bounding box deltas to proposals.\n",
    "        \"\"\"\n",
    "        anchors = ops.box_convert(anchors, in_fmt=\"xyxy\", out_fmt=\"cxcywh\")\n",
    "        anchor_cx, anchor_cy, anchor_w, anchor_h = anchors.split(1, dim=1)\n",
    "\n",
    "        # Decode deltas\n",
    "        dx, dy, dw, dh = deltas.split(1, dim=1)\n",
    "        pred_cx = dx * anchor_w + anchor_cx\n",
    "        pred_cy = dy * anchor_h + anchor_cy\n",
    "        pred_w = torch.exp(dw) * anchor_w\n",
    "        pred_h = torch.exp(dh) * anchor_h\n",
    "\n",
    "        # Convert back to (x1, y1, x2, y2)\n",
    "        decoded_boxes = torch.cat([\n",
    "            pred_cx - 0.5 * pred_w,  # x1\n",
    "            pred_cy - 0.5 * pred_h,  # y1\n",
    "            pred_cx + 0.5 * pred_w,  # x2\n",
    "            pred_cy + 0.5 * pred_h   # y2\n",
    "        ], dim=1)\n",
    "\n",
    "        return decoded_boxes\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "model = FasterRCNN(num_classes=2)  # 91 classes including background (for COCO)\n",
    "\n",
    "# Example input image tensor (batch_size=1, channels=3, height=512, width=512)\n",
    "input_tensor = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBox Refinements: torch.Size([1, 4])\n",
      "tensor([[0.1030, 0.0754, 0.2179, 0.0481]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.ops import RoIAlign\n",
    "import torchvision.ops as ops\n",
    "\n",
    "device = device = torch.device(\"cuda\")\n",
    "\n",
    "def decode_boxes(deltas, anchors):\n",
    "    \"\"\"\n",
    "    Decode bounding box deltas to proposals.\n",
    "\n",
    "    Args:\n",
    "        deltas: [num_anchors, 4] Predicted bbox deltas (dx, dy, dw, dh).\n",
    "        anchors: [num_anchors, 4] Anchor boxes (x1, y1, x2, y2).\n",
    "\n",
    "    Returns:\n",
    "        decoded_boxes: [num_anchors, 4] Decoded boxes in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    # Convert anchors to center format (cx, cy, w, h)\n",
    "    anchors = ops.box_convert(anchors, in_fmt=\"xyxy\", out_fmt=\"cxcywh\")\n",
    "    anchor_cx, anchor_cy, anchor_w, anchor_h = anchors.split(1, dim=1)\n",
    "\n",
    "    # Decode deltas\n",
    "    dx, dy, dw, dh = deltas.split(1, dim=1)\n",
    "    pred_cx = dx * anchor_w + anchor_cx\n",
    "    pred_cy = dy * anchor_h + anchor_cy\n",
    "    pred_w = torch.exp(dw) * anchor_w\n",
    "    pred_h = torch.exp(dh) * anchor_h\n",
    "\n",
    "    # Convert back to (x1, y1, x2, y2)\n",
    "    decoded_boxes = torch.cat([\n",
    "        pred_cx - 0.5 * pred_w,  # x1\n",
    "        pred_cy - 0.5 * pred_h,  # y1\n",
    "        pred_cx + 0.5 * pred_w,  # x2\n",
    "        pred_cy + 0.5 * pred_h   # y2\n",
    "    ], dim=1)\n",
    "\n",
    "    return decoded_boxes\n",
    "\n",
    "\n",
    "def generate_anchors(base_size, scales, aspect_ratios, feature_size, stride, device):\n",
    "    \"\"\"\n",
    "    Generate anchors for the feature map.\n",
    "\n",
    "    Args:\n",
    "        base_size (int): Base size of the anchors.\n",
    "        scales (list of float): Scaling factors for the anchors.\n",
    "        aspect_ratios (list of float): Aspect ratios (width/height) for the anchors.\n",
    "        feature_size (tuple of int): Feature map size (height, width).\n",
    "        stride (int): Stride of the feature map relative to the input image.\n",
    "\n",
    "    Returns:\n",
    "        anchors (torch.Tensor): [num_anchors, 4] Anchor boxes (x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    anchors = []\n",
    "    for y in range(feature_size[0]):\n",
    "        for x in range(feature_size[1]):\n",
    "            cx = x * stride + stride / 2  # Center x\n",
    "            cy = y * stride + stride / 2  # Center y\n",
    "            for scale in scales:\n",
    "                for ratio in aspect_ratios:\n",
    "                    w = base_size * scale * (ratio ** 0.5)\n",
    "                    h = base_size * scale / (ratio ** 0.5)\n",
    "                    anchors.append([cx - w / 2, cy - h / 2, cx + w / 2, cy + h / 2])\n",
    "    anchors = torch.tensor(anchors, dtype=torch.float32, device=device)\n",
    "    return anchors\n",
    "\n",
    "\n",
    "def generate_proposals(rpn_cls_logits, rpn_bbox_preds, anchors, image_size, device, top_n=1):\n",
    "    batch_size = rpn_cls_logits.size(0)\n",
    "    proposals = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        # Flatten predictions and anchors\n",
    "        cls_logits = rpn_cls_logits[b].permute(1, 2, 0).reshape(-1, 2)\n",
    "        bbox_preds = rpn_bbox_preds[b].permute(1, 2, 0).reshape(-1, 4)\n",
    "        \n",
    "        # Apply softmax to classification logits\n",
    "        scores = F.softmax(cls_logits, dim=-1)[:, 1]  # Keep \"objectness\" score\n",
    "        \n",
    "        # Decode predicted deltas to proposals\n",
    "        decoded_boxes = decode_boxes(bbox_preds, anchors)\n",
    "        \n",
    "        # Clip proposals to image boundaries\n",
    "        decoded_boxes = ops.clip_boxes_to_image(decoded_boxes, image_size)\n",
    "        \n",
    "        # Filter proposals by score and apply NMS\n",
    "        keep = ops.nms(decoded_boxes, scores, iou_threshold=0.7)\n",
    "        keep = keep[:top_n]  # Keep top_n proposals (1 in this case)\n",
    "        \n",
    "        # Add batch index\n",
    "        batch_proposals = torch.cat(\n",
    "            [torch.full((len(keep), 1), b, dtype=torch.float32, device=decoded_boxes.device), \n",
    "             decoded_boxes[keep]], \n",
    "            dim=1\n",
    "        )\n",
    "        proposals.append(batch_proposals)\n",
    "\n",
    "    # Concatenate all proposals across batches\n",
    "    return torch.cat(proposals, dim=0)\n",
    "\n",
    "\n",
    "class SimpleRCNN(nn.Module):\n",
    "    def __init__(self, num_anchors=9):\n",
    "        super(SimpleRCNN, self).__init__()\n",
    "        \n",
    "        # Use pretrained ResNet50 as the backbone\n",
    "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Use layers up to the last conv layer\n",
    "        \n",
    "        # Freeze the initial layers to retain pretrained weights\n",
    "        for param in list(self.backbone.parameters())[:6]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Region Proposal Network (RPN)\n",
    "        self.rpn_conv = nn.Conv2d(2048, 512, kernel_size=3, padding=1)  # RPN Conv layer\n",
    "        self.rpn_cls = nn.Conv2d(512, num_anchors * 2, kernel_size=1)  # 2 class logits per anchor (object, no object)\n",
    "        self.rpn_reg = nn.Conv2d(512, num_anchors * 4, kernel_size=1)  # 4 bbox coords per anchor\n",
    "\n",
    "        # RoI Align (assumes a fixed feature map size of 7x7 for simplicity)\n",
    "        self.roi_align = RoIAlign((7, 7), spatial_scale=1.0 / 16, sampling_ratio=2)\n",
    "\n",
    "        # Fully connected layers for classification and regression\n",
    "        self.fc1 = nn.Linear(2048 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)  # Output bbox coords (x, y, w, h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: Extract features\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Step 2: Generate RPN outputs\n",
    "        rpn_feat = F.relu(self.rpn_conv(features))\n",
    "        rpn_cls_logits = self.rpn_cls(rpn_feat)\n",
    "        rpn_bbox_preds = self.rpn_reg(rpn_feat)\n",
    "\n",
    "        # Step 3: Generate anchors\n",
    "        feature_size = (features.size(2), features.size(3))  # (height, width) of feature map\n",
    "        stride = 16  # Assuming input image is downsampled by 16x in the backbone\n",
    "        anchors = generate_anchors(base_size=16, scales=[1.0, 2.0, 0.5], aspect_ratios=[0.5, 1.0, 2.0],\n",
    "                                    feature_size=feature_size, stride=stride, device=x.device)\n",
    "\n",
    "        # Step 4: Generate proposals from RPN outputs\n",
    "        proposals = generate_proposals(rpn_cls_logits, rpn_bbox_preds, anchors, x.size()[2:], device=x.device, top_n=1)\n",
    "\n",
    "        # Step 5: Perform RoI Align\n",
    "        roi_features = self.roi_align(features, proposals)\n",
    "\n",
    "        # Step 6: Predict bounding boxes\n",
    "        roi_flattened = roi_features.view(roi_features.size(0), -1)\n",
    "        fc1_out = F.relu(self.fc1(roi_flattened))\n",
    "        bbox = self.fc2(fc1_out)\n",
    "\n",
    "        return bbox, rpn_cls_logits, rpn_bbox_preds\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Instantiate the model\n",
    "model = SimpleRCNN()\n",
    "\n",
    "# Example input image tensor (batch_size=1, channels=3, height=512, width=512)\n",
    "input_tensor = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Forward pass\n",
    "bbox, rpn_cls_logits, rpn_bbox_preds = model(input_tensor)\n",
    "\n",
    "print(\"BBox Refinements:\", bbox.shape)\n",
    "print(bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[6.7338e+00, 2.1669e+02, 2.1322e+01, 2.4020e+02],\n",
      "        [7.3939e-02, 2.1984e+02, 1.4008e+01, 2.4506e+02],\n",
      "        [1.6245e+01, 2.4760e+02, 3.0937e+01, 2.7193e+02],\n",
      "        [1.1728e+01, 2.2938e+02, 2.6318e+01, 2.5386e+02],\n",
      "        [1.6404e+01, 1.6736e+02, 3.1472e+01, 1.9157e+02],\n",
      "        [4.1947e+00, 2.0261e+02, 1.8998e+01, 2.2696e+02],\n",
      "        [3.7853e+02, 2.2926e+02, 3.9305e+02, 2.5461e+02],\n",
      "        [4.8136e+02, 1.1674e+02, 4.9684e+02, 1.4060e+02],\n",
      "        [2.4170e+01, 2.3280e+02, 3.9172e+01, 2.5792e+02],\n",
      "        [2.7160e+01, 1.8698e+01, 4.1049e+01, 4.4315e+01],\n",
      "        [1.2006e+01, 2.5850e+02, 2.6945e+01, 2.8408e+02],\n",
      "        [6.4320e+00, 3.1730e+02, 2.0979e+01, 3.4268e+02],\n",
      "        [9.5328e+00, 2.7622e+02, 2.4986e+01, 3.0214e+02],\n",
      "        [9.3420e+00, 2.6525e+02, 2.3677e+01, 2.8911e+02],\n",
      "        [1.6185e-02, 7.8325e+01, 1.2723e+01, 1.0349e+02],\n",
      "        [1.4086e+01, 2.4168e+02, 2.8848e+01, 2.6631e+02],\n",
      "        [2.4506e+00, 6.0200e+01, 1.6104e+01, 8.5631e+01],\n",
      "        [1.6404e+01, 2.1890e+02, 3.1589e+01, 2.4413e+02],\n",
      "        [1.3051e-01, 2.3776e+02, 1.3765e+01, 2.6235e+02],\n",
      "        [2.1051e+01, 2.5043e+02, 3.5951e+01, 2.7504e+02],\n",
      "        [3.1006e+01, 1.6985e+02, 4.7024e+01, 1.9406e+02],\n",
      "        [2.0498e-01, 2.6851e+02, 1.3799e+01, 2.9306e+02],\n",
      "        [4.8649e+02, 2.2384e+02, 5.0172e+02, 2.4853e+02],\n",
      "        [0.0000e+00, 2.7833e+01, 1.4080e+01, 4.4294e+01],\n",
      "        [1.8859e+02, 2.5361e+02, 2.0354e+02, 2.7861e+02],\n",
      "        [2.1585e+01, 2.2372e+02, 3.6273e+01, 2.4927e+02],\n",
      "        [0.0000e+00, 2.2468e+01, 1.4732e+01, 3.8943e+01],\n",
      "        [4.0303e+02, 4.2126e-01, 4.1764e+02, 1.9227e+01],\n",
      "        [1.9862e+01, 1.5737e-01, 4.8032e+01, 8.1980e+00],\n",
      "        [2.1422e+01, 5.1189e-01, 3.5527e+01, 1.9493e+01],\n",
      "        [6.6265e-02, 1.0175e+02, 1.2665e+01, 1.2604e+02],\n",
      "        [4.0854e+02, 5.0330e-01, 4.2254e+02, 2.0313e+01],\n",
      "        [4.7007e+02, 3.9791e+02, 4.8442e+02, 4.2246e+02],\n",
      "        [5.2907e-01, 1.0927e+00, 6.9388e+01, 4.9691e+01],\n",
      "        [4.8003e+02, 4.5599e+02, 4.9504e+02, 4.8068e+02],\n",
      "        [1.1921e+01, 2.1154e+02, 2.6721e+01, 2.3644e+02],\n",
      "        [2.9549e+02, 7.7846e+01, 3.1089e+02, 1.0363e+02],\n",
      "        [1.9218e-01, 3.6040e+02, 1.3744e+01, 3.8511e+02],\n",
      "        [1.8418e+01, 2.3612e+02, 3.3353e+01, 2.6039e+02],\n",
      "        [1.3674e+01, 1.7208e+02, 2.9070e+01, 1.9769e+02],\n",
      "        [4.5034e+00, 3.0649e+02, 1.8833e+01, 3.3103e+02],\n",
      "        [4.6816e+02, 2.0680e+01, 4.8208e+02, 4.5567e+01],\n",
      "        [1.1691e+01, 3.1484e+02, 2.6612e+01, 3.3914e+02],\n",
      "        [8.9693e+00, 1.8397e+02, 2.3806e+01, 2.0841e+02],\n",
      "        [1.3895e+01, 1.9630e+02, 2.8583e+01, 2.2059e+02],\n",
      "        [1.4027e+01, 3.4912e+02, 2.8745e+01, 3.7433e+02],\n",
      "        [8.2313e-02, 3.1023e+02, 1.3800e+01, 3.3605e+02],\n",
      "        [3.7549e+02, 4.8173e+02, 3.8989e+02, 5.0742e+02],\n",
      "        [3.7625e+02, 1.1559e+00, 4.5788e+02, 4.9634e+01],\n",
      "        [4.7465e+02, 6.7285e+01, 5.1200e+02, 1.7524e+02],\n",
      "        [2.1798e+01, 4.9602e+02, 3.6008e+01, 5.1200e+02],\n",
      "        [4.8744e+02, 2.4857e+02, 5.0210e+02, 2.7340e+02],\n",
      "        [4.7982e+02, 3.1719e+02, 4.9481e+02, 3.4259e+02],\n",
      "        [9.3121e+00, 7.3387e-01, 2.3377e+01, 2.0293e+01],\n",
      "        [1.1554e+01, 1.6045e+02, 2.6048e+01, 1.8542e+02],\n",
      "        [2.9567e+02, 8.6990e+01, 3.1071e+02, 1.1163e+02],\n",
      "        [4.7222e+02, 4.2758e+01, 4.8697e+02, 6.8673e+01],\n",
      "        [2.6403e+01, 5.4983e-01, 4.0673e+01, 1.9972e+01],\n",
      "        [1.2988e+00, 1.9676e+02, 1.5926e+01, 2.2147e+02],\n",
      "        [2.7866e+02, 5.7484e+01, 2.9354e+02, 8.2701e+01],\n",
      "        [4.7433e+02, 4.3875e+02, 5.1200e+02, 5.1200e+02],\n",
      "        [3.9883e+02, 4.8523e+02, 4.1314e+02, 5.0974e+02],\n",
      "        [3.5708e+02, 4.7684e+02, 3.7187e+02, 5.0134e+02],\n",
      "        [1.3633e+01, 2.8871e+02, 2.8860e+01, 3.1292e+02],\n",
      "        [1.1929e-01, 3.9765e+02, 1.3383e+01, 4.2165e+02],\n",
      "        [4.8003e+02, 3.8157e+02, 4.9502e+02, 4.0685e+02],\n",
      "        [4.8473e+02, 2.8647e+02, 4.9927e+02, 3.1111e+02],\n",
      "        [4.8033e+02, 3.8214e+01, 4.9484e+02, 6.3474e+01],\n",
      "        [1.4597e+01, 2.6807e+02, 2.9390e+01, 2.9289e+02],\n",
      "        [2.1549e+01, 1.5834e+02, 3.6446e+01, 1.8306e+02],\n",
      "        [2.4415e+01, 6.5727e+01, 3.9408e+01, 9.0483e+01],\n",
      "        [6.6546e+00, 4.0593e+02, 2.0838e+01, 4.2933e+02],\n",
      "        [3.8536e+02, 4.6467e-01, 3.9963e+02, 2.0812e+01],\n",
      "        [6.4865e+00, 3.2704e+02, 2.0728e+01, 3.5141e+02],\n",
      "        [1.8314e+02, 4.8870e+02, 1.9790e+02, 5.1167e+02],\n",
      "        [4.3937e+00, 2.7313e+02, 1.9037e+01, 2.9720e+02],\n",
      "        [4.8457e+02, 1.0456e+02, 4.9943e+02, 1.2933e+02],\n",
      "        [4.8744e+02, 2.9465e+02, 5.0178e+02, 3.1976e+02],\n",
      "        [1.6294e+02, 1.4825e-01, 1.9065e+02, 8.1098e+00],\n",
      "        [3.1869e+01, 4.9698e-01, 4.6114e+01, 1.9993e+01],\n",
      "        [2.0858e+00, 2.9971e+02, 1.6755e+01, 3.2404e+02],\n",
      "        [4.9593e-01, 1.9041e-01, 2.6706e+01, 8.2586e+00],\n",
      "        [4.8390e+02, 4.4024e+02, 4.9926e+02, 4.6519e+02],\n",
      "        [2.3784e+01, 3.6517e+02, 3.9236e+01, 3.9067e+02],\n",
      "        [3.6883e+00, 3.6153e+02, 1.9004e+01, 3.8644e+02],\n",
      "        [4.8445e+02, 1.7516e+02, 4.9958e+02, 1.9999e+02],\n",
      "        [1.2965e-01, 4.4592e+02, 1.3587e+01, 4.7081e+02],\n",
      "        [4.8540e+02, 6.0089e+01, 5.0015e+02, 8.4981e+01],\n",
      "        [1.3343e+01, 3.6591e+02, 2.8539e+01, 3.9102e+02],\n",
      "        [1.6098e+01, 3.8856e+02, 3.1510e+01, 4.1296e+02],\n",
      "        [4.7228e+02, 3.5684e+02, 4.8695e+02, 3.8215e+02],\n",
      "        [4.1613e+00, 2.9174e+02, 1.8737e+01, 3.1597e+02],\n",
      "        [6.7515e+00, 2.8483e+02, 2.1087e+01, 3.0848e+02],\n",
      "        [4.7140e+02, 3.3064e+02, 4.8711e+02, 3.5568e+02],\n",
      "        [2.6553e+02, 6.5862e+01, 2.8072e+02, 9.1928e+01],\n",
      "        [6.1871e+00, 3.9802e+02, 2.1018e+01, 4.2205e+02],\n",
      "        [1.6047e+00, 2.0673e+02, 1.5734e+01, 2.3145e+02],\n",
      "        [2.4867e-01, 2.5316e+02, 1.3353e+01, 2.7750e+02],\n",
      "        [1.6590e+01, 3.6021e+02, 3.1941e+01, 3.8468e+02],\n",
      "        [4.0485e+02, 2.2818e+00, 4.6199e+02, 6.5352e+01]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.6387, 0.6362, 0.6359, 0.6307, 0.6282, 0.6282, 0.6277, 0.6253, 0.6243,\n",
      "        0.6237, 0.6224, 0.6223, 0.6220, 0.6215, 0.6205, 0.6198, 0.6194, 0.6185,\n",
      "        0.6184, 0.6181, 0.6176, 0.6174, 0.6170, 0.6164, 0.6162, 0.6161, 0.6155,\n",
      "        0.6154, 0.6151, 0.6146, 0.6140, 0.6140, 0.6139, 0.6137, 0.6136, 0.6135,\n",
      "        0.6133, 0.6133, 0.6131, 0.6129, 0.6127, 0.6126, 0.6126, 0.6118, 0.6103,\n",
      "        0.6099, 0.6097, 0.6097, 0.6096, 0.6096, 0.6094, 0.6093, 0.6085, 0.6080,\n",
      "        0.6080, 0.6079, 0.6073, 0.6072, 0.6071, 0.6069, 0.6064, 0.6064, 0.6063,\n",
      "        0.6062, 0.6061, 0.6059, 0.6059, 0.6058, 0.6058, 0.6058, 0.6055, 0.6053,\n",
      "        0.6042, 0.6040, 0.6039, 0.6037, 0.6037, 0.6036, 0.6035, 0.6032, 0.6025,\n",
      "        0.6025, 0.6022, 0.6022, 0.6022, 0.6021, 0.6020, 0.6019, 0.6018, 0.6015,\n",
      "        0.6013, 0.6010, 0.6007, 0.6006, 0.6005, 0.6001, 0.6001, 0.5997, 0.5995,\n",
      "        0.5994], device='cuda:0', grad_fn=<IndexBackward0>)}]\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2)\n",
    "\n",
    "# Example input image tensor (batch_size=1, channels=3, height=512, width=512)\n",
    "input_tensor = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Forward pass\n",
    "bbox = model(input_tensor)\n",
    "\n",
    "print(bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 90\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, device, optimizer, scheduler, checkpoint_path)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Validate the model\u001b[39;00m\n\u001b[1;32m     93\u001b[0m val_iou \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n",
      "Cell \u001b[0;32mIn[34], line 30\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Forward pass: Pass only images to the model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate the loss (you need to handle the loss computation inside the model or separately)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(outputs, targets)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 37\u001b[0m, in \u001b[0;36mFasterRCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Step 1: Extract features\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Step 2: Generate RPN outputs\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     rpn_feat \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn_conv(features))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "# Define IoU computation\n",
    "def calculate_iou(pred_boxes, target_boxes):\n",
    "    \"\"\"\n",
    "    Computes IoU between predicted and target boxes.\n",
    "    Args:\n",
    "        pred_boxes (Tensor): Predicted boxes, shape (N, 4).\n",
    "        target_boxes (Tensor): Target boxes, shape (M, 4).\n",
    "    Returns:\n",
    "        IoU scores: Tensor of shape (N, M).\n",
    "    \"\"\"\n",
    "    return ops.box_iou(pred_boxes, target_boxes)\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = torch.stack([img.to(device) for img in images])\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Pass only images to the model\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate the loss (you need to handle the loss computation inside the model or separately)\n",
    "        total_loss = model.compute_loss(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_iou = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Validation\", leave=False):\n",
    "            # Move data to device\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = model(images)  # During evaluation, model outputs predictions\n",
    "            \n",
    "            # Compute IoU between predicted and target boxes\n",
    "            for output, target in zip(outputs, targets):\n",
    "                pred_boxes = output[\"boxes\"].detach()\n",
    "                target_boxes = target[\"boxes\"]\n",
    "\n",
    "                if len(pred_boxes) > 0 and len(target_boxes) > 0:\n",
    "                    iou = box_iou(pred_boxes, target_boxes).mean().item()\n",
    "                    total_iou += iou\n",
    "                else:\n",
    "                    total_iou += 0  # No predictions or no targets\n",
    "\n",
    "                num_samples += 1\n",
    "\n",
    "    # Calculate average IoU\n",
    "    avg_iou = total_iou / num_samples if num_samples > 0 else 0.0\n",
    "\n",
    "    return avg_iou\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, optimizer, scheduler, checkpoint_path):\n",
    "    best_iou = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train the model\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "\n",
    "        # Validate the model\n",
    "        val_iou = evaluate(model, val_loader, device)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_iou > best_iou:\n",
    "            best_iou = val_iou\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model saved with IoU: {best_iou:.4f}\")\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation IoU: {val_iou:.4f}\")\n",
    "\n",
    "    print(f\"Training complete. Best Validation IoU: {best_iou:.4f}\")\n",
    "\n",
    "\n",
    "# Set up optimizer, scheduler, and device\n",
    "#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2).to(device)\n",
    "model = FasterRCNN(num_classes=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "checkpoint_path = \"best_model.pth\"\n",
    "num_epochs = 10\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    val_data_loader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    checkpoint_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "output_directory = \"2024-fall-ml-3-hw-4-wheres-waldo/outputs\"\n",
    "if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "# Function to predict bounding box on a new image\n",
    "# Function to predict bounding box on a new image\n",
    "# Function to predict bounding box on a new image\n",
    "def predict(model, image_path, transform=None):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Open image and convert to RGB\n",
    "    orig_width, orig_height = image.size  # Get original image dimensions\n",
    "    if transform:\n",
    "        image = transform(image)  # Apply transformations\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Ensure image is a Tensor and move it to the correct device\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Predict the bounding box\n",
    "    with torch.no_grad():\n",
    "        # Get the model's output (a list of dictionaries in this case)\n",
    "        outputs = model(image)\n",
    "        if isinstance(outputs, list):  # Check if the output is a list\n",
    "            outputs = outputs[0]  # Get the first dictionary in the list\n",
    "        \n",
    "        # Check if any bounding boxes were predicted\n",
    "        if len(outputs['boxes']) == 0:\n",
    "            print(f\"No objects detected in {image_path}\")\n",
    "            return None  # No detection, return None or handle as needed\n",
    "        \n",
    "        # Extract the predicted bounding box (assuming the first detected object)\n",
    "        predicted_bbox = outputs['boxes'][0].cpu().numpy()  # Assuming 'boxes' contains the bounding boxes\n",
    "\n",
    "    # Scale bbox back to original image dimensions\n",
    "    predicted_bbox[0] *= orig_width / image_sz  # x_min\n",
    "    predicted_bbox[1] *= orig_height / image_sz  # y_min\n",
    "    predicted_bbox[2] *= orig_width / image_sz  # x_max\n",
    "    predicted_bbox[3] *= orig_height / image_sz  # y_max\n",
    "\n",
    "    return predicted_bbox\n",
    "\n",
    "\n",
    "# Define image transformations (resize, normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_sz, image_sz)),  # Resize the image to 512x512\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "#For each image in test folder, predict, draw bounding box, save image, and save into csv file for submission\n",
    "test_images = [img for img in os.listdir(test_folder) if img.endswith(\".jpg\")]\n",
    "predictions = []\n",
    "\n",
    "for name in test_images:\n",
    "    image_path = os.path.join(test_folder, name)  # Replace with the path to your test image\n",
    "    predicted_bbox = predict(model, image_path, transform)\n",
    "    \n",
    "    if predicted_bbox is None:\n",
    "        continue  # Skip the image if no bounding box is detected\n",
    "\n",
    "    # Print the predicted bounding box (x_min, y_min, x_max, y_max)\n",
    "    print(\"Predicted Bounding Box:\", predicted_bbox)\n",
    "\n",
    "    # Plot the image and the predicted bounding box\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Add the bounding box only if a prediction was made\n",
    "    plt.gca().add_patch(plt.Rectangle(\n",
    "        (predicted_bbox[0], predicted_bbox[1]),  # (x_min, y_min)\n",
    "        predicted_bbox[2] - predicted_bbox[0],  # Width (x_max - x_min)\n",
    "        predicted_bbox[3] - predicted_bbox[1],  # Height (y_max - y_min)\n",
    "        linewidth=2, edgecolor='r', facecolor='none'\n",
    "    ))\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig(os.path.join(output_directory, name), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Extract bounding box coordinates\n",
    "    x_min, y_min, x_max, y_max = predicted_bbox\n",
    "    predictions.append([name, x_min, y_min, x_max, y_max])\n",
    "\n",
    "# Save predictions to CSV\n",
    "df = pd.DataFrame(predictions, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "df.to_csv(os.path.join(output_directory, 'predictions.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
