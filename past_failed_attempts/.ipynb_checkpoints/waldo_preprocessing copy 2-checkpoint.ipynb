{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Wheres Waldo?\n",
    "### Name: Eileanor LaRocco\n",
    "In this assignment, you will develop an object detection algorithm to locate Waldo in a set of images. You will develop a model to detect the bounding box around Waldo. Your final task is to submit your predictions on Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process/Issues\n",
    "- Double-checked that the images we were given were correctly bounded (did this by visualizing the boxes on the images - they look good!)\n",
    "- Complication: Originally when I creating augmented images, the bounding box labels did not also augment. I also had to try out a few types of augmentation to see what made sense for waldo. The augmented images may still not be as different from one another as they could be which could allow the model to favor the training images that occur more frequently.\n",
    "- Complication: Similarly, when resizing the images, ensuring the bounding boxes not only are also adjusted if necessary, but ensuring they do not get cut off and the image is not stretched/shrunk too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/albumentations/check_version.py:51: UserWarning: Error fetching version info <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import opendatasets as od\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading 2024-fall-ml-3-hw-4-wheres-waldo.zip to ./2024-fall-ml-3-hw-4-wheres-waldo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38.2M/38.2M [00:02<00:00, 19.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./2024-fall-ml-3-hw-4-wheres-waldo/2024-fall-ml-3-hw-4-wheres-waldo.zip to ./2024-fall-ml-3-hw-4-wheres-waldo\n"
     ]
    }
   ],
   "source": [
    "od.download('https://www.kaggle.com/competitions/2024-fall-ml-3-hw-4-wheres-waldo/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train\" # Original Train Images\n",
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\" # Original Test Images\n",
    "annotations_file = \"2024-fall-ml-3-hw-4-wheres-waldo/annotations.csv\" # Original Annotations File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Images (Crop/Augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train and validation sets\n",
    "annotations = pd.read_csv(annotations_file)\n",
    "image_files = annotations[\"filename\"].unique()\n",
    "train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv_by_column(input_csv, output_csv, column_name, values_list):\n",
    "    \"\"\"\n",
    "    Filters rows in a CSV file and keeps only those where the specified column's value is in a given list.\n",
    "\n",
    "    Parameters:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to save the filtered CSV file.\n",
    "        column_name (str): Column to filter on.\n",
    "        values_list (list): List of values to keep.\n",
    "    \"\"\"\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df[column_name].isin(values_list)]\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    filtered_df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split annotations into train and val\n",
    "values_list = list(train_images)\n",
    "\n",
    "# Example usage\n",
    "output_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\"  # Replace with your output file path\n",
    "column_name = \"filename\"  # Replace with the column you want to filter\n",
    "\n",
    "filter_csv_by_column(annotations_file, output_csv, column_name, values_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "values_list = list(val_images)\n",
    "\n",
    "# Example usage\n",
    "output_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\"  # Replace with your output file path\n",
    "column_name = \"filename\"  # Replace with the column you want to filter\n",
    "\n",
    "filter_csv_by_column(annotations_file, output_csv, column_name, values_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved: 18.jpg\n",
      "Moved: 5.jpg\n",
      "Moved: 19.jpg\n",
      "Moved: 22.jpg\n",
      "Moved: 1.jpg\n",
      "Moved: 3.jpg\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def split_directory(source_dir, target_dir, file_list):\n",
    "    \"\"\"Splits files from source_dir to target_dir based on file_list.\"\"\"\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    for file_name in file_list:\n",
    "        source_path = os.path.join(source_dir, file_name)\n",
    "        target_path = os.path.join(target_dir, file_name)\n",
    "\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.move(source_path, target_path)\n",
    "            print(f\"Moved: {file_name}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_dir = train_folder\n",
    "    target_dir = \"2024-fall-ml-3-hw-4-wheres-waldo/train/val\"\n",
    "    file_list = list(val_images)\n",
    "\n",
    "    split_directory(source_dir, target_dir, file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "class WaldoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transforms=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = F.to_tensor(image)\n",
    "        \n",
    "        # Read bounding box data, ensuring all are converted to float\n",
    "        box_data = self.img_labels.iloc[idx, 4:8].values\n",
    "        boxes = []\n",
    "        for item in box_data:\n",
    "            try:\n",
    "                boxes.append(float(item))\n",
    "            except ValueError as e:\n",
    "                raise ValueError(f\"Error converting bounding box data to float: {e}\")\n",
    "\n",
    "        # Create tensors\n",
    "        boxes = torch.as_tensor([boxes], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "            image = np.array(image)\n",
    "            target = np.array(target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Create the dataset\n",
    "train_dataset = WaldoDataset(annotations_file= \"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\"\n",
    "                             , img_dir=train_folder\n",
    "                             , transforms = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.Resize((224, 224)),\n",
    "                                            #torchvision.transforms.ToTensor(),\n",
    "                                            #torchvision.transforms.ToPILImage()\n",
    "                                        ]))\n",
    "val_dataset = WaldoDataset(annotations_file= \"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\"\n",
    "                           , img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/val\"\n",
    "                        , transforms = torchvision.transforms.Compose([\n",
    "                                        torchvision.transforms.Resize((224, 224)),\n",
    "                                        #torchvision.transforms.ToTensor(),\n",
    "                                        #torchvision.transforms.ToPILImage()\n",
    "                                    ]))\n",
    "\n",
    "# Now, you can use this dataset with a DataLoader to train your model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: zip(*x)\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: zip(*x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch\n",
      "torch.Size([16, 3, 416, 416])\n",
      "torch.Size([16, 4])\n",
      "Training batch\n",
      "torch.Size([11, 3, 416, 416])\n",
      "torch.Size([11, 4])\n",
      "Validation batch\n",
      "torch.Size([7, 3, 416, 416])\n",
      "torch.Size([7, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, img_size=(416, 416), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            img_size (tuple): Desired image size (height, width) for YOLO input.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the row corresponding to the image\n",
    "        img_name = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        # Get original image dimensions\n",
    "        original_width, original_height = image.size\n",
    "        \n",
    "        # Transform image if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get the bounding box information (x_center, y_center, width, height)\n",
    "        bbox = self.annotations.iloc[idx, 4:].values.astype('float32')\n",
    "        \n",
    "        # Normalize the bounding box coordinates to [0, 1] range based on the original image size\n",
    "        x_center, y_center, w, h = bbox\n",
    "        x_center /= original_width\n",
    "        y_center /= original_height\n",
    "        w /= original_width\n",
    "        h /= original_height\n",
    "        \n",
    "        # Package the image and bounding box into a dictionary\n",
    "        target = torch.tensor([x_center, y_center, w, h])\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    \n",
    "    # Stack the images into a tensor\n",
    "    images = torch.stack(images, 0)\n",
    "    \n",
    "    # Stack the targets into a tensor\n",
    "    targets = torch.stack(targets, 0)\n",
    "    \n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# Example of how to instantiate and use the dataset and DataLoader\n",
    "\n",
    "# Image transformations (resize, normalization, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = YOLODataset(csv_file='2024-fall-ml-3-hw-4-wheres-waldo/annotations.csv', img_dir='2024-fall-ml-3-hw-4-wheres-waldo/train/train/', transform=transform)\n",
    "\n",
    "# Split dataset into training and validation sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Instantiate the DataLoader for both training and validation sets\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate through the training DataLoader\n",
    "for imgs, targets in train_dataloader:\n",
    "    print(\"Training batch\")\n",
    "    print(imgs.shape)  # Should be [batch_size, 3, 416, 416]\n",
    "    print(targets.shape)  # Should be [batch_size, 4]\n",
    "\n",
    "# Iterate through the validation DataLoader\n",
    "for imgs, targets in val_dataloader:\n",
    "    print(\"Validation batch\")\n",
    "    print(imgs.shape)  # Should be [batch_size, 3, 416, 416]\n",
    "    print(targets.shape)  # Should be [batch_size, 4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 208, 208])\n",
      "torch.Size([1, 18, 208, 208])\n",
      "torch.Size([1, 18, 208, 208])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleYOLOv3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleYOLOv3, self).__init__()\n",
    "\n",
    "        # Backbone: Feature extractor (simplified)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Add more convolutional layers as needed\n",
    "        )\n",
    "\n",
    "        # Detection head (simplified)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, (5 + num_classes) * 3, 1),  # 1 bounding boxes per cell\n",
    "        )\n",
    "\n",
    "        #self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        print(x.size())\n",
    "        x = self.head(x)\n",
    "        print(x.size())\n",
    "        #x = x.view(x.size(0), 3, self.num_classes + 5, x.size(2), x.size(3)) \n",
    "        return x \n",
    "\n",
    "# Instantiate and check the model\n",
    "model = SimpleYOLOv3(num_classes=1)\n",
    "input_image = torch.randn(1, 3, 416, 416)  # Example batch\n",
    "output = model(input_image)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "torch.Size([16, 16, 208, 208])\n",
      "torch.Size([16, 18, 208, 208])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (208) must match the size of tensor b (16) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     evaluate(model, val_data_loader, device)\n",
      "Cell \u001b[0;32mIn[69], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     46\u001b[0m true_boxes \u001b[38;5;241m=\u001b[39m targets[:, :\u001b[38;5;241m4\u001b[39m]  \u001b[38;5;66;03m# Ground truth boxes\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Losses\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m loss_loc \u001b[38;5;241m=\u001b[39m \u001b[43miou_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_boxes\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# IoU loss\u001b[39;00m\n\u001b[1;32m     50\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_loc\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[69], line 28\u001b[0m, in \u001b[0;36miou_loss\u001b[0;34m(pred_boxes, true_boxes)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miou_loss\u001b[39m(pred_boxes, true_boxes):\n\u001b[0;32m---> 28\u001b[0m     iou \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m iou\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[69], line 13\u001b[0m, in \u001b[0;36mcompute_iou\u001b[0;34m(pred_boxes, true_boxes)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_iou\u001b[39m(pred_boxes, true_boxes):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# pred_boxes and true_boxes should be in (x_min, y_min, x_max, y_max)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     inter_xmin \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_boxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     inter_ymin \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(pred_boxes[:, \u001b[38;5;241m1\u001b[39m], true_boxes[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     15\u001b[0m     inter_xmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(pred_boxes[:, \u001b[38;5;241m2\u001b[39m], true_boxes[:, \u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (208) must match the size of tensor b (16) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "model = SimpleYOLOv3(num_classes=1)\n",
    "\n",
    "# IoU calculation\n",
    "def compute_iou(pred_boxes, true_boxes):\n",
    "    # pred_boxes and true_boxes should be in (x_min, y_min, x_max, y_max)\n",
    "    inter_xmin = torch.max(pred_boxes[:, 0], true_boxes[:, 0])\n",
    "    inter_ymin = torch.max(pred_boxes[:, 1], true_boxes[:, 1])\n",
    "    inter_xmax = torch.min(pred_boxes[:, 2], true_boxes[:, 2])\n",
    "    inter_ymax = torch.min(pred_boxes[:, 3], true_boxes[:, 3])\n",
    "\n",
    "    inter_area = torch.clamp(inter_xmax - inter_xmin, min=0) * torch.clamp(inter_ymax - inter_ymin, min=0)\n",
    "    pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "    true_area = (true_boxes[:, 2] - true_boxes[:, 0]) * (true_boxes[:, 3] - true_boxes[:, 1])\n",
    "\n",
    "    union_area = pred_area + true_area - inter_area\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "# Simple IoU loss function\n",
    "def iou_loss(pred_boxes, true_boxes):\n",
    "    iou = compute_iou(pred_boxes, true_boxes)\n",
    "    return 1 - iou.mean()  # We want to maximize IoU, so minimize 1 - IoU\n",
    "\n",
    "# Custom YOLOv3 training loop\n",
    "def train(model, train_data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, targets in train_data_loader:\n",
    "        images = images#.to(device)\n",
    "        targets = targets#.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Extract predicted boxes (assuming only bounding box prediction)\n",
    "        pred_boxes = predictions[:, :4]  # first 4 are bounding box coordinates\n",
    "\n",
    "        # Assume targets only contain bounding box coordinates (no objectness or class)\n",
    "        true_boxes = targets[:, :4]  # Ground truth boxes\n",
    "\n",
    "        # Losses\n",
    "        loss_loc = iou_loss(pred_boxes, true_boxes)  # IoU loss\n",
    "        total_loss += loss_loc.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss_loc.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_data_loader)\n",
    "    print(f\"Training loss: {avg_loss}\")\n",
    "\n",
    "# Evaluation (testing) function\n",
    "def evaluate(model, val_data_loader, device):\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_data_loader:\n",
    "            images = images#.to(device)\n",
    "            targets = targets#.to(device)\n",
    "\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Extract predicted boxes and target boxes\n",
    "            pred_boxes = predictions[:, :4]\n",
    "            true_boxes = targets[:, :4]\n",
    "\n",
    "            # Calculate IoU for the batch\n",
    "            iou = compute_iou(pred_boxes, true_boxes)\n",
    "            total_iou += iou.mean().item()\n",
    "\n",
    "    avg_iou = total_iou / len(val_data_loader)\n",
    "    print(f\"Average IoU on test set: {avg_iou}\")\n",
    "\n",
    "# Initialize model, optimizer, and device\n",
    "model = SimpleYOLOv3(num_classes=1)#.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train(model, train_data_loader, optimizer, device)\n",
    "    evaluate(model, val_data_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO model\n",
    "#model = YOLO(\"yolov5su.pt\")  # Load pretrained weights\n",
    "#model.train(data=\"yolo.yaml\", epochs=15, imgsz=640, pretrained=True, augment=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 210.5ms\n",
      "1: 640x640 (no detections), 210.5ms\n",
      "2: 640x640 (no detections), 210.5ms\n",
      "3: 640x640 (no detections), 210.5ms\n",
      "4: 640x640 (no detections), 210.5ms\n",
      "5: 640x640 (no detections), 210.5ms\n",
      "6: 640x640 (no detections), 210.5ms\n",
      "7: 640x640 (no detections), 210.5ms\n",
      "8: 640x640 (no detections), 210.5ms\n",
      "Speed: 1.5ms preprocess, 210.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolo_test_predictions/train2\u001b[0m\n",
      "0 label saved to yolo_test_predictions/train2/labels\n",
      "Predictions saved to yolo_test_predictions/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\"\n",
    "\n",
    "# Predict on test images\n",
    "test_images = [os.path.join(test_folder, img) for img in os.listdir(test_folder) if img.endswith(\".jpg\")]\n",
    "results = model.predict(source=test_images, save=True, save_txt=True, project=\"yolo_test_predictions\")\n",
    "\n",
    "# Prepare to save the predictions\n",
    "output_csv_path = os.path.join(\"yolo_test_predictions\", \"predictions.csv\")\n",
    "predictions = []\n",
    "\n",
    "# Process results\n",
    "for result in results:\n",
    "    image_name = os.path.basename(result.path)  # Get the image name\n",
    "    if result.boxes is not None and len(result.boxes) > 0:  # Check if there are predictions\n",
    "        # Convert result.boxes to tensor for easier access\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()  # Convert bounding boxes to array\n",
    "        confidences = result.boxes.conf.cpu().numpy()  # Convert confidence scores to array\n",
    "\n",
    "        # Find the index of the box with the highest confidence\n",
    "        best_idx = confidences.argmax()\n",
    "        best_box = boxes[best_idx]\n",
    "        conf = confidences[best_idx]\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        x_min, y_min, x_max, y_max = best_box\n",
    "        predictions.append([image_name, x_min, y_min, x_max, y_max, conf])\n",
    "    else:\n",
    "        # No predictions for this image\n",
    "        predictions.append([image_name, None, None, None, None, None])\n",
    "\n",
    "# Save predictions to CSV\n",
    "df = pd.DataFrame(predictions, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"confidence\"])\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
