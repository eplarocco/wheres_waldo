{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79dfb172-1d03-4c60-8473-e9cd0d8abe7e",
   "metadata": {},
   "source": [
    "# Assignment 4: Wheres Waldo?\n",
    "### Name: Eileanor LaRocco\n",
    "In this assignment, you will develop an object detection algorithm to locate Waldo in a set of images. You will develop a model to detect the bounding box around Waldo. Your final task is to submit your predictions on Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b4eec-db14-4c9a-bc16-a6f6d848b74f",
   "metadata": {},
   "source": [
    "Hello - I have worked on this for 7 days straight, spending close to 60 hours on this assignment (if not more) at this point and I am about to cry. I have been debugging for most of those hours, Rivanna keeps not giving me sessions for hours and sometimes when it does it doesn’t even partition me a gpu. I am going mad and I have no intentions of ever creating a neural network from scratch ever again in my research or future career. Here is what I spent the 60 hours completing including just a few of the many issues I encountered:\n",
    "\n",
    "##### Data preprocessing:\n",
    "First I examined the training images. I noticed that many of the images looked like the test images but a few did not. Additionally, a few images had multiple “Waldo” bounding boxes labeled in the annotations.csv.\n",
    "\n",
    "I tried two main data preprocessing techniques:\n",
    "\n",
    "1. Following the method in this paper (https://cs231n.stanford.edu/2024/papers/development-of-waldonet-a-novel-approach-to-solving-wheres-waldo.pdf) I first saved images of Waldo’s head to a folder using the annotations.csv. I then removed any that were too large, too few pixels to be useful, or did not look like how Waldo normally presents in the busy scenes (he tends to be facing sideways vs front-on). I then cropped each training image (only the ones with the busy background scenes) into 128x128 sections and randomly superimposed a random Waldo head onto each, saving new bounding box coordinates to a csv file. \n",
    "2. I thought that maybe 128x128 was too small of an image to learn on and then extrapolate to 1000-2000 pixel sized images (possibly why predictions on the test set were so far off). I also thought that maybe superimposing Waldo was not behaving as I thought. So for the second method I tried, I randomly cropped each training image (only the ones with the busy background scenes) into 512x512 pixel sections that all contained the full original waldo bounding boxes. This way waldo still presented as he normally does but his location is still different within each training image.\n",
    "\n",
    "I then split the resulting images into training and testing sets and used them in the data loader code we were provided (although I did have to change part of it as my new annotations had different column indexes for the bounding box coordinates compared to the annotations folder we were provided. I then saved the training images with the bounding boxes overlayed so I could visually check that the new bounding boxes were correct (this was an issue for a bit as they weren’t resizing correctly and therefore our-of-bounds from the new image size).\n",
    "\n",
    "Both processing techniques probably would have worked equally well, however, my model was not working properly and I thought that maybe it was because I superimposed Waldo without the background removed and that was what wasn’t allowing it to learn (jt was not).\n",
    "\n",
    "\n",
    "##### Model Building:\n",
    "Where I spent about 30-40 of the hours. I have no idea how we were supposed to go from implementing resnet50 from scratch to faster rcnn or yolo. It is a wild jump in understanding of both tensorflow and neural network architecture. I tried though.\n",
    "\n",
    "From my understanding faster rcnn contains the following parts:\n",
    "Resnet50 backbone to extract features from the input image\n",
    "Regional Proposal Network (RPN) to propose the regions most likely to contain the waldo using anchor boxes with different scales and aspect ratios to do so so that waldo can be predicted no matter his size\n",
    "Region of Interests (ROIs) - Classification (target vs background) and Regression (bounding boxes) pieces\n",
    "\n",
    "How this translates without a wild amount of errors to tensor flow still evades me. Because of this, I first tried a pretty simple neural network with 3 convolution layers. As you could assume, this did not perform very well as it was literally just a cnn that produced a tensor of 4 values for bounding box coordinates. The model did not learn anything while training - the validation iou stayed at 0 and even the training loss barely improved. I did think this was strange because after enough iterations even with a terrible model the training loss should decrease. Because of this I tried using PyTorch’s fasterrcnn_resnet50_fpn model to see if there was an issue with my training images. There was not - after just 10 epochs the PyTorch model was able to find waldo in one of the test images. So from there the only option left was to create a faster rcnn model from scratch. I tried many ways to implement this and I never ended up with a model that fully worked. Either the model worked on a random test input tensor but did not work within my training/evaluation loop or it didn’t work at all. A large part of the issues revolved around computing the training loss.\n",
    "\n",
    "At some point I did get a small cnn version working (which was how I got the predictions.csv I submitted to Kaggle with a 0 IOU) but after 20 iterations I now have no idea where the code is that did that. The code I am submitting does not work but it includes my attempt at coding something similar to a faster rcnn model.\n",
    "\n",
    "##### Training/Evaluation:\n",
    "I created a training and testing loop to train the model. For the training function I set it up to calculate loss given the image and target since that is what faster rcnn does. For the evaluation function I set it up to calculate iou as that is what the Kaggle competition is training on and I wanted to save the model with the best iou score. I originally set up the training and testing loop to assume an input similar to the faster rccn model in PyTorch with output of bounding boxes, labels, and scores for each image. This created a lot of errors when I tried to use my own model with this training loop as my model did not include a loss function calculated in the forward pass as the faster rcnn PyTorch model does.\n",
    "\n",
    "\n",
    "##### Predictions:\n",
    "Given that the images I trained on were of the same size (512x512) and the images we were given to test on were larger and of differing sizes, I transformed the test images similarly (resizing and normalizing) and once predicting the bounding boxes given the model (predicting on the resized images) I had to transform the predicted bounding boxes to scale back to the original test image size. I then saved each annotation and file name to the csv to create the submission file and saved the test images with the bounding boxes drawn on them to check the predictions visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04eaa232-7212-4e66-a6e0-afe762232827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35adc85c-0eb7-483c-8f96-e9df9d45cac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = device = torch.device(\"cuda\") #mps/cuda\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747f6593-83cf-45fb-9252-ff8295819a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train\" # Original Train Images\n",
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\" # Original Test Images\n",
    "annotations_file = \"2024-fall-ml-3-hw-4-wheres-waldo/annotations.csv\" # Original Annotations File\n",
    "image_sz = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7c2ffa-c6b2-41f2-9e9e-bc207ca33687",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dce73f1-c817-4e89-ace3-54b2115b601c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataset (Train and Test Loaders)\n",
    "class WaldoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transforms=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        #image = torchvision.transforms.ToTensor()(image)  # Convert to tensor\n",
    "        \n",
    "        box_data = self.img_labels.iloc[idx, 1:].values\n",
    "        boxes = [float(item) for item in box_data]\n",
    "        \n",
    "        boxes = torch.as_tensor([boxes], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Set up the dataset and data loaders\n",
    "train_dataset = WaldoDataset(\n",
    "    annotations_file=\"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\",\n",
    "    img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/chunks\",\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((image_sz, image_sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_dataset = WaldoDataset(\n",
    "    annotations_file=\"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\",\n",
    "    img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/val\",\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((image_sz, image_sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: list(zip(*x))\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: list(zip(*x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5867901-f082-4f7f-bbb6-1cd36a3bdec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset (Train and Test Loaders)\n",
    "class WaldoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transforms=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        #image = torchvision.transforms.ToTensor()(image)  # Convert to tensor\n",
    "        \n",
    "        box_data = self.img_labels.iloc[idx, 1:].values\n",
    "        boxes = [float(item) for item in box_data]\n",
    "        \n",
    "        boxes = torch.as_tensor([boxes], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Set up the dataset and data loaders\n",
    "train_dataset = WaldoDataset(\n",
    "    annotations_file=\"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\",\n",
    "    img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/chunks\",\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((image_sz, image_sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_dataset = WaldoDataset(\n",
    "    annotations_file=\"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\",\n",
    "    img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/val\",\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((image_sz, image_sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: list(zip(*x))\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: list(zip(*x))\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.ops import RoIAlign\n",
    "import torchvision.ops as ops\n",
    "\n",
    "\n",
    "def decode_boxes(deltas, anchors):\n",
    "    # Convert anchors to center format (cx, cy, w, h)\n",
    "    anchors = ops.box_convert(anchors, in_fmt=\"xyxy\", out_fmt=\"cxcywh\")\n",
    "    anchor_cx, anchor_cy, anchor_w, anchor_h = anchors.split(1, dim=1)\n",
    "\n",
    "    # Decode deltas\n",
    "    dx, dy, dw, dh = deltas.split(1, dim=1)\n",
    "    pred_cx = dx * anchor_w + anchor_cx\n",
    "    pred_cy = dy * anchor_h + anchor_cy\n",
    "    pred_w = torch.exp(dw) * anchor_w\n",
    "    pred_h = torch.exp(dh) * anchor_h\n",
    "\n",
    "    # Convert back to (x1, y1, x2, y2)\n",
    "    decoded_boxes = torch.cat([\n",
    "        pred_cx - 0.5 * pred_w,  # x1\n",
    "        pred_cy - 0.5 * pred_h,  # y1\n",
    "        pred_cx + 0.5 * pred_w,  # x2\n",
    "        pred_cy + 0.5 * pred_h   # y2\n",
    "    ], dim=1)\n",
    "\n",
    "    return decoded_boxes\n",
    "\n",
    "\n",
    "def generate_anchors(base_size, scales, aspect_ratios, feature_size, stride, device):\n",
    "    anchors = []\n",
    "    for y in range(feature_size[0]):\n",
    "        for x in range(feature_size[1]):\n",
    "            cx = x * stride + stride / 2  # Center x\n",
    "            cy = y * stride + stride / 2  # Center y\n",
    "            for scale in scales:\n",
    "                for ratio in aspect_ratios:\n",
    "                    w = base_size * scale * (ratio ** 0.5)\n",
    "                    h = base_size * scale / (ratio ** 0.5)\n",
    "                    anchors.append([cx - w / 2, cy - h / 2, cx + w / 2, cy + h / 2])\n",
    "    anchors = torch.tensor(anchors, dtype=torch.float32, device=device)\n",
    "    return anchors\n",
    "\n",
    "\n",
    "def generate_proposals(rpn_cls_logits, rpn_bbox_preds, anchors, image_size, device, top_n=1):\n",
    "    batch_size = rpn_cls_logits.size(0)\n",
    "    proposals = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        # Flatten predictions and anchors\n",
    "        cls_logits = rpn_cls_logits[b].permute(1, 2, 0).reshape(-1, 2)\n",
    "        bbox_preds = rpn_bbox_preds[b].permute(1, 2, 0).reshape(-1, 4)\n",
    "        \n",
    "        # Apply softmax to classification logits\n",
    "        scores = F.softmax(cls_logits, dim=-1)[:, 1]\n",
    "        \n",
    "        # Decode predicted deltas to proposals\n",
    "        decoded_boxes = decode_boxes(bbox_preds, anchors)\n",
    "        \n",
    "        # Clip proposals to image boundaries\n",
    "        decoded_boxes = ops.clip_boxes_to_image(decoded_boxes, image_size)\n",
    "        \n",
    "        # Filter proposals by score and apply NMS\n",
    "        keep = ops.nms(decoded_boxes, scores, iou_threshold=0.7)\n",
    "        keep = keep[:top_n]  # Keep top_n proposals (1 in this case)\n",
    "        \n",
    "        # Add batch index\n",
    "        batch_proposals = torch.cat(\n",
    "            [torch.full((len(keep), 1), b, dtype=torch.float32, device=decoded_boxes.device), \n",
    "             decoded_boxes[keep]], \n",
    "            dim=1\n",
    "        )\n",
    "        proposals.append(batch_proposals)\n",
    "\n",
    "    # Concatenate all proposals across batches\n",
    "    return torch.cat(proposals, dim=0)\n",
    "\n",
    "\n",
    "class SimpleRCNN(nn.Module):\n",
    "    def __init__(self, num_anchors=9):\n",
    "        super(SimpleRCNN, self).__init__()\n",
    "        \n",
    "        # Use pretrained ResNet50 as the backbone\n",
    "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Use layers up to the last conv layer\n",
    "        \n",
    "        # Freeze the initial layers to retain pretrained weights\n",
    "        for param in list(self.backbone.parameters())[:6]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Region Proposal Network (RPN)\n",
    "        self.rpn_conv = nn.Conv2d(2048, 512, kernel_size=3, padding=1)  # RPN Conv layer\n",
    "        self.rpn_cls = nn.Conv2d(512, num_anchors * 2, kernel_size=1)  # 2 class logits per anchor (object, no object)\n",
    "        self.rpn_reg = nn.Conv2d(512, num_anchors * 4, kernel_size=1)  # 4 bbox coords per anchor\n",
    "\n",
    "        # RoI Align (assumes a fixed feature map size of 7x7 for simplicity)\n",
    "        self.roi_align = RoIAlign((7, 7), spatial_scale=1.0 / 16, sampling_ratio=2)\n",
    "\n",
    "        # Fully connected layers for classification and regression\n",
    "        self.fc1 = nn.Linear(2048 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)  # Output bbox coords (x, y, w, h)\n",
    "\n",
    "    def forward(self, x, mode='train', targets=None):\n",
    "        # Step 1: Extract features\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Step 2: Generate RPN outputs\n",
    "        rpn_feat = F.relu(self.rpn_conv(features))\n",
    "        rpn_cls_logits = self.rpn_cls(rpn_feat)\n",
    "        rpn_bbox_preds = self.rpn_reg(rpn_feat)\n",
    "\n",
    "        # Step 3: Generate anchors\n",
    "        feature_size = (features.size(2), features.size(3))  # (height, width) of feature map\n",
    "        stride = 16  # Assuming input image is downsampled by 16x in the backbone\n",
    "        anchors = generate_anchors(base_size=16, scales=[1.0, 2.0, 0.5], aspect_ratios=[0.5, 1.0, 2.0],\n",
    "                                    feature_size=feature_size, stride=stride, device=x.device)\n",
    "\n",
    "        # Step 4: Generate proposals from RPN outputs\n",
    "        proposals = generate_proposals(rpn_cls_logits, rpn_bbox_preds, anchors, x.size()[2:], device=x.device, top_n=1)\n",
    "\n",
    "        # Step 5: Perform RoI Align\n",
    "        roi_features = self.roi_align(features, proposals)\n",
    "\n",
    "        # Step 6: Predict bounding boxes\n",
    "        roi_flattened = roi_features.view(roi_features.size(0), -1)\n",
    "        fc1_out = F.relu(self.fc1(roi_flattened))\n",
    "        bbox = self.fc2(fc1_out)\n",
    "\n",
    "        if mode == 'train' and targets is not None:\n",
    "            # Return loss during training\n",
    "            loss = self.compute_loss(rpn_cls_logits, rpn_bbox_preds, proposals, targets)\n",
    "            return bbox, loss\n",
    "        else:\n",
    "            # Return predictions during evaluation\n",
    "            return bbox\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, rpn_cls_logits, rpn_bbox_preds, proposals, targets):\n",
    "        # Flatten rpn_cls_logits and gt_labels\n",
    "        num_proposals = rpn_cls_logits.size(0)\n",
    "        gt_labels = torch.cat([t['labels'] for t in targets])  # Flatten all target labels\n",
    "\n",
    "        if gt_labels.size(0) != num_proposals:\n",
    "            raise ValueError(f'Mismatch between the number of proposals ({num_proposals}) and ground truth labels ({gt_labels.size(0)})')\n",
    "\n",
    "        # Classification loss (cross entropy)\n",
    "        rpn_cls_loss = F.cross_entropy(rpn_cls_logits.view(-1, 2), gt_labels.long())  # 2 classes: object, background\n",
    "\n",
    "        # Bounding box regression loss (smooth L1 loss)\n",
    "        # Only consider proposals with positive class (object)\n",
    "        positive_indices = gt_labels > 0\n",
    "        rpn_bbox_loss = F.smooth_l1_loss(rpn_bbox_preds[positive_indices], proposals[positive_indices])\n",
    "\n",
    "        return rpn_cls_loss + rpn_bbox_loss\n",
    "\n",
    "\n",
    "\n",
    "model = SimpleRCNN()\n",
    "# Example input image tensor (batch_size=1, channels=3, height=512, width=512)\n",
    "input_tensor = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Forward pass during evaluation (no targets)\n",
    "bbox = model(input_tensor, mode='eval')\n",
    "\n",
    "print(\"BBox Refinements:\", bbox.shape)\n",
    "print(bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5de1a-f068-4549-98d2-45c89c59c203",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba7363e-11d8-4913-9a3b-17b924ee32f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBox Refinements: torch.Size([1, 4])\n",
      "tensor([[ 0.0222, -0.0452, -0.1202, -0.0582]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.ops import RoIAlign\n",
    "import torchvision.ops as ops\n",
    "\n",
    "\n",
    "def decode_boxes(deltas, anchors):\n",
    "    # Convert anchors to center format (cx, cy, w, h)\n",
    "    anchors = ops.box_convert(anchors, in_fmt=\"xyxy\", out_fmt=\"cxcywh\")\n",
    "    anchor_cx, anchor_cy, anchor_w, anchor_h = anchors.split(1, dim=1)\n",
    "\n",
    "    # Decode deltas\n",
    "    dx, dy, dw, dh = deltas.split(1, dim=1)\n",
    "    pred_cx = dx * anchor_w + anchor_cx\n",
    "    pred_cy = dy * anchor_h + anchor_cy\n",
    "    pred_w = torch.exp(dw) * anchor_w\n",
    "    pred_h = torch.exp(dh) * anchor_h\n",
    "\n",
    "    # Convert back to (x1, y1, x2, y2)\n",
    "    decoded_boxes = torch.cat([\n",
    "        pred_cx - 0.5 * pred_w,  # x1\n",
    "        pred_cy - 0.5 * pred_h,  # y1\n",
    "        pred_cx + 0.5 * pred_w,  # x2\n",
    "        pred_cy + 0.5 * pred_h   # y2\n",
    "    ], dim=1)\n",
    "\n",
    "    return decoded_boxes\n",
    "\n",
    "\n",
    "def generate_anchors(base_size, scales, aspect_ratios, feature_size, stride, device):\n",
    "    anchors = []\n",
    "    for y in range(feature_size[0]):\n",
    "        for x in range(feature_size[1]):\n",
    "            cx = x * stride + stride / 2  # Center x\n",
    "            cy = y * stride + stride / 2  # Center y\n",
    "            for scale in scales:\n",
    "                for ratio in aspect_ratios:\n",
    "                    w = base_size * scale * (ratio ** 0.5)\n",
    "                    h = base_size * scale / (ratio ** 0.5)\n",
    "                    anchors.append([cx - w / 2, cy - h / 2, cx + w / 2, cy + h / 2])\n",
    "    anchors = torch.tensor(anchors, dtype=torch.float32, device=device)\n",
    "    return anchors\n",
    "\n",
    "\n",
    "def generate_proposals(rpn_cls_logits, rpn_bbox_preds, anchors, image_size, device, top_n=1):\n",
    "    batch_size = rpn_cls_logits.size(0)\n",
    "    proposals = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        # Flatten predictions and anchors\n",
    "        cls_logits = rpn_cls_logits[b].permute(1, 2, 0).reshape(-1, 2)\n",
    "        bbox_preds = rpn_bbox_preds[b].permute(1, 2, 0).reshape(-1, 4)\n",
    "        \n",
    "        # Apply softmax to classification logits\n",
    "        scores = F.softmax(cls_logits, dim=-1)[:, 1]\n",
    "        \n",
    "        # Decode predicted deltas to proposals\n",
    "        decoded_boxes = decode_boxes(bbox_preds, anchors)\n",
    "        \n",
    "        # Clip proposals to image boundaries\n",
    "        decoded_boxes = ops.clip_boxes_to_image(decoded_boxes, image_size)\n",
    "        \n",
    "        # Filter proposals by score and apply NMS\n",
    "        keep = ops.nms(decoded_boxes, scores, iou_threshold=0.7)\n",
    "        keep = keep[:top_n]  # Keep top_n proposals (1 in this case)\n",
    "        \n",
    "        # Add batch index\n",
    "        batch_proposals = torch.cat(\n",
    "            [torch.full((len(keep), 1), b, dtype=torch.float32, device=decoded_boxes.device), \n",
    "             decoded_boxes[keep]], \n",
    "            dim=1\n",
    "        )\n",
    "        proposals.append(batch_proposals)\n",
    "\n",
    "    # Concatenate all proposals across batches\n",
    "    return torch.cat(proposals, dim=0)\n",
    "\n",
    "\n",
    "class SimpleRCNN(nn.Module):\n",
    "    def __init__(self, num_anchors=9):\n",
    "        super(SimpleRCNN, self).__init__()\n",
    "\n",
    "        # Use pretrained ResNet50 as the backbone\n",
    "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Use layers up to the last conv layer\n",
    "\n",
    "        # Freeze the initial layers to retain pretrained weights\n",
    "        for param in list(self.backbone.parameters())[:6]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Region Proposal Network (RPN)\n",
    "        self.rpn_conv = nn.Conv2d(2048, 512, kernel_size=3, padding=1)  # RPN Conv layer\n",
    "        self.rpn_cls = nn.Conv2d(512, num_anchors * 2, kernel_size=1)  # 2 class logits per anchor (object, no object)\n",
    "        self.rpn_reg = nn.Conv2d(512, num_anchors * 4, kernel_size=1)  # 4 bbox coords per anchor\n",
    "\n",
    "        # RoI Align (assumes a fixed feature map size of 7x7 for simplicity)\n",
    "        self.roi_align = RoIAlign((7, 7), spatial_scale=1.0 / 16, sampling_ratio=2)\n",
    "\n",
    "        # Fully connected layers for classification and regression\n",
    "        self.fc1 = nn.Linear(2048 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)  # Output bbox coords (x, y, w, h)\n",
    "\n",
    "    def forward(self, x, mode='train', targets=None):\n",
    "        # Step 1: Extract features\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Step 2: Generate RPN outputs\n",
    "        rpn_feat = F.relu(self.rpn_conv(features))\n",
    "        rpn_cls_logits = self.rpn_cls(rpn_feat)\n",
    "        rpn_bbox_preds = self.rpn_reg(rpn_feat)\n",
    "\n",
    "        # Step 3: Generate anchors\n",
    "        feature_size = (features.size(2), features.size(3))  # (height, width) of feature map\n",
    "        stride = 16  # Assuming input image is downsampled by 16x in the backbone\n",
    "        anchors = generate_anchors(base_size=16, scales=[1.0, 2.0, 0.5], aspect_ratios=[0.5, 1.0, 2.0],\n",
    "                                    feature_size=feature_size, stride=stride, device=x.device)\n",
    "\n",
    "        # Step 4: Generate proposals from RPN outputs\n",
    "        proposals = generate_proposals(rpn_cls_logits, rpn_bbox_preds, anchors, x.size()[2:], device=x.device, top_n=1)\n",
    "\n",
    "        # Step 5: Perform RoI Align\n",
    "        roi_features = self.roi_align(features, proposals)\n",
    "\n",
    "        # Step 6: Predict bounding boxes\n",
    "        roi_flattened = roi_features.view(roi_features.size(0), -1)\n",
    "        fc1_out = F.relu(self.fc1(roi_flattened))\n",
    "        bbox = self.fc2(fc1_out)\n",
    "\n",
    "        if mode == 'train' and targets is not None:\n",
    "            # Return loss during training\n",
    "            loss = self.compute_loss(rpn_cls_logits, rpn_bbox_preds, proposals, targets)\n",
    "            return bbox, loss\n",
    "        else:\n",
    "            # Return predictions during evaluation\n",
    "            return bbox\n",
    "\n",
    "    def compute_loss(self, rpn_cls_logits, rpn_bbox_preds, proposals, targets):\n",
    "        # Flatten rpn_cls_logits and gt_labels\n",
    "        num_proposals = rpn_cls_logits.size(0)\n",
    "        gt_labels = torch.cat([t['labels'] for t in targets])  # Flatten all target labels\n",
    "\n",
    "        if gt_labels.size(0) != num_proposals:\n",
    "            raise ValueError(f'Mismatch between the number of proposals ({num_proposals}) and ground truth labels ({gt_labels.size(0)})')\n",
    "\n",
    "        # Classification loss (cross entropy)\n",
    "        rpn_cls_loss = F.cross_entropy(rpn_cls_logits.view(-1, 2), gt_labels.long())  # 2 classes: object, background\n",
    "\n",
    "        # Bounding box regression loss (smooth L1 loss)\n",
    "        # Only consider proposals with positive class (object)\n",
    "        positive_indices = gt_labels > 0\n",
    "        rpn_bbox_loss = F.smooth_l1_loss(rpn_bbox_preds[positive_indices], proposals[positive_indices])\n",
    "\n",
    "        return rpn_cls_loss + rpn_bbox_loss\n",
    "\n",
    "\n",
    "model = SimpleRCNN()\n",
    "# Example input image tensor (batch_size=1, channels=3, height=512, width=512)\n",
    "input_tensor = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Forward pass during evaluation (no targets)\n",
    "bbox = model(input_tensor, mode='eval')\n",
    "\n",
    "print(\"BBox Refinements:\", bbox.shape)\n",
    "print(bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ab4c2-82ac-4801-8723-da8cc1c75722",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed64f28b-9516-4c28-8e56-bb49254cdfe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (36864) to match target batch_size (16).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 80\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, device, optimizer, scheduler, checkpoint_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Validate the model\u001b[39;00m\n\u001b[1;32m     83\u001b[0m val_iou \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n",
      "Cell \u001b[0;32mIn[20], line 23\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Ensure `mode='train'` is passed correctly\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m _, losses \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass mode explicitly\u001b[39;00m\n\u001b[1;32m     24\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m losses\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 133\u001b[0m, in \u001b[0;36mSimpleRCNN.forward\u001b[0;34m(self, x, mode, targets)\u001b[0m\n\u001b[1;32m    129\u001b[0m bbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(fc1_out)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Return loss during training\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrpn_cls_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpn_bbox_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bbox, loss\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Return predictions during evaluation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 148\u001b[0m, in \u001b[0;36mSimpleRCNN.compute_loss\u001b[0;34m(self, rpn_cls_logits, rpn_bbox_preds, proposals, targets)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMismatch between the number of proposals (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proposals\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and ground truth labels (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt_labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Classification loss (cross entropy)\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m rpn_cls_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrpn_cls_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 2 classes: object, background\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Bounding box regression loss (smooth L1 loss)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Only consider proposals with positive class (object)\u001b[39;00m\n\u001b[1;32m    152\u001b[0m positive_indices \u001b[38;5;241m=\u001b[39m gt_labels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (36864) to match target batch_size (16)."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "# Define IoU computation\n",
    "def calculate_iou(pred_boxes, target_boxes):\n",
    "    return ops.box_iou(pred_boxes, target_boxes)\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = torch.stack([img.to(device) for img in images])\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure `mode='train'` is passed correctly\n",
    "        _, losses = model(images, mode='train', targets=targets)  # Pass mode explicitly\n",
    "        total_loss = sum(loss for loss in losses.values())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_iou = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_data_loader:\n",
    "            # Move data to device\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = model(images, mode='eval')  # During evaluation, model outputs predictions\n",
    "            \n",
    "            # Compute IoU between predicted and target boxes\n",
    "            for output, target in zip(outputs, targets):\n",
    "                pred_boxes = output[\"boxes\"].detach()\n",
    "                target_boxes = target[\"boxes\"]\n",
    "\n",
    "                if len(pred_boxes) > 0 and len(target_boxes) > 0:\n",
    "                    iou = box_iou(pred_boxes, target_boxes).mean().item()\n",
    "                    total_iou += iou\n",
    "                else:\n",
    "                    total_iou += 0  # No predictions or no targets\n",
    "\n",
    "                num_samples += 1\n",
    "\n",
    "    # Calculate average IoU\n",
    "    avg_iou = total_iou / num_samples if num_samples > 0 else 0.0\n",
    "\n",
    "    return avg_iou\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, optimizer, scheduler, checkpoint_path):\n",
    "    best_iou = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train the model\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "\n",
    "        # Validate the model\n",
    "        val_iou = evaluate(model, val_loader, device)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_iou > best_iou:\n",
    "            best_iou = val_iou\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model saved with IoU: {best_iou:.4f}\")\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation IoU: {val_iou:.4f}\")\n",
    "\n",
    "    print(f\"Training complete. Best Validation IoU: {best_iou:.4f}\")\n",
    "\n",
    "\n",
    "# Set up optimizer, scheduler, and device\n",
    "model = SimpleRCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "checkpoint_path = \"best_model.pth\"\n",
    "num_epochs = 10\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    val_data_loader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    checkpoint_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4593a65-8fa6-4a87-8413-879cf1435c39",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832215df-637d-4f0c-b81d-2836b3f002b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "output_directory = \"2024-fall-ml-3-hw-4-wheres-waldo/outputs\"\n",
    "if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "# Function to predict bounding box on a new image\n",
    "# Function to predict bounding box on a new image\n",
    "# Function to predict bounding box on a new image\n",
    "def predict(model, image_path, transform=None):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Open image and convert to RGB\n",
    "    orig_width, orig_height = image.size  # Get original image dimensions\n",
    "    if transform:\n",
    "        image = transform(image)  # Apply transformations\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Ensure image is a Tensor and move it to the correct device\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Predict the bounding box\n",
    "    with torch.no_grad():\n",
    "        # Get the model's output (a list of dictionaries in this case)\n",
    "        outputs = model(image)\n",
    "        if isinstance(outputs, list):  # Check if the output is a list\n",
    "            outputs = outputs[0]  # Get the first dictionary in the list\n",
    "        \n",
    "        # Check if any bounding boxes were predicted\n",
    "        if len(outputs['boxes']) == 0:\n",
    "            print(f\"No objects detected in {image_path}\")\n",
    "            return None  # No detection, return None or handle as needed\n",
    "        \n",
    "        # Extract the predicted bounding box (assuming the first detected object)\n",
    "        predicted_bbox = outputs['boxes'][0].cpu().numpy()  # Assuming 'boxes' contains the bounding boxes\n",
    "\n",
    "    # Scale bbox back to original image dimensions\n",
    "    predicted_bbox[0] *= orig_width / image_sz  # x_min\n",
    "    predicted_bbox[1] *= orig_height / image_sz  # y_min\n",
    "    predicted_bbox[2] *= orig_width / image_sz  # x_max\n",
    "    predicted_bbox[3] *= orig_height / image_sz  # y_max\n",
    "\n",
    "    return predicted_bbox\n",
    "\n",
    "\n",
    "# Define image transformations (resize, normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_sz, image_sz)),  # Resize the image to 512x512\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "#For each image in test folder, predict, draw bounding box, save image, and save into csv file for submission\n",
    "test_images = [img for img in os.listdir(test_folder) if img.endswith(\".jpg\")]\n",
    "predictions = []\n",
    "\n",
    "for name in test_images:\n",
    "    image_path = os.path.join(test_folder, name)  # Replace with the path to your test image\n",
    "    predicted_bbox = predict(model, image_path, transform)\n",
    "    \n",
    "    if predicted_bbox is None:\n",
    "        continue  # Skip the image if no bounding box is detected\n",
    "\n",
    "    # Print the predicted bounding box (x_min, y_min, x_max, y_max)\n",
    "    print(\"Predicted Bounding Box:\", predicted_bbox)\n",
    "\n",
    "    # Plot the image and the predicted bounding box\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Add the bounding box only if a prediction was made\n",
    "    plt.gca().add_patch(plt.Rectangle(\n",
    "        (predicted_bbox[0], predicted_bbox[1]),  # (x_min, y_min)\n",
    "        predicted_bbox[2] - predicted_bbox[0],  # Width (x_max - x_min)\n",
    "        predicted_bbox[3] - predicted_bbox[1],  # Height (y_max - y_min)\n",
    "        linewidth=2, edgecolor='r', facecolor='none'\n",
    "    ))\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig(os.path.join(output_directory, name), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Extract bounding box coordinates\n",
    "    x_min, y_min, x_max, y_max = predicted_bbox\n",
    "    predictions.append([name, x_min, y_min, x_max, y_max])\n",
    "\n",
    "# Save predictions to CSV\n",
    "df = pd.DataFrame(predictions, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "df.to_csv(os.path.join(output_directory, 'predictions.csv'), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
