{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Wheres Waldo?\n",
    "### Name: Eileanor LaRocco\n",
    "In this assignment, you will develop an object detection algorithm to locate Waldo in a set of images. You will develop a model to detect the bounding box around Waldo. Your final task is to submit your predictions on Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process/Issues\n",
    "- Double-checked that the images we were given were correctly bounded (did this by visualizing the boxes on the images - they look good!)\n",
    "- Complication: Originally when I creating augmented images, the bounding box labels did not also augment. I also had to try out a few types of augmentation to see what made sense for waldo. The augmented images may still not be as different from one another as they could be which could allow the model to favor the training images that occur more frequently.\n",
    "- Complication: Similarly, when resizing the images, ensuring the bounding boxes not only are also adjusted if necessary, but ensuring they do not get cut off and the image is not stretched/shrunk too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/albumentations/check_version.py:51: UserWarning: Error fetching version info <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import opendatasets as od\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./2024-fall-ml-3-hw-4-wheres-waldo\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download('https://www.kaggle.com/competitions/2024-fall-ml-3-hw-4-wheres-waldo/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train\" # Original Train Images\n",
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\" # Original Test Images\n",
    "annotations_file = \"2024-fall-ml-3-hw-4-wheres-waldo/annotations.csv\" # Original Annotations File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Images (Crop/Augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Image Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 8.jpg, Width: 2800, Height: 1760\n",
      "Image: 9.jpg, Width: 1298, Height: 951\n",
      "Image: 14.jpg, Width: 1700, Height: 2340\n",
      "Image: 15.jpg, Width: 1600, Height: 1006\n",
      "Image: 17.jpg, Width: 1599, Height: 1230\n",
      "Image: 16.jpg, Width: 1525, Height: 3415\n",
      "Image: 12.jpg, Width: 1276, Height: 1754\n",
      "Image: 13.jpg, Width: 1280, Height: 864\n",
      "Image: 11.jpg, Width: 2828, Height: 1828\n",
      "Image: 10.jpg, Width: 1600, Height: 980\n",
      "Image: 21.jpg, Width: 2048, Height: 1515\n",
      "Image: 20.jpg, Width: 2953, Height: 2088\n",
      "Image: 22.jpg, Width: 500, Height: 256\n",
      "Image: 23.jpg, Width: 325, Height: 300\n",
      "Image: 27.jpg, Width: 591, Height: 629\n",
      "Image: 26.jpg, Width: 600, Height: 374\n",
      "Image: 18.jpg, Width: 1590, Height: 981\n",
      "Image: 24.jpg, Width: 456, Height: 256\n",
      "Image: 25.jpg, Width: 413, Height: 500\n",
      "Image: 19.jpg, Width: 1280, Height: 864\n",
      "Image: 4.jpg, Width: 2048, Height: 1272\n",
      "Image: 5.jpg, Width: 2100, Height: 1760\n",
      "Image: 7.jpg, Width: 1949, Height: 1419\n",
      "Image: 6.jpg, Width: 2048, Height: 1454\n",
      "Image: 2.jpg, Width: 1286, Height: 946\n",
      "Image: 3.jpg, Width: 2048, Height: 1346\n",
      "Image: 1.jpg, Width: 2048, Height: 1251\n",
      "Image: 8.jpg, Width: 1286, Height: 944\n",
      "Image: 9.jpg, Width: 900, Height: 648\n",
      "Image: 4.jpg, Width: 736, Height: 536\n",
      "Image: 5.jpg, Width: 954, Height: 702\n",
      "Image: 7.jpg, Width: 661, Height: 720\n",
      "Image: 6.jpg, Width: 700, Height: 700\n",
      "Image: 2.jpg, Width: 2560, Height: 1680\n",
      "Image: 3.jpg, Width: 1772, Height: 1112\n",
      "Image: 1.jpg, Width: 960, Height: 671\n"
     ]
    }
   ],
   "source": [
    "# Train Images\n",
    "\n",
    "# Iterate over all images in the folder\n",
    "for image_name in os.listdir(train_folder):\n",
    "    if image_name.endswith((\".jpg\")):  # Check for common image extensions\n",
    "        image_path = os.path.join(train_folder, image_name)\n",
    "        \n",
    "        # Read the image using OpenCV\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is not None:\n",
    "            height, width, channels = img.shape  # Get image size (height, width, channels)\n",
    "            print(f\"Image: {image_name}, Width: {width}, Height: {height}\")\n",
    "        else:\n",
    "            print(f\"Could not read image: {image_name}\")\n",
    "\n",
    "# Test Images\n",
    "\n",
    "# Iterate over all images in the folder\n",
    "for image_name in os.listdir(test_folder):\n",
    "    if image_name.endswith((\".jpg\")):  # Check for common image extensions\n",
    "        image_path = os.path.join(test_folder, image_name)\n",
    "        \n",
    "        # Read the image using OpenCV\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is not None:\n",
    "            height, width, channels = img.shape  # Get image size (height, width, channels)\n",
    "            print(f\"Image: {image_name}, Width: {width}, Height: {height}\")\n",
    "        else:\n",
    "            print(f\"Could not read image: {image_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images and annotations resized and saved.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "resized_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train/resized\"\n",
    "resized_annotations_file = \"2024-fall-ml-3-hw-4-wheres-waldo/resized_annotations.csv\"\n",
    "target_size = (1000, 600)  # Target size for resizing images\n",
    "\n",
    "# Read the annotations CSV file\n",
    "annotations_df = pd.read_csv(annotations_file)\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(resized_folder, exist_ok=True)\n",
    "\n",
    "# List to store updated bounding boxes\n",
    "updated_annotations = []\n",
    "\n",
    "# Iterate over all images in the annotation file\n",
    "for index, row in annotations_df.iterrows():\n",
    "    image_name = row[\"filename\"]\n",
    "    xmin, ymin, xmax, ymax = row[\"xmin\"], row[\"ymin\"], row[\"xmax\"], row[\"ymax\"]\n",
    "    \n",
    "    # Load the image\n",
    "    image_path = os.path.join(train_folder, image_name)\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    if img is not None:\n",
    "        original_height, original_width = img.shape[:2]\n",
    "        \n",
    "        # Calculate the resizing scale factors\n",
    "        scale_x = target_size[0] / original_width\n",
    "        scale_y = target_size[1] / original_height\n",
    "        \n",
    "        # Resize the image\n",
    "        resized_img = cv2.resize(img, target_size)\n",
    "        \n",
    "        # Adjust bounding boxes based on the scaling factors\n",
    "        xmin_new = int(xmin * scale_x)\n",
    "        ymin_new = int(ymin * scale_y)\n",
    "        xmax_new = int(xmax * scale_x)\n",
    "        ymax_new = int(ymax * scale_y)\n",
    "        \n",
    "        # Save the resized image\n",
    "        resized_image_path = os.path.join(resized_folder, image_name)\n",
    "        cv2.imwrite(resized_image_path, resized_img)\n",
    "        \n",
    "        # Add the updated annotation to the list\n",
    "        updated_annotations.append([image_name, xmin_new, ymin_new, xmax_new, ymax_new])\n",
    "\n",
    "# Save the updated annotations to a new CSV file\n",
    "updated_annotations_df = pd.DataFrame(updated_annotations, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "updated_annotations_df.to_csv(resized_annotations_file, index=False)\n",
    "\n",
    "print(\"Images and annotations resized and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (for model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "yolo_train_dir = \"datasets/yolo_dataset/train\"\n",
    "yolo_val_dir = \"datasets/yolo_dataset/val\"\n",
    "\n",
    "#Saved Predictions\n",
    "yolo_test_dir = \"yolo_test_predictions\"\n",
    "\n",
    "# Create necessary folders\n",
    "os.makedirs(yolo_train_dir, exist_ok=True)\n",
    "os.makedirs(yolo_val_dir, exist_ok=True)\n",
    "os.makedirs(yolo_test_dir, exist_ok=True)\n",
    "\n",
    "# Load annotations\n",
    "annotations = pd.read_csv(resized_annotations_file)\n",
    "\n",
    "# Function to convert annotations to YOLO format\n",
    "def convert_to_yolo_format(row, img_width, img_height):\n",
    "    x_center = (row[\"xmin\"] + row[\"xmax\"]) / 2 / img_width\n",
    "    y_center = (row[\"ymin\"] + row[\"ymax\"]) / 2 / img_height\n",
    "    width = (row[\"xmax\"] - row[\"xmin\"]) / img_width\n",
    "    height = (row[\"ymax\"] - row[\"ymin\"]) / img_height\n",
    "    return f\"0 {x_center} {y_center} {width} {height}\"\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "image_files = annotations[\"filename\"].unique()\n",
    "train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to prepare YOLO format data\n",
    "def prepare_yolo_data(image_list, output_dir):\n",
    "    for img_name in image_list:\n",
    "        img_path = os.path.join(resized_folder, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img_height, img_width, _ = img.shape\n",
    "\n",
    "        # Filter annotations for this image\n",
    "        image_annotations = annotations[annotations[\"filename\"] == img_name]\n",
    "\n",
    "        # YOLO annotations file\n",
    "        yolo_annotations = []\n",
    "        for _, row in image_annotations.iterrows():\n",
    "            yolo_line = convert_to_yolo_format(row, img_width, img_height)\n",
    "            yolo_annotations.append(yolo_line)\n",
    "\n",
    "        # Save image and annotation\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        shutil.copy(img_path, os.path.join(output_dir, f\"{base_name}.jpg\"))\n",
    "        with open(os.path.join(output_dir, f\"{base_name}.txt\"), \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_annotations))\n",
    "\n",
    "# Prepare training and validation data\n",
    "prepare_yolo_data(train_images, yolo_train_dir)\n",
    "prepare_yolo_data(val_images, yolo_val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv_by_column(input_csv, output_csv, column_name, values_list):\n",
    "    \"\"\"\n",
    "    Filters rows in a CSV file and keeps only those where the specified column's value is in a given list.\n",
    "\n",
    "    Parameters:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to save the filtered CSV file.\n",
    "        column_name (str): Column to filter on.\n",
    "        values_list (list): List of values to keep.\n",
    "    \"\"\"\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df[column_name].isin(values_list)]\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    filtered_df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split annotations into train and val\n",
    "values_list = []\n",
    "directory = \"datasets/yolo_dataset/train\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.jpg'):\n",
    "        values_list.append(filename)\n",
    "\n",
    "# Example usage\n",
    "input_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/resized_annotations.csv\"\n",
    "output_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\"\n",
    "column_name = \"filename\"\n",
    "\n",
    "filter_csv_by_column(input_csv, output_csv, column_name, values_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "values_list = []\n",
    "directory = \"datasets/yolo_dataset/val\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.jpg'):\n",
    "        values_list.append(filename)\n",
    "\n",
    "# Example usage\n",
    "input_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/resized_annotations.csv\"\n",
    "output_csv = \"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\" \n",
    "column_name = \"filename\" \n",
    "\n",
    "filter_csv_by_column(input_csv, output_csv, column_name, values_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "class WaldoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = F.to_tensor(image)\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Read bounding box data, ensuring all are converted to float\n",
    "        box_data = self.img_labels.iloc[idx, 1:5].values\n",
    "        boxes = []\n",
    "        for item in box_data:\n",
    "            try:\n",
    "                boxes.append(float(item))\n",
    "            except ValueError as e:\n",
    "                raise ValueError(f\"Error converting bounding box data to float: {e}\")\n",
    "\n",
    "        # Create tensors\n",
    "        boxes = torch.as_tensor([boxes], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        image, target = self.transform(image, target)\n",
    "\n",
    "        target = F.to_tensor(target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Create the dataset\n",
    "train_dataset = WaldoDataset(annotations_file= \"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\", img_dir=\"datasets/yolo_dataset/train\")\n",
    "test_dataset = WaldoDataset(annotations_file= \"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\", img_dir=\"datasets/yolo_dataset/val\")\n",
    "\n",
    "# Now, you can use this dataset with a DataLoader to train your model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    #collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    #collate_fn=lambda x: tuple(zip(*x))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box predictions:  tensor([[-0.0638,  0.0943, -0.0380,  0.0581],\n",
      "        [-0.0603,  0.1020, -0.0275,  0.0441]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleBBoxModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleBBoxModel, self).__init__()\n",
    "        \n",
    "        # Define a simple CNN architecture\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Define a fully connected layer to output 4 values for the bounding box\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)  # Flattening 64 channels of 16x16 feature maps\n",
    "        self.fc2 = nn.Linear(128, 4)  # Outputting the 4 bounding box coordinates\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Pooling to reduce the spatial dimensions\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)  # Pooling again\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)  # Final pooling\n",
    "        \n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        \n",
    "        # Pass through the fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        bbox = self.fc2(x)  # Output 4 values for the bounding box\n",
    "        \n",
    "        return bbox\n",
    "\n",
    "# Example usage:\n",
    "model = SimpleBBoxModel()\n",
    "\n",
    "# Input a batch of 128x128 RGB images (batch_size=2)\n",
    "images = torch.randn(2, 3, 128, 128)  # Random images for demonstration\n",
    "\n",
    "# Forward pass\n",
    "output = model(images)\n",
    "print(\"Bounding box predictions: \", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 9, 128, 128, 4]' is invalid for input of size 1152",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 195\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Run the model on the image\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 195\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# If no targets are passed, it will return the predictions\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[85], line 84\u001b[0m, in \u001b[0;36mFasterRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     81\u001b[0m rpn_logits, rpn_bbox_deltas, anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn(features, (height, width))\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Decode Proposals (Using anchors and bbox deltas)\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_proposals\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpn_bbox_deltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Apply NMS on Proposals\u001b[39;00m\n\u001b[1;32m     87\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_nms(proposals, rpn_logits, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "Cell \u001b[0;32mIn[85], line 112\u001b[0m, in \u001b[0;36mFasterRCNN.decode_proposals\u001b[0;34m(self, anchors, bbox_deltas, feature_map_shape)\u001b[0m\n\u001b[1;32m    109\u001b[0m num_anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m  \u001b[38;5;66;03m# Number of anchors per location (e.g., 9)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Reshape bbox_deltas to match the number of anchors (batch_size, num_anchors * height * width, 4)\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m bbox_deltas \u001b[38;5;241m=\u001b[39m \u001b[43mbbox_deltas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_anchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m bbox_deltas \u001b[38;5;241m=\u001b[39m bbox_deltas\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# [batch_size, num_anchors * height * width, 4]\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Expand anchors to match the number of proposals (batch_size, num_anchors * height * width, 4)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 9, 128, 128, 4]' is invalid for input of size 1152"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.ops import roi_align, nms\n",
    "\n",
    "# Define the Backbone\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet50 = torchvision.models.resnet50(pretrained=False)\n",
    "        self.backbone = nn.Sequential(*list(resnet50.children())[:-2])  # Remove classification head\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Define the RPN (Region Proposal Network) with custom anchor generation\n",
    "class RPN(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.cls_logits = nn.Conv2d(512, 9 * 2, kernel_size=1)  # 9 anchors per location\n",
    "        self.bbox_pred = nn.Conv2d(512, 9 * 4, kernel_size=1)   # 9 anchors per location\n",
    "\n",
    "    def generate_anchors(self, feature_map_size, aspect_ratios, sizes):\n",
    "        # Generate anchors in (xmin, ymin, xmax, ymax) format\n",
    "        anchors = []\n",
    "        for size in sizes:\n",
    "            for aspect_ratio in aspect_ratios:\n",
    "                w = size * aspect_ratio**0.5\n",
    "                h = size / aspect_ratio**0.5\n",
    "                # Generate anchors with (xmin, ymin, xmax, ymax)\n",
    "                anchors.append([-w/2, -h/2, w/2, h/2])  # Anchors centered at (0, 0)\n",
    "        return torch.tensor(anchors).float()\n",
    "\n",
    "    def forward(self, feature_map, image_size):\n",
    "        x = self.conv(feature_map)  # Convolution layer output\n",
    "        logits = self.cls_logits(x)  # Classification logits\n",
    "        bbox_deltas = self.bbox_pred(x)  # Bounding box deltas\n",
    "\n",
    "        # Generate custom anchors (9 anchors for each location)\n",
    "        anchors = self.generate_anchors(feature_map.shape[-2:], aspect_ratios=[0.5, 1.0, 2.0], sizes=[8, 16, 32])\n",
    "\n",
    "        # Reshape logits to [batch_size, num_anchors, height, width]\n",
    "        logits = logits.permute(0, 2, 3, 1).contiguous().view(logits.shape[0], -1, 2)\n",
    "        bbox_deltas = bbox_deltas.permute(0, 2, 3, 1).contiguous().view(bbox_deltas.shape[0], -1, 4)\n",
    "\n",
    "        return logits, bbox_deltas, anchors\n",
    "\n",
    "# Define the Detection Head\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.cls_score = nn.Linear(1024, num_classes)  # Classification scores\n",
    "        self.bbox_pred = nn.Linear(1024, num_classes * 4)  # Bounding box regression\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        cls_scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "        return cls_scores, bbox_deltas\n",
    "\n",
    "# Define Faster R-CNN with Custom RPN\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.rpn = RPN(2048)  # 2048 channels from ResNet50 backbone\n",
    "        self.roi_align = roi_align\n",
    "        self.head = DetectionHead(2048, num_classes)\n",
    "    \n",
    "    def forward(self, images, targets=None):\n",
    "        # Backbone Forward Pass\n",
    "        features = self.backbone(images)\n",
    "        batch_size, _, height, width = images.shape\n",
    "\n",
    "        # RPN Forward Pass\n",
    "        rpn_logits, rpn_bbox_deltas, anchors = self.rpn(features, (height, width))\n",
    "\n",
    "        # Decode Proposals (Using anchors and bbox deltas)\n",
    "        proposals = self.decode_proposals(anchors, rpn_bbox_deltas, (height, width))\n",
    "\n",
    "        # Apply NMS on Proposals\n",
    "        proposals = self.apply_nms(proposals, rpn_logits, threshold=0.7)\n",
    "\n",
    "        # RoI Align\n",
    "        pooled_features = self.roi_align(features, proposals, output_size=(4, 4), spatial_scale=1/16)\n",
    "\n",
    "        # Detection Head Forward Pass\n",
    "        cls_scores, bbox_deltas = self.head(pooled_features)\n",
    "\n",
    "        # Loss Computation\n",
    "        if targets is not None:\n",
    "            losses = self.compute_losses(rpn_logits, rpn_bbox_deltas, cls_scores, bbox_deltas, anchors, proposals, targets)\n",
    "            return losses\n",
    "        else:\n",
    "            return self.post_process(cls_scores, bbox_deltas, proposals)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def decode_proposals(self, anchors, bbox_deltas, feature_map_shape):\n",
    "        # Ensure bbox_deltas has the expected shape [batch_size, num_anchors * height * width, 4]\n",
    "        batch_size = bbox_deltas.shape[0]\n",
    "        height, width = feature_map_shape\n",
    "        num_anchors = 9  # Number of anchors per location (e.g., 9)\n",
    "\n",
    "        # Reshape bbox_deltas to match the number of anchors (batch_size, num_anchors * height * width, 4)\n",
    "        bbox_deltas = bbox_deltas.view(batch_size, num_anchors, height, width, 4)\n",
    "        bbox_deltas = bbox_deltas.permute(0, 3, 1, 2).contiguous().view(batch_size, -1, 4)  # [batch_size, num_anchors * height * width, 4]\n",
    "\n",
    "        # Expand anchors to match the number of proposals (batch_size, num_anchors * height * width, 4)\n",
    "        anchors = anchors.view(1, -1, 4).expand(batch_size, -1, 4)\n",
    "\n",
    "        # Apply deltas to anchors (decode proposals)\n",
    "        proposals = self.apply_bbox_deltas(anchors, bbox_deltas)\n",
    "        \n",
    "        return proposals\n",
    "\n",
    "    \n",
    "    def apply_bbox_deltas(self, anchors, bbox_deltas):\n",
    "        \"\"\"\n",
    "        Apply bounding box deltas to anchors to get proposals.\n",
    "        \n",
    "        Arguments:\n",
    "        - anchors: Tensor of shape [batch_size, num_anchors, 4] representing the base boxes.\n",
    "        - bbox_deltas: Tensor of shape [batch_size, num_anchors, 4] representing the bounding box deltas (4 per anchor).\n",
    "        \n",
    "        Returns:\n",
    "        - proposals: Tensor of shape [batch_size, num_anchors, 4] representing the decoded bounding box proposals.\n",
    "        \"\"\"\n",
    "        # Apply deltas to the anchors\n",
    "        anchors = anchors.float()  # Make sure anchors are float32 for numerical stability\n",
    "\n",
    "        # Get the current box coordinates\n",
    "        anchor_x, anchor_y, anchor_w, anchor_h = anchors.split(1, dim=-1)\n",
    "\n",
    "        # Split the bbox_deltas into 4 parts (dx, dy, dw, dh)\n",
    "        dx, dy, dw, dh = bbox_deltas.split(1, dim=-1)\n",
    "\n",
    "        # Apply the deltas\n",
    "        pred_ctr_x = dx * anchor_w + anchor_x\n",
    "        pred_ctr_y = dy * anchor_h + anchor_y\n",
    "        pred_w = torch.exp(dw) * anchor_w\n",
    "        pred_h = torch.exp(dh) * anchor_h\n",
    "\n",
    "        # Convert the predictions back to x1, y1, x2, y2 format (bounding box corners)\n",
    "        proposals = torch.cat([\n",
    "            pred_ctr_x - 0.5 * pred_w,  # x1\n",
    "            pred_ctr_y - 0.5 * pred_h,  # y1\n",
    "            pred_ctr_x + 0.5 * pred_w,  # x2\n",
    "            pred_ctr_y + 0.5 * pred_h   # y2\n",
    "        ], dim=-1)\n",
    "\n",
    "        return proposals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def apply_nms(self, proposals, logits, threshold=0.7):\n",
    "        # Convert logits to scores (foreground probability)\n",
    "        scores = torch.sigmoid(logits[:, :, 1])  # Use the foreground class score\n",
    "\n",
    "        # Apply NMS: proposals are anchors, scores are foreground probabilities\n",
    "        keep = nms(proposals, scores, threshold)\n",
    "        return proposals[keep]\n",
    "\n",
    "    def compute_losses(self, rpn_logits, rpn_bbox_deltas, cls_scores, bbox_deltas, anchors, proposals, targets):\n",
    "        # Compute RPN and detection head losses (simplified)\n",
    "        return {\n",
    "            \"rpn_cls_loss\": torch.tensor(0.0),\n",
    "            \"rpn_bbox_loss\": torch.tensor(0.0),\n",
    "            \"head_cls_loss\": torch.tensor(0.0),\n",
    "            \"head_bbox_loss\": torch.tensor(0.0),\n",
    "        }\n",
    "\n",
    "    def post_process(self, cls_scores, bbox_deltas, proposals):\n",
    "        # Simplified post-processing\n",
    "        return cls_scores, bbox_deltas, proposals\n",
    "\n",
    "# Instantiate the Model\n",
    "model = FasterRCNN(num_classes=2)  # Example: 1 class + background\n",
    "\n",
    "# Example image: A random 128x128 image with 3 color channels (simulating an RGB image)\n",
    "image = torch.randn(2, 3, 128, 128)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run the model on the image\n",
    "with torch.no_grad():\n",
    "    output = model(image)  # If no targets are passed, it will return the predictions\n",
    "    print(\"Predictions:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone features shape: torch.Size([2, 2048, 4, 4])\n",
      "Feature map type: <class 'torch.Tensor'>\n",
      "Feature map shape: torch.Size([2, 2048, 4, 4])\n",
      "image size: (128, 128)\n",
      "RPN feature map shape: torch.Size([2, 512, 4, 4])\n",
      "Image size passed to AnchorGenerator: (128, 128)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ImageList' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 166\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Run the model on the image and target\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Forward pass: predictions and losses if targets are provided\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m#losses = model(image, target)\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m#print(\"Losses:\", losses)\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Inference mode: Get predictions if no target is provided\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# If no targets are passed, it will return the predictions\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[73], line 92\u001b[0m, in \u001b[0;36mFasterRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     89\u001b[0m batch_size, _, height, width \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# RPN Forward Pass\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m rpn_logits, rpn_bbox_deltas, anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Decode Proposals\u001b[39;00m\n\u001b[1;32m     95\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_proposals(anchors, rpn_bbox_deltas)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[73], line 48\u001b[0m, in \u001b[0;36mRPN.forward\u001b[0;34m(self, feature_map, image_size)\u001b[0m\n\u001b[1;32m     45\u001b[0m image_list \u001b[38;5;241m=\u001b[39m ImageList(image_tensor, image_size)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Now, you can pass the ImageList object to the AnchorGenerator\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnchors type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(anchors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnchors shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manchors\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(anchors,\u001b[38;5;250m \u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/detection/anchor_utils.py:116\u001b[0m, in \u001b[0;36mAnchorGenerator.forward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_list: ImageList, feature_maps: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tensor]:\n\u001b[0;32m--> 116\u001b[0m     grid_sizes \u001b[38;5;241m=\u001b[39m [\u001b[43mfeature_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m feature_map \u001b[38;5;129;01min\u001b[39;00m feature_maps]\n\u001b[1;32m    117\u001b[0m     image_size \u001b[38;5;241m=\u001b[39m image_list\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    118\u001b[0m     dtype, device \u001b[38;5;241m=\u001b[39m feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageList' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import roi_align, nms\n",
    "\n",
    "# Define the Backbone\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet50 = torchvision.models.resnet50(pretrained=False)\n",
    "        self.backbone = nn.Sequential(*list(resnet50.children())[:-2])  # Remove classification head\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Define the RPN (Region Proposal Network)\n",
    "class RPN(nn.Module):\n",
    "    def __init__(self, in_channels, anchor_generator):\n",
    "        super().__init__()\n",
    "        self.anchor_generator = anchor_generator\n",
    "        self.conv = nn.Conv2d(in_channels, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.cls_logits = nn.Conv2d(512, anchor_generator.num_anchors_per_location()[0] * 2, kernel_size=1)\n",
    "        self.bbox_pred = nn.Conv2d(512, anchor_generator.num_anchors_per_location()[0] * 4, kernel_size=1)\n",
    "    \n",
    "    def forward(self, feature_map, image_size):\n",
    "        print(f\"Feature map type: {type(feature_map)}\")\n",
    "        print(f\"Feature map shape: {getattr(feature_map, 'shape', 'No shape attribute')}\")\n",
    "        print(\"image size:\", image_size)\n",
    "        x = self.conv(feature_map)  # Convolution layer output\n",
    "        logits = self.cls_logits(x)  # Classification logits\n",
    "        bbox_deltas = self.bbox_pred(x)  # Bounding box deltas\n",
    "\n",
    "        # Debugging shapes\n",
    "        print(\"RPN feature map shape:\", x.shape)\n",
    "        print(\"Image size passed to AnchorGenerator:\", image_size)\n",
    "\n",
    "        from torchvision.models.detection.image_list import ImageList\n",
    "\n",
    "        # Example image tensor (e.g., a batch of images)\n",
    "        image_tensor = torch.randn(1, 3, 128, 128)  # Example tensor of shape [batch_size, channels, height, width]\n",
    "        image_size = (128, 128)  # The corresponding image size (height, width)\n",
    "\n",
    "        # Create an ImageList object\n",
    "        image_list = ImageList(image_tensor, image_size)\n",
    "\n",
    "        # Now, you can pass the ImageList object to the AnchorGenerator\n",
    "        anchors = self.anchor_generator([x], [image_list])[0]\n",
    "\n",
    "        print(f\"Anchors type: {type(anchors)}\")\n",
    "        print(f\"Anchors shape: {anchors.shape if isinstance(anchors, torch.Tensor) else 'Unknown'}\")\n",
    "\n",
    "        return logits, bbox_deltas, anchors\n",
    "\n",
    "# Define the Detection Head\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.cls_score = nn.Linear(1024, num_classes)  # Classification scores\n",
    "        self.bbox_pred = nn.Linear(1024, num_classes * 4)  # Bounding box regression\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        cls_scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "        return cls_scores, bbox_deltas\n",
    "\n",
    "# Define Faster R-CNN\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.anchor_generator = AnchorGenerator(\n",
    "            sizes=((8, 16, 32, 64, 128),),  # 128 since 128x128 image\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),)  # Aspect ratios remain the same\n",
    "        )\n",
    "        self.rpn = RPN(2048, self.anchor_generator)\n",
    "        self.roi_align = roi_align\n",
    "        self.head = DetectionHead(2048, num_classes)\n",
    "    \n",
    "    def forward(self, images, targets=None):\n",
    "        # Backbone Forward Pass\n",
    "        features = self.backbone(images)\n",
    "        print(\"Backbone features shape:\", features.shape)\n",
    "        batch_size, _, height, width = images.shape\n",
    "\n",
    "        # RPN Forward Pass\n",
    "        rpn_logits, rpn_bbox_deltas, anchors = self.rpn(features, (height, width))\n",
    "\n",
    "        # Decode Proposals\n",
    "        proposals = self.decode_proposals(anchors, rpn_bbox_deltas)\n",
    "\n",
    "        # Apply NMS on Proposals\n",
    "        proposals = self.apply_nms(proposals, rpn_logits, threshold=0.7)\n",
    "\n",
    "        # RoI Align\n",
    "        pooled_features = self.roi_align(features, proposals, output_size=(4, 4), spatial_scale=1/16) #match output from resnet ((2048, 4, 4))\n",
    "\n",
    "\n",
    "        # Detection Head Forward Pass\n",
    "        cls_scores, bbox_deltas = self.head(pooled_features)\n",
    "\n",
    "        # Loss Computation\n",
    "        if targets is not None:\n",
    "            losses = self.compute_losses(rpn_logits, rpn_bbox_deltas, cls_scores, bbox_deltas, anchors, proposals, targets)\n",
    "            return losses\n",
    "        else:\n",
    "            return self.post_process(cls_scores, bbox_deltas, proposals)\n",
    "\n",
    "    def decode_proposals(self, anchors, bbox_deltas):\n",
    "        # Apply deltas to anchors to get proposals (not shown here for brevity)\n",
    "        return proposals\n",
    "\n",
    "    def apply_nms(self, proposals, logits, threshold=0.7):\n",
    "        scores = torch.sigmoid(logits.squeeze(1))\n",
    "        keep = nms(proposals, scores, threshold)\n",
    "        return proposals[keep]\n",
    "\n",
    "    def compute_losses(self, rpn_logits, rpn_bbox_deltas, cls_scores, bbox_deltas, anchors, proposals, targets):\n",
    "        # Compute RPN and detection head losses\n",
    "        # (e.g., Binary Cross-Entropy, Smooth L1 Loss)\n",
    "        return {\n",
    "            \"rpn_cls_loss\": rpn_cls_loss,\n",
    "            \"rpn_bbox_loss\": rpn_bbox_loss,\n",
    "            \"head_cls_loss\": head_cls_loss,\n",
    "            \"head_bbox_loss\": head_bbox_loss,\n",
    "        }\n",
    "\n",
    "    def post_process(self, cls_scores, bbox_deltas, proposals):\n",
    "        # Decode final predictions and apply NMS (not shown here for brevity)\n",
    "        return final_detections\n",
    "\n",
    "# Instantiate the Model\n",
    "model = FasterRCNN(num_classes=2)  # Example: 1 class + background\n",
    "\n",
    "# Example image: A random 128x128 image with 3 color channels (simulating an RGB image)\n",
    "image = torch.randn(2, 3, 128, 128)\n",
    "\n",
    "# Example target: A dictionary with ground truth data\n",
    "# The target should include the bounding boxes and labels for the object(s) in the image\n",
    "target = [{\n",
    "    'boxes': torch.tensor([[30.0, 30.0, 100.0, 100.0],[30.0, 30.0, 100.0, 100.0]]),  # Example bounding box [xmin, ymin, xmax, ymax]\n",
    "    'labels': torch.tensor([1, 1]),  # Example class label (1 for the object class, 0 is for background)\n",
    "    'image_id': torch.tensor([0, 1]),  # Image id (if using a dataset with multiple images)\n",
    "    'area': torch.tensor([100, 100]),  # Area of the bounding box (width * height)\n",
    "    'iscrowd': torch.tensor([0, 0])  # Indicates whether the object is a crowd (used in COCO dataset)\n",
    "}]\n",
    "\n",
    "# Instantiate the model (use the code from previous steps)\n",
    "model = FasterRCNN(num_classes=2)  # 1 class + background\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run the model on the image and target\n",
    "with torch.no_grad():\n",
    "    # Forward pass: predictions and losses if targets are provided\n",
    "    #losses = model(image, target)\n",
    "    #print(\"Losses:\", losses)\n",
    "\n",
    "    # Inference mode: Get predictions if no target is provided\n",
    "    output = model(image)  # If no targets are passed, it will return the predictions\n",
    "    print(\"Predictions:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone features shape: torch.Size([1, 2048, 4, 4])\n",
      "torch.Size([1, 2048, 4, 4])\n",
      "(128, 128)\n",
      "RPN feature map shape: torch.Size([1, 512, 4, 4])\n",
      "Image size passed to AnchorGenerator: (128, 128)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 173\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Run the model on the image and target\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Forward pass: predictions and losses if targets are provided\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m#losses = model(image, target)\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m#print(\"Losses:\", losses)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# Inference mode: Get predictions if no target is provided\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# If no targets are passed, it will return the predictions\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 81\u001b[0m, in \u001b[0;36mFasterRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     78\u001b[0m batch_size, _, height, width \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# RPN Forward Pass\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m rpn_logits, rpn_bbox_deltas, anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Decode Proposals\u001b[39;00m\n\u001b[1;32m     84\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_proposals(anchors, rpn_bbox_deltas)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 38\u001b[0m, in \u001b[0;36mRPN.forward\u001b[0;34m(self, feature_map, image_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size passed to AnchorGenerator:\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_size)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Ensure feature map is passed as a list\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated anchors type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(anchors))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated anchors shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, anchors\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(anchors, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m anchors)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/detection/anchor_utils.py:116\u001b[0m, in \u001b[0;36mAnchorGenerator.forward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_list: ImageList, feature_maps: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tensor]:\n\u001b[0;32m--> 116\u001b[0m     grid_sizes \u001b[38;5;241m=\u001b[39m [\u001b[43mfeature_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m feature_map \u001b[38;5;129;01min\u001b[39;00m feature_maps]\n\u001b[1;32m    117\u001b[0m     image_size \u001b[38;5;241m=\u001b[39m image_list\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    118\u001b[0m     dtype, device \u001b[38;5;241m=\u001b[39m feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import roi_align, nms\n",
    "\n",
    "# Define the Backbone\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet50 = torchvision.models.resnet50(pretrained=False)\n",
    "        self.backbone = nn.Sequential(*list(resnet50.children())[:-2])  # Remove classification head\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Define the RPN (Region Proposal Network)\n",
    "class RPN(nn.Module):\n",
    "    def __init__(self, in_channels, anchor_generator):\n",
    "        super().__init__()\n",
    "        self.anchor_generator = anchor_generator\n",
    "        self.conv = nn.Conv2d(in_channels, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.cls_logits = nn.Conv2d(512, anchor_generator.num_anchors_per_location()[0] * 2, kernel_size=1)\n",
    "        self.bbox_pred = nn.Conv2d(512, anchor_generator.num_anchors_per_location()[0] * 4, kernel_size=1)\n",
    "    \n",
    "    def forward(self, feature_map, image_size):\n",
    "        print(feature_map.shape)\n",
    "        print(image_size)\n",
    "        x = self.conv(feature_map)  # Convolution layer output\n",
    "        logits = self.cls_logits(x)  # Classification logits\n",
    "        bbox_deltas = self.bbox_pred(x)  # Bounding box deltas\n",
    "\n",
    "        # Debugging shapes\n",
    "        print(\"RPN feature map shape:\", x.shape)\n",
    "        print(\"Image size passed to AnchorGenerator:\", image_size)\n",
    "\n",
    "        # Ensure feature map is passed as a list\n",
    "        anchors = self.anchor_generator([x], [(image_size[1], image_size[0])])[0]\n",
    "        print(\"Generated anchors type:\", type(anchors))\n",
    "        print(\"Generated anchors shape:\", anchors.shape if isinstance(anchors, torch.Tensor) else anchors)\n",
    "\n",
    "        return logits, bbox_deltas, anchors\n",
    "\n",
    "# Define the Detection Head\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels * 4 * 4, 1024)  # Adjusting for smaller feature map size\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.cls_score = nn.Linear(1024, num_classes)  # Classification scores\n",
    "        self.bbox_pred = nn.Linear(1024, num_classes * 4)  # Bounding box regression\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        cls_scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "        return cls_scores, bbox_deltas\n",
    "\n",
    "# Define Faster R-CNN\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.anchor_generator = AnchorGenerator(\n",
    "            sizes=((8, 16, 32, 64, 128),),  # Adjust anchor sizes for small image size\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),)  # Aspect ratios remain the same\n",
    "        )\n",
    "        self.rpn = RPN(2048, self.anchor_generator)\n",
    "        self.roi_align = roi_align\n",
    "        self.head = DetectionHead(2048, num_classes)\n",
    "    \n",
    "    def forward(self, images, targets=None):\n",
    "        # Backbone Forward Pass\n",
    "        features = self.backbone(images)\n",
    "        print(\"Backbone features shape:\", features.shape)\n",
    "        batch_size, _, height, width = images.shape\n",
    "\n",
    "        # RPN Forward Pass\n",
    "        rpn_logits, rpn_bbox_deltas, anchors = self.rpn(features, (height, width))\n",
    "\n",
    "        # Decode Proposals\n",
    "        proposals = self.decode_proposals(anchors, rpn_bbox_deltas)\n",
    "\n",
    "        # Apply NMS on Proposals\n",
    "        proposals = self.apply_nms(proposals, rpn_logits, threshold=0.7)\n",
    "\n",
    "        # RoI Align\n",
    "        pooled_features = self.roi_align(features, proposals, output_size=(4, 4), spatial_scale=1/16)  # Adjust for smaller image size\n",
    "\n",
    "        # Detection Head Forward Pass\n",
    "        cls_scores, bbox_deltas = self.head(pooled_features)\n",
    "\n",
    "        # Loss Computation\n",
    "        if targets is not None:\n",
    "            losses = self.compute_losses(rpn_logits, rpn_bbox_deltas, cls_scores, bbox_deltas, anchors, proposals, targets)\n",
    "            return losses\n",
    "        else:\n",
    "            return self.post_process(cls_scores, bbox_deltas, proposals)\n",
    "\n",
    "    def decode_proposals(self, anchors, bbox_deltas):\n",
    "        # Apply deltas to anchors to get proposals (not shown here for brevity)\n",
    "        return proposals\n",
    "\n",
    "    def apply_nms(self, proposals, logits, threshold=0.7):\n",
    "        scores = torch.sigmoid(logits.squeeze(1))\n",
    "        keep = nms(proposals, scores, threshold)\n",
    "        return proposals[keep]\n",
    "\n",
    "    def compute_losses(self, rpn_logits, rpn_bbox_deltas, cls_scores, bbox_deltas, anchors, proposals, targets):\n",
    "        # Compute RPN and detection head losses\n",
    "        # (e.g., Binary Cross-Entropy, Smooth L1 Loss)\n",
    "        return {\n",
    "            \"rpn_cls_loss\": rpn_cls_loss,\n",
    "            \"rpn_bbox_loss\": rpn_bbox_loss,\n",
    "            \"head_cls_loss\": head_cls_loss,\n",
    "            \"head_bbox_loss\": head_bbox_loss,\n",
    "        }\n",
    "\n",
    "    def post_process(self, cls_scores, bbox_deltas, proposals):\n",
    "        # Decode final predictions and apply NMS (not shown here for brevity)\n",
    "        return final_detections\n",
    "\n",
    "# Instantiate the Model\n",
    "model = FasterRCNN(num_classes=2)  # Example: 1 class + background\n",
    "\n",
    "\"\"\"\n",
    "# Example Inputs\n",
    "images = torch.randn(1, 3, 128, 128)  # Batch of one image\n",
    "targets = [\n",
    "    {\n",
    "        \"boxes\": torch.tensor([[50, 50, 200, 200]], dtype=torch.float32),\n",
    "        \"labels\": torch.tensor([1], dtype=torch.int64)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Forward Pass (Training)\n",
    "losses = model(images, targets)\n",
    "print(\"Losses:\", losses)\n",
    "\n",
    "# Forward Pass (Inference)\n",
    "detections = model(images)\n",
    "print(\"Detections:\", detections)\n",
    "\"\"\"\n",
    "\n",
    "# Example image: A random 128x128 image with 3 color channels (simulating an RGB image)\n",
    "image = torch.randn(1, 3, 128, 128)\n",
    "\n",
    "# Example target: A dictionary with ground truth data\n",
    "# The target should include the bounding boxes and labels for the object(s) in the image\n",
    "target = [{\n",
    "    'boxes': torch.tensor([[30.0, 30.0, 100.0, 100.0]]),  # Example bounding box [xmin, ymin, xmax, ymax]\n",
    "    'labels': torch.tensor([1]),  # Example class label (1 for the object class, 0 is for background)\n",
    "    'image_id': torch.tensor([0]),  # Image id (if using a dataset with multiple images)\n",
    "    'area': torch.tensor([4900.0]),  # Area of the bounding box (width * height)\n",
    "    'iscrowd': torch.tensor([0])  # Indicates whether the object is a crowd (used in COCO dataset)\n",
    "}]\n",
    "\n",
    "# Instantiate the model (use the code from previous steps)\n",
    "model = FasterRCNN(num_classes=2)  # 1 class + background\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run the model on the image and target\n",
    "with torch.no_grad():\n",
    "    # Forward pass: predictions and losses if targets are provided\n",
    "    #losses = model(image, target)\n",
    "    #print(\"Losses:\", losses)\n",
    "\n",
    "    # Inference mode: Get predictions if no target is provided\n",
    "    output = model(image)  # If no targets are passed, it will return the predictions\n",
    "    print(\"Predictions:\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Compose.__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     evaluate(model, test_loader, device)\n",
      "Cell \u001b[0;32mIn[78], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     34\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape())\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[77], line 46\u001b[0m, in \u001b[0;36mWaldoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m area\n\u001b[1;32m     44\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miscrowd\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m iscrowd\n\u001b[0;32m---> 46\u001b[0m image, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m target \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mto_tensor(target)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "\u001b[0;31mTypeError\u001b[0m: Compose.__call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "model = SimpleYOLOv3(num_classes=1)\n",
    "\n",
    "# IoU calculation\n",
    "def compute_iou(pred_boxes, true_boxes):\n",
    "    # pred_boxes and true_boxes should be in (x_min, y_min, x_max, y_max)\n",
    "    inter_xmin = torch.max(pred_boxes[:, 0], true_boxes[:, 0])\n",
    "    inter_ymin = torch.max(pred_boxes[:, 1], true_boxes[:, 1])\n",
    "    inter_xmax = torch.min(pred_boxes[:, 2], true_boxes[:, 2])\n",
    "    inter_ymax = torch.min(pred_boxes[:, 3], true_boxes[:, 3])\n",
    "\n",
    "    inter_area = torch.clamp(inter_xmax - inter_xmin, min=0) * torch.clamp(inter_ymax - inter_ymin, min=0)\n",
    "    pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "    true_area = (true_boxes[:, 2] - true_boxes[:, 0]) * (true_boxes[:, 3] - true_boxes[:, 1])\n",
    "\n",
    "    union_area = pred_area + true_area - inter_area\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "# Simple IoU loss function\n",
    "def iou_loss(pred_boxes, true_boxes):\n",
    "    iou = compute_iou(pred_boxes, true_boxes)\n",
    "    return 1 - iou.mean()  # We want to maximize IoU, so minimize 1 - IoU\n",
    "\n",
    "# Custom YOLOv3 training loop\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        print(images.shape())\n",
    "        print(images)\n",
    "        print(targets.shape())\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Extract predicted boxes and target boxes (for simplicity, assuming one grid cell)\n",
    "        pred_boxes = predictions[:, :4]  # first 4 are bounding box coordinates\n",
    "        pred_conf = predictions[:, 4]    # 5th is objectness confidence\n",
    "        pred_class = predictions[:, 5:]  # remaining are class predictions\n",
    "\n",
    "        true_boxes = targets[:, :4]  # Ground truth boxes\n",
    "        true_conf = targets[:, 4]    # Objectness confidence\n",
    "        true_class = targets[:, 5:]  # Ground truth class\n",
    "\n",
    "        # Losses\n",
    "        loss_loc = iou_loss(pred_boxes, true_boxes)  # IoU loss\n",
    "        loss_conf = torch.nn.BCEWithLogitsLoss()(pred_conf, true_conf)  # Confidence loss\n",
    "        loss_class = torch.nn.BCEWithLogitsLoss()(pred_class, true_class)  # Classification loss\n",
    "\n",
    "        # Total loss (sum or weighted sum)\n",
    "        loss = loss_loc + loss_conf + loss_class\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training loss: {avg_loss}\")\n",
    "\n",
    "# Evaluation (testing) function\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Extract predicted boxes and target boxes\n",
    "            pred_boxes = predictions[:, :4]\n",
    "            true_boxes = targets[:, :4]\n",
    "\n",
    "            # Calculate IoU for the batch\n",
    "            iou = compute_iou(pred_boxes, true_boxes)\n",
    "            total_iou += iou.mean().item()\n",
    "\n",
    "    avg_iou = total_iou / len(test_loader)\n",
    "    print(f\"Average IoU on test set: {avg_iou}\")\n",
    "\n",
    "# Initialize model, optimizer, and device\n",
    "model = SimpleYOLOv3(num_classes=1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train(model, train_loader, optimizer, device)\n",
    "    evaluate(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO model\n",
    "#model = YOLO(\"yolov5su.pt\")  # Load pretrained weights\n",
    "#model.train(data=\"yolo.yaml\", epochs=15, imgsz=640, pretrained=True, augment=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 210.5ms\n",
      "1: 640x640 (no detections), 210.5ms\n",
      "2: 640x640 (no detections), 210.5ms\n",
      "3: 640x640 (no detections), 210.5ms\n",
      "4: 640x640 (no detections), 210.5ms\n",
      "5: 640x640 (no detections), 210.5ms\n",
      "6: 640x640 (no detections), 210.5ms\n",
      "7: 640x640 (no detections), 210.5ms\n",
      "8: 640x640 (no detections), 210.5ms\n",
      "Speed: 1.5ms preprocess, 210.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolo_test_predictions/train2\u001b[0m\n",
      "0 label saved to yolo_test_predictions/train2/labels\n",
      "Predictions saved to yolo_test_predictions/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\"\n",
    "\n",
    "# Predict on test images\n",
    "test_images = [os.path.join(test_folder, img) for img in os.listdir(test_folder) if img.endswith(\".jpg\")]\n",
    "results = model.predict(source=test_images, save=True, save_txt=True, project=\"yolo_test_predictions\")\n",
    "\n",
    "# Prepare to save the predictions\n",
    "output_csv_path = os.path.join(\"yolo_test_predictions\", \"predictions.csv\")\n",
    "predictions = []\n",
    "\n",
    "# Process results\n",
    "for result in results:\n",
    "    image_name = os.path.basename(result.path)  # Get the image name\n",
    "    if result.boxes is not None and len(result.boxes) > 0:  # Check if there are predictions\n",
    "        # Convert result.boxes to tensor for easier access\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()  # Convert bounding boxes to array\n",
    "        confidences = result.boxes.conf.cpu().numpy()  # Convert confidence scores to array\n",
    "\n",
    "        # Find the index of the box with the highest confidence\n",
    "        best_idx = confidences.argmax()\n",
    "        best_box = boxes[best_idx]\n",
    "        conf = confidences[best_idx]\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        x_min, y_min, x_max, y_max = best_box\n",
    "        predictions.append([image_name, x_min, y_min, x_max, y_max, conf])\n",
    "    else:\n",
    "        # No predictions for this image\n",
    "        predictions.append([image_name, None, None, None, None, None])\n",
    "\n",
    "# Save predictions to CSV\n",
    "df = pd.DataFrame(predictions, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"confidence\"])\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
